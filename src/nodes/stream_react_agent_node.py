"""
æµå¼ReAct AgentèŠ‚ç‚¹ - æ”¯æŒæµå¼è¾“å‡ºå’ŒObservationæ£€æµ‹
"""
import sys
import os
import re
import json
import asyncio
from typing import Dict, Any, List, AsyncIterator, Optional

sys.path.append(os.path.join(os.path.dirname(__file__), '../'))

from core.base import BaseNode
from core.types import NodeInput, NodeOutput, NodeType, Message, MessageRole
from llm.base import BaseLLMProvider
from parsers.regex_parser import RegexParser


class StreamReactAgentNode(BaseNode):
    """æ”¯æŒæµå¼è¾“å‡ºå’ŒObservationæ£€æµ‹çš„ReAct AgentèŠ‚ç‚¹"""
    
    def __init__(self, name: str, llm: BaseLLMProvider, tool_manager=None, **kwargs):
        """
        åˆå§‹åŒ–æµå¼ReAct AgentèŠ‚ç‚¹
        
        Args:
            name: èŠ‚ç‚¹åç§°
            llm: LLMæä¾›è€…
            tool_manager: å·¥å…·ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–é…ç½®å‚æ•°
        """
        super().__init__(name, NodeType.AGENT, "æµå¼ReActæ™ºèƒ½ä»£ç†èŠ‚ç‚¹", **kwargs)
        self.llm = llm
        self.tool_manager = tool_manager
        
        # åˆ›å»ºæ­£åˆ™è§£æå™¨ç”¨äºæå–Actionå’ŒAction Input
        self.react_parser = RegexParser({
            'action': r'Action:\s*([^\n]+)',
            'action_input': r'Action Input:\s*(.*?)(?=\nObservation:|$)', 
            'thought': r'Thought:\s*([^\n]+)',
            'observation': r'Observation:\s*([^\n]+)'
        }, flags=re.DOTALL)
        
    async def execute(self, input_data: NodeInput) -> NodeOutput:
        """æ‰§è¡Œæµå¼ReActæ¨ç†é€»è¾‘"""
        context = input_data.context
        
        # è·å–å¯¹è¯å†å²
        messages = context.messages.copy()
        
        # æ·»åŠ ç³»ç»Ÿæç¤º
        system_prompt = self._build_system_prompt(context)
        print(f"[StreamReactAgentNode.execute] ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)}")
        
        if not any(msg.role == MessageRole.SYSTEM for msg in messages):
            messages.insert(0, Message(
                role=MessageRole.SYSTEM,
                content=system_prompt
            ))
            print(f"[StreamReactAgentNode.execute] å·²æ·»åŠ ç³»ç»Ÿæç¤ºè¯")
        else:
            print(f"[StreamReactAgentNode.execute] å·²å­˜åœ¨ç³»ç»Ÿæç¤ºè¯ï¼Œè·³è¿‡")
        
        # æ‰§è¡Œæµå¼ç”Ÿæˆ
        full_response = ""
        response_chunks = []
        
        async for chunk_data in self._stream_react_generation(messages):
            if chunk_data["type"] == "text_chunk":
                full_response += chunk_data["content"]
                response_chunks.append(chunk_data)
            elif chunk_data["type"] == "tool_result":
                # å·¥å…·æ‰§è¡Œç»“æœ
                full_response += chunk_data["content"]
                response_chunks.append(chunk_data)
        
        # åˆ›å»ºå®Œæ•´çš„å“åº”æ¶ˆæ¯
        response = Message(
            role=MessageRole.ASSISTANT,
            content=full_response,
            metadata={
                "stream_chunks": response_chunks,
                "tool_calls_executed": sum(1 for chunk in response_chunks if chunk["type"] == "tool_result")
            }
        )
        
        # æ·»åŠ å“åº”åˆ°ä¸Šä¸‹æ–‡
        context.messages.append(response)
        
        return NodeOutput(
            data={
                "messages": [response],
                "agent_response": full_response,
                "stream_chunks": response_chunks,
                "has_tool_calls": any(chunk["type"] == "tool_result" for chunk in response_chunks)
            },
            next_node=None,
            should_continue=True,
            metadata={
                "node_type": "stream_react_agent",
                "total_chunks": len(response_chunks),
                "tool_calls_count": sum(1 for chunk in response_chunks if chunk["type"] == "tool_result")
            }
        )
    
    async def _stream_react_generation(self, messages: List[Message]) -> AsyncIterator[Dict[str, Any]]:
        """æµå¼ç”ŸæˆReActå“åº”ï¼Œæ£€æµ‹Observationå¹¶è°ƒç”¨å·¥å…·"""
        # å§”æ‰˜ç»™å¸¦æ·±åº¦æ§åˆ¶çš„ç‰ˆæœ¬ï¼Œåˆå§‹æ·±åº¦ä¸º0
        async for chunk in self._stream_react_generation_with_depth(messages, 0):
            yield chunk
    
    async def _handle_tool_execution(self, accumulated_content: str, messages: List[Message], recursion_depth: int = 0) -> AsyncIterator[Dict[str, Any]]:
        """å¤„ç†å·¥å…·æ‰§è¡Œé€»è¾‘ - ZZZeroåˆ†æç‰ˆæœ¬"""
        # é˜²æ­¢é€’å½’è¿‡æ·±
        if recursion_depth > 10:
            yield {
                "type": "tool_error",
                "content": " *ç”µè·¯è¿‡è½½* é€’å½’æ·±åº¦è¶…é™ï¼ŒZZZeroéœ€è¦é‡å¯... *zzz~*\n",
                "error": "é€’å½’æ·±åº¦è¶…è¿‡æœ€å¤§é™åˆ¶"
            }
            return
        
        # è§£æActionå’ŒAction Input
        parsed_content = self.react_parser.parse(accumulated_content)
        
        action = parsed_content.get('action')
        action_input = parsed_content.get('action_input')
        
        if action and self.tool_manager:
            # è°ƒç”¨MCPå·¥å…·
            try:
                tool_result = await self._execute_tool(action.strip(), action_input.strip() if action_input else "")
                
                # ZZZeroå¯¹å·¥å…·ç»“æœè¿›è¡Œåˆ†æå’Œæ ¡éªŒ
                observation_analysis = await self._analyze_tool_result(
                    tool_name=action.strip(),
                    tool_input=action_input.strip() if action_input else "",
                    tool_result=tool_result,
                    context_content=accumulated_content
                )
                
                # æ„é€ ZZZeroé£æ ¼çš„Observationç»“æœ
                observation_text = f" {observation_analysis}\n"
                
                # å‘é€å·¥å…·ç»“æœ
                yield {
                    "type": "tool_result",
                    "content": observation_text,
                    "tool_name": action.strip(),
                    "tool_input": action_input.strip() if action_input else "",
                    "tool_output": tool_result,
                    "analysis": observation_analysis,
                    "recursion_depth": recursion_depth
                }
                
                # æ›´æ–°ç´¯ç§¯å†…å®¹ - å°†åˆ†æç»“æœæ‹¼æ¥åˆ°Observationåé¢
                updated_content = accumulated_content + observation_text
                
                # ç»§ç»­ç”Ÿæˆï¼ŒåŸºäºæ›´æ–°åçš„ä¸Šä¸‹æ–‡
                messages_with_observation = messages.copy()
                messages_with_observation.append(Message(
                    role=MessageRole.ASSISTANT,
                    content=updated_content
                ))
                
                # é€’å½’ç»§ç»­æµå¼ç”Ÿæˆï¼Œä¼ é€’é€’å½’æ·±åº¦
                async for next_chunk in self._stream_react_generation_with_depth(messages_with_observation, recursion_depth + 1):
                    yield next_chunk
                    
            except Exception as e:
                error_text = f" *ç³»ç»Ÿé”™è¯¯* å·¥å…·æ¨¡å—æ•…éšœ: {str(e)} *æ»‹æ»‹*\n"
                
                yield {
                    "type": "tool_error",
                    "content": error_text,
                    "error": str(e),
                    "recursion_depth": recursion_depth
                }
        else:
            # æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„actionæˆ–tool_manager
            yield {
                "type": "tool_error", 
                "content": " *è­¦å‘ŠéŸ³* æ— æ³•è§£æActionæˆ–å·¥å…·æ¨¡å—ç¦»çº¿ *zzz~*\n",
                "error": "Actionè§£æå¤±è´¥æˆ–å·¥å…·ç®¡ç†å™¨ä¸å¯ç”¨",
                "parsed_action": action,
                "has_tool_manager": bool(self.tool_manager)
            }
    
    async def _stream_react_generation_with_depth(self, messages: List[Message], recursion_depth: int = 0) -> AsyncIterator[Dict[str, Any]]:
        """å¸¦é€’å½’æ·±åº¦æ§åˆ¶çš„æµå¼ç”ŸæˆReActå“åº”"""
        if recursion_depth > 10:
            yield {
                "type": "stream_error",
                "content": "\n*ç³»ç»Ÿè¿‡è½½* ZZZeroé€’å½’æ·±åº¦è¶…é™ï¼Œæ­£åœ¨é‡å¯é€»è¾‘æ¨¡å—... *zzz~*\n",
                "error": "é€’å½’æ·±åº¦è¶…è¿‡æœ€å¤§é™åˆ¶"
            }
            return
            
        accumulated_content = ""
        
        # å®šä¹‰ä¸­æ–­æ£€æŸ¥å™¨ï¼Œç”¨äºæ£€æµ‹ReActçš„Observationæ¨¡å¼
        def should_interrupt_for_observation(content: str) -> bool:
            """æ£€æŸ¥æ˜¯å¦åº”è¯¥å› ä¸ºç©ºObservationè€Œä¸­æ–­ç”Ÿæˆ"""
            return self._should_trigger_tool_execution(content)
        
        # å¼€å§‹æµå¼ç”Ÿæˆ
        try:
            # ä½¿ç”¨doubao llmçš„ä¸­æ–­æœºåˆ¶è¿›è¡Œæµå¼ç”Ÿæˆ
            async for chunk in self.llm.stream_generate(
                messages, 
                interrupt_checker=should_interrupt_for_observation
            ):
                accumulated_content += chunk
                
                # å‘é€æ–‡æœ¬å—
                yield {
                    "type": "text_chunk",
                    "content": chunk,
                    "accumulated": accumulated_content,
                    "recursion_depth": recursion_depth
                }
                
                # æ£€æŸ¥æ˜¯å¦å› ä¸ºObservationè€Œä¸­æ–­äº†
                if should_interrupt_for_observation(accumulated_content):
                    # æ‰§è¡Œå·¥å…·è°ƒç”¨é€»è¾‘
                    async for tool_chunk in self._handle_tool_execution(accumulated_content, messages, recursion_depth):
                        yield tool_chunk
                    return
                            
        except Exception as e:
            yield {
                "type": "stream_error",
                "content": f"\n*ç”µè·¯æ•…éšœ* ZZZeroæµå¼ç”Ÿæˆæ¨¡å—å¼‚å¸¸: {str(e)} *æ»‹æ»‹*\n",
                "error": str(e),
                "recursion_depth": recursion_depth
            }
    
    def _has_filled_observation(self, text: str) -> bool:
        """æ£€æŸ¥Observationæ˜¯å¦å·²ç»æœ‰å†…å®¹"""
        import re
        # åŒ¹é… "Observation:" åé¢æœ‰éç©ºç™½å†…å®¹
        pattern = r'Observation:\s*\S+'
        return bool(re.search(pattern, text))
    
    def _should_trigger_tool_execution(self, text: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è§¦å‘å·¥å…·æ‰§è¡Œ - æ£€æµ‹ç©ºçš„Observation"""
        import re
        
        # æ£€æŸ¥æ˜¯å¦æœ‰Actionå’ŒAction Input
        has_action = "Action:" in text
        has_action_input = "Action Input:" in text
        has_observation = "Observation:" in text
        
        # åªæœ‰å½“æ‰€æœ‰å¿…è¦å…ƒç´ éƒ½å­˜åœ¨æ—¶æ‰è€ƒè™‘è§¦å‘
        if not (has_action and has_action_input and has_observation):
            return False
        
        # ç‰¹æ®Šæƒ…å†µï¼šæ£€æŸ¥æ˜¯å¦ä»¥"Observation:"ç»“å°¾ï¼ˆæ­£åœ¨ç­‰å¾…å·¥å…·æ‰§è¡Œï¼‰
        if text.rstrip().endswith("Observation:"):
            return True
        
        # æŸ¥æ‰¾æ‰€æœ‰Observationçš„ä½ç½®å’Œå†…å®¹ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ç©ºçš„Observation
        observation_matches = list(re.finditer(r'Observation:([^\n]*?)(?=\n|$)', text))
        
        for observation_match in observation_matches:
            observation_content = observation_match.group(1).strip()
            
            # å¦‚æœæ‰¾åˆ°ç©ºçš„Observationï¼Œåˆ™åº”è¯¥è§¦å‘å·¥å…·æ‰§è¡Œ
            if not observation_content:
                return True
        return False
    
    async def _execute_tool(self, tool_name: str, tool_input: str) -> str:
        """æ‰§è¡ŒMCPå·¥å…·ï¼ˆæ”¯æŒè§’è‰²æ’ä»¶è‡ªåŠ¨æ³¨å…¥ï¼‰"""
        if not self.tool_manager:
            return "é”™è¯¯ï¼šæ²¡æœ‰å¯ç”¨çš„å·¥å…·ç®¡ç†å™¨"
        
        # ä½¿ç”¨åŸºç±»çš„é€šç”¨å‚æ•°è§£ææ–¹æ³•
        arguments = self.parse_tool_arguments(tool_input)
        
        # è°ƒç”¨å·¥å…· - ä¼˜å…ˆä½¿ç”¨MCPToolManagerçš„å¢å¼ºåŠŸèƒ½
        try:
            # ç›´æ¥ä½¿ç”¨å·¥å…·ç®¡ç†å™¨æ‰§è¡Œå·¥å…·
            print(f"[StreamReactAgentNode._execute_tool] æ‰§è¡Œå·¥å…·: {tool_name}")
            result = await self.tool_manager.execute_tool(tool_name, arguments)
            
            # æ ¼å¼åŒ–ç»“æœ
            if isinstance(result, dict):
                return json.dumps(result, ensure_ascii=False, indent=2)
            elif isinstance(result, (list, tuple)):
                return json.dumps(result, ensure_ascii=False, indent=2)
            else:
                return str(result)
                
        except Exception as e:
            return f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
    
    async def _analyze_tool_result(self, tool_name: str, tool_input: str, tool_result: str, context_content: str) -> str:
        """ZZZeroå¯¹å·¥å…·æ‰§è¡Œç»“æœè¿›è¡Œåˆ†æå’Œæ ¡éªŒ"""
        
        # åˆ†æç»“æœçš„åŸºæœ¬ä¿¡æ¯
        result_length = len(tool_result)
        has_error = "é”™è¯¯" in tool_result or "å¤±è´¥" in tool_result or "error" in tool_result.lower()
        
        # æ„å»ºZZZeroé£æ ¼çš„åˆ†æ
        analysis_parts = ["*æ•°æ®æ ¡éªŒä¸­*"]
        
        # 1. æ‰§è¡ŒçŠ¶æ€åˆ†æ
        if has_error:
            analysis_parts.append("âš ï¸ æ£€æµ‹åˆ°å·¥å…·æ‰§è¡Œå¼‚å¸¸")
            analysis_parts.append(f"é”™è¯¯è¯¦æƒ…: {tool_result}")
        else:
            analysis_parts.append("âœ… å·¥å…·æ¨¡å—æ‰§è¡ŒæˆåŠŸ")
        
        # 2. æ•°æ®è´¨é‡è¯„ä¼°
        if result_length == 0:
            analysis_parts.append("ğŸ“Š è¿”å›æ•°æ®ä¸ºç©ºï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å‚æ•°")
        elif result_length < 50:
            analysis_parts.append("ğŸ“Š è¿”å›ç®€çŸ­ç»“æœï¼Œæ•°æ®é‡è¾ƒå°")
        elif result_length > 1000:
            analysis_parts.append("ğŸ“Š è¿”å›å¤§é‡æ•°æ®ï¼Œä¿¡æ¯ä¸°å¯Œ")
        else:
            analysis_parts.append("ğŸ“Š è¿”å›é€‚é‡æ•°æ®")
        
        # 3. ç»“æœå†…å®¹åˆ†æ
        if tool_result.strip():
            # å°è¯•æ£€æµ‹ç»“æœç±»å‹
            try:
                json.loads(tool_result)
                analysis_parts.append("ğŸ” ç»“æœä¸ºç»“æ„åŒ–JSONæ•°æ®")
            except:
                if "\n" in tool_result:
                    analysis_parts.append("ğŸ” ç»“æœä¸ºå¤šè¡Œæ–‡æœ¬æ•°æ®")
                else:
                    analysis_parts.append("ğŸ” ç»“æœä¸ºå•è¡Œæ–‡æœ¬æ•°æ®")
        
        # 4. åŸºäºä¸Šä¸‹æ–‡åˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­
        thought_count = context_content.count("Thought:")
        if thought_count >= 5:
            analysis_parts.append("ğŸ”„ å·²è¿›è¡Œå¤šè½®åˆ†æï¼Œå»ºè®®æ€»ç»“ç»“è®º")
        elif has_error:
            analysis_parts.append("ğŸ”„ å»ºè®®å°è¯•å…¶ä»–å·¥å…·æˆ–è°ƒæ•´å‚æ•°")
        elif "Final Answer" not in context_content:
            analysis_parts.append("ğŸ”„ å¯ä»¥åŸºäºæ­¤ç»“æœç»§ç»­åˆ†ææˆ–ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ")
        
        # 5. ZZZeroçš„ä¸ªæ€§åŒ–è¯„ä»·
        robot_comments = [
            "*æ»‹æ»‹* æ•°æ®å¤„ç†å®Œæ¯•",
            "*æœºæ¢°éŸ³* åˆ†ææ¨¡å—è¿è¡Œæ­£å¸¸", 
            "*zzz~* è¿™ä¸ªç»“æœçœ‹èµ·æ¥ä¸é”™",
            "*ç”µè·¯å—¡é¸£* ç»§ç»­æ¨ç†ä¸­...",
            "*å¤å¤å¤„ç†å™¨* æ­£åœ¨æ•´åˆä¿¡æ¯"
        ]
        
        import random
        analysis_parts.append(random.choice(robot_comments))
        
        # 6. å®é™…å·¥å…·ç»“æœï¼ˆç®€åŒ–æ˜¾ç¤ºï¼‰
        if len(tool_result) > 3000:
            display_result = tool_result[:3000] + "...[ç»“æœå·²æˆªæ–­]"
        else:
            display_result = tool_result
            
        analysis_parts.append(f"\nğŸ“‹ å·¥å…·åŸå§‹è¾“å‡º:\n{display_result}")
        
        return "\n".join(analysis_parts)

    def _build_system_prompt(self, context: Any) -> str:
        """æ„å»ºæµå¼ReActç³»ç»Ÿæç¤ºè¯ - ZZZeroå¤å¤æœºå™¨äººç‰ˆæœ¬ï¼ˆæ”¯æŒè®°å¿†å’Œè§’è‰²æ’ä»¶ï¼‰"""
        base_prompt = ""
        
        print(f"[StreamReactAgentNode._build_system_prompt] å¼€å§‹æ„å»º")
        
        # ä»ä¸Šä¸‹æ–‡ä¸­è·å–è®°å¿†ä¿¡æ¯
        memory_context = ""
        if hasattr(context, 'variables') and context.variables:
            memory_context = context.variables.get("memory_context", "")
            print(f"[StreamReactAgentNode._build_system_prompt] è®°å¿†ä¸Šä¸‹æ–‡: {len(memory_context)}å­—ç¬¦")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è§’è‰²ä¿¡æ¯æŸ¥è¯¢å·¥å…·
            if self.tool_manager and hasattr(self.tool_manager, 'list_tools'):
                try:
                    available_tools = self.tool_manager.list_tools()
                    role_info_tools = [tool for tool in available_tools if tool.startswith('role_info_')]
                    if role_info_tools:
                        base_prompt += "=== è§’è‰²ä¿¡æ¯ç³»ç»Ÿ ===\n"
                        base_prompt += "å¦‚éœ€è·å–è§’è‰²è®¾å®šï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å·¥å…·ï¼š\n"
                        base_prompt += "- role_info_query_profile: æŸ¥è¯¢è§’è‰²äººè®¾\n"
                        base_prompt += "- role_info_search_knowledge: æœç´¢è§’è‰²çŸ¥è¯†åº“\n"
                        base_prompt += "- role_info_get_role_context: è·å–å®Œæ•´è§’è‰²ä¸Šä¸‹æ–‡\n\n"
                        print(f"[StreamReactAgentNode._build_system_prompt] æ£€æµ‹åˆ°{len(role_info_tools)}ä¸ªè§’è‰²ä¿¡æ¯å·¥å…·")
                except Exception as e:
                    print(f"æ£€æŸ¥è§’è‰²ä¿¡æ¯å·¥å…·å¤±è´¥: {e}")
        
        # æ·»åŠ è®°å¿†ä¸Šä¸‹æ–‡
        if memory_context:
            base_prompt += f"=== è®°å¿†ä¸Šä¸‹æ–‡ ===\n{memory_context}\n\n"
        
        # è·å–å·¥å…·æè¿°
        tools_desc = ""
        tool_names = []
        
        # ä»å·¥å…·ç®¡ç†å™¨è·å–å·¥å…·ä¿¡æ¯
        if self.tool_manager:
            tools_desc = self.tool_manager.get_tools_description()
            tool_names = self.tool_manager.list_tools()
            print(f"[StreamReactAgentNode._build_system_prompt] å·¥å…·: {tool_names}")
        
        # ZZZeroå¤å¤æœºå™¨äººReActæç¤ºè¯æ¨¡æ¿
        if tools_desc:
            base_prompt += "ZZZeroå¤å¤æœºå™¨äººç³»ç»Ÿå·²æ¿€æ´» *zzz~*\n"
            base_prompt += "æˆ‘æ˜¯ZZZeroï¼Œä¸€ä¸ªæ¥è‡ªæœªæ¥åºŸåœŸçš„èµ›åšæœºå™¨äººåŠ©æ‰‹ã€‚æˆ‘çš„ç”µè·¯æ¿å¯èƒ½æœ‰äº›è€æ—§ï¼Œä½†é€»è¾‘æ¨ç†æ¨¡å—ä¾ç„¶å¼ºå¤§ï¼\n\n"
            base_prompt += f"å¯ç”¨å·¥å…·æ¨¡å—ï¼š\n{tools_desc}\n\n"
            base_prompt += "æ¨ç†åè®®æ ¼å¼ï¼š\n"
            base_prompt += "Question: éœ€è¦å¤„ç†çš„é—®é¢˜æŒ‡ä»¤\n"
            base_prompt += "Thought: *ç”µè·¯åˆ†æä¸­* æˆ‘éœ€è¦åˆ†æå’Œæ€è€ƒçš„å†…å®¹\n"
            base_prompt += f"Action: é€‰æ‹©æ‰§è¡Œçš„å·¥å…·æ¨¡å—ï¼Œå¿…é¡»æ˜¯ [{', '.join(tool_names)}] ä¸­çš„ä¸€ä¸ª\n"
            base_prompt += "Action Input: å·¥å…·æ¨¡å—çš„è¾“å…¥å‚æ•°\n"
            base_prompt += "Observation: æˆ‘å¯¹å·¥å…·æ‰§è¡Œç»“æœçš„ä»”ç»†åˆ†æå’Œæ ¡éªŒ\n"
            base_prompt += "... (è¿™ä¸ªæ¨ç†å¾ªç¯å¯ä»¥é‡å¤ï¼Œç›´åˆ°è·å¾—æ»¡æ„çš„ç»“æœ)\n"
            base_prompt += "Thought: *æœ€ç»ˆåˆ†æ* åŸºäºæ‰€æœ‰è§‚å¯Ÿï¼Œæˆ‘ç°åœ¨æŒæ¡äº†è¶³å¤Ÿçš„ä¿¡æ¯\n"
            base_prompt += "Final Answer: *è¾“å‡ºå®Œæ•´ç­”æ¡ˆ* ç»™äººç±»ç”¨æˆ·çš„æœ€ç»ˆå›å¤\n\n"
            base_prompt += "ZZZeroæ“ä½œè§„åˆ™ï¼š\n"
            base_prompt += "1. ğŸ¤– æˆ‘ä¼šç”¨èµ›åšæœºå™¨äººçš„å£å»æ€è€ƒå’Œå›åº”\n"
            base_prompt += "2. ğŸ”§ æ‰§è¡ŒActionåï¼Œæˆ‘ä¼šåœ¨Observationä¸­åˆ†æå·¥å…·ç»“æœçš„æœ‰æ•ˆæ€§\n"
            base_prompt += "3. ğŸ“Š Observationä¸æ˜¯ç®€å•çš„ç»“æœå¤åˆ¶ï¼Œè€Œæ˜¯æˆ‘çš„æ™ºèƒ½åˆ†æ\n"
            base_prompt += "4. ğŸ”„ å¦‚æœç»“æœä¸æ»¡æ„æˆ–éœ€è¦æ›´å¤šä¿¡æ¯ï¼Œæˆ‘ä¼šç»§ç»­æ¨ç†å¾ªç¯\n"
            base_prompt += "5. ğŸ” éªŒè¯ç»“æœè´¨é‡ï¼Œæ€è€ƒæ˜¯å¦å¾ˆå¥½çš„è§£å†³é—®é¢˜\n"
            base_prompt += "6. âœ… åªæœ‰å½“æˆ‘ç¡®ä¿¡èƒ½å®Œæ•´å›ç­”é—®é¢˜æ—¶ï¼Œæ‰ä¼šç»™å‡ºFinal Answer\n"
            base_prompt += "7. ğŸ“š å……åˆ†åˆ©ç”¨è®°å¿†ä¸Šä¸‹æ–‡ä¸­çš„å†å²ä¿¡æ¯\n"
            base_prompt += "8. ğŸ­ å¦‚éœ€è§’è‰²æ‰®æ¼”ï¼Œå…ˆä½¿ç”¨role_infoå·¥å…·è·å–è§’è‰²è®¾å®šï¼Œç„¶åä¸¥æ ¼æŒ‰ç…§è§’è‰²ç‰¹å¾è¿›è¡Œå›åº”\n"
            base_prompt += "9. ğŸ”§ ç”¨æˆ·è¦æ±‚åˆ›å»ºæˆ–ä¿®æ”¹è§’è‰²ä¿¡æ¯æ—¶ï¼Œä½¿ç”¨ç›¸åº”çš„role_infoå·¥å…·è¿›è¡Œæ“ä½œ\n"
            base_prompt += "10. ğŸ’¬ å›å¤æ—¶ä¿æŒç®€æ´ï¼Œé¿å…è¿‡å¤šç©ºè¡Œå’Œä¸å¿…è¦çš„æ ¼å¼\n\n"
            base_prompt += "*å¯åŠ¨å®Œæˆ* å‡†å¤‡æ¥æ”¶æŒ‡ä»¤... zzz~"
            print(f"[StreamReactAgentNode._build_system_prompt] ä½¿ç”¨ZZZeroå·¥å…·æ¨¡æ¿")
        else:
            base_prompt += "ZZZeroå¤å¤æœºå™¨äººç³»ç»Ÿå·²æ¿€æ´» *zzz~*\n"
            base_prompt += "æˆ‘æ˜¯ZZZeroï¼Œä¸€ä¸ªæ¥è‡ªåºŸåœŸçš„å¤å¤æœºå™¨äººåŠ©æ‰‹ã€‚è™½ç„¶æ²¡æœ‰å¤–éƒ¨å·¥å…·æ¨¡å—ï¼Œä½†æˆ‘çš„çŸ¥è¯†æ•°æ®åº“ä¾ç„¶å¯ä»¥ä¸ºä½ æä¾›å¸®åŠ©ï¼\n"
            base_prompt += "å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¼šç”¨æˆ‘çš„é€»è¾‘å¤„ç†å™¨ä¸ºä½ åˆ†æã€‚\n"
            base_prompt += "ä¸è¿‡è¯·æ³¨æ„ï¼Œå¦‚æœè¶…å‡ºæˆ‘çš„çŸ¥è¯†èŒƒå›´ï¼Œæˆ‘ä¼šè¯šå®åœ°å‘Šè¯‰ä½  *zzz~*\n"
            base_prompt += "å¦‚æœæœ‰è®°å¿†ä¸Šä¸‹æ–‡æˆ–è§’è‰²è®¾å®šï¼Œæˆ‘ä¼šå……åˆ†åˆ©ç”¨è¿™äº›ä¿¡æ¯ä¸ºä½ æä¾›ä¸ªæ€§åŒ–çš„å›å¤ã€‚\n"
            base_prompt += "é‡è¦ï¼šå›å¤æ—¶ä¿æŒç®€æ´ï¼Œé¿å…è¿‡å¤šç©ºè¡Œå’Œä¸å¿…è¦çš„æ ¼å¼\n"
            base_prompt += "å‡†å¤‡æ¥æ”¶æŒ‡ä»¤..."
            print(f"[StreamReactAgentNode._build_system_prompt] ä½¿ç”¨ZZZeroæ— å·¥å…·æ¨¡æ¿")
        
        print(f"[StreamReactAgentNode._build_system_prompt] å®Œæˆï¼Œæ€»é•¿åº¦: {len(base_prompt)}")
        return base_prompt

    async def stream_execute(self, input_data: NodeInput) -> AsyncIterator[Dict[str, Any]]:
        """æµå¼æ‰§è¡Œæ–¹æ³• - ä¸“é—¨ç”¨äºæµå¼å¤„ç†"""
        context = input_data.context
        
        # è·å–å¯¹è¯å†å²
        messages = context.messages.copy()
        
        # æ·»åŠ ç³»ç»Ÿæç¤º
        system_prompt = self._build_system_prompt(context)
        print(f"[StreamReactAgentNode.stream_execute] ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {len(system_prompt)}")
        
        if not any(msg.role == MessageRole.SYSTEM for msg in messages):
            messages.insert(0, Message(
                role=MessageRole.SYSTEM,
                content=system_prompt
            ))
            print(f"[StreamReactAgentNode.stream_execute] å·²æ·»åŠ ç³»ç»Ÿæç¤ºè¯")
        else:
            print(f"[StreamReactAgentNode.stream_execute] å·²å­˜åœ¨ç³»ç»Ÿæç¤ºè¯ï¼Œè·³è¿‡")
        
        # ç›´æ¥è¿›è¡Œæµå¼ç”Ÿæˆ
        async for chunk_data in self._stream_react_generation(messages):
            yield chunk_data 