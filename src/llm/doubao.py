"""
è±†åŒ…(Doubao) LLM å®ç°
"""
import json
import aiohttp
from typing import List, Dict, Any, AsyncIterator, Optional, NamedTuple
from datetime import datetime

from .base import BaseLLMProvider, LLMFactory, ThinkResult
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from core.types import Message, MessageRole


class DoubaoLLM(BaseLLMProvider):
    """è±†åŒ…LLMå®ç°"""
    
    async def initialize(self):
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        # å¦‚æœæ²¡æœ‰ä¼ å…¥api_keyï¼Œå°è¯•ä»ç¯å¢ƒå˜é‡è·å–
        if not self.config.api_key:
            # æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„ç¯å¢ƒå˜é‡å
            env_keys = ['ARK_API_KEY', 'DOUBAO_API_KEY', 'ARK_API_KEY_ME']
            for env_key in env_keys:
                api_key = os.getenv(env_key)
                if api_key:
                    self.config.api_key = api_key
                    break
        
        # å†æ¬¡æ£€æŸ¥æ˜¯å¦æœ‰APIå¯†é’¥
        if not self.config.api_key:
            raise ValueError("è±†åŒ…APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® ARK_API_KEY æˆ– DOUBAO_API_KEY")
            
        # è®¾ç½®é»˜è®¤APIåŸºç¡€URL
        if not self.config.api_base:
            # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„URL
            self.config.api_base = os.getenv('DOUBAO_BASE_URL', "https://ark.cn-beijing.volces.com/api/v3")
    
    async def _think(self, 
                   messages: List[Message],
                   **kwargs) -> ThinkResult:
        """
        ä½¿ç”¨DeepSeek R1æ¨ç†æ¨¡å‹è¿›è¡Œæ€è€ƒ
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ThinkResult: åŒ…å«æ¨ç†è¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆçš„ç»“æœ
        """
        # è·å–DeepSeek R1æ¨¡å‹åç§°ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è·å–
        deepseek_model = os.getenv('DOUBAO_MODEL_DEEPSEEKR1', 'deepseek-reasoner')
        
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        formatted_messages = self._convert_messages(messages)
        
        # æ„å»ºè¯·æ±‚æ•°æ® - DeepSeek R1æ¨ç†æ¨¡å‹ä¸“ç”¨é…ç½®
        data = {
            "model": deepseek_model,
            "messages": formatted_messages,
            "temperature": kwargs.get("temperature", 0.6),  # DeepSeek R1æ¨èæ¸©åº¦
            "max_tokens": kwargs.get("max_tokens", 16384),  # DeepSeek R1æœ€å¤§æ”¯æŒ16384 tokens
        }
        
        # DeepSeek R1ä¸æ”¯æŒçš„å‚æ•°ï¼Œéœ€è¦è¿‡æ»¤
        unsupported_params = ['top_p', 'presence_penalty', 'frequency_penalty']
        if self.config.extra_params:
            filtered_params = {k: v for k, v in self.config.extra_params.items() 
                             if k not in unsupported_params}
            data.update(filtered_params)
            
        # å‘é€è¯·æ±‚
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.config.api_base}/chat/completions",
                headers=headers,
                json=data,
                timeout=aiohttp.ClientTimeout(total=self.config.timeout)
            ) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"è±†åŒ…APIè°ƒç”¨å¤±è´¥: {error_text}")
                    
                result = await response.json()
                
                # æå–æ¨ç†å†…å®¹å’Œæœ€ç»ˆç­”æ¡ˆ
                choice = result["choices"][0]["message"]
                reasoning_content = choice.get("reasoning_content", "")
                content = choice.get("content", "")
                
                # æ„å»ºå…ƒæ•°æ®
                metadata = {
                    "model": deepseek_model,
                    "usage": result.get("usage", {}),
                    "finish_reason": result["choices"][0].get("finish_reason"),
                    "has_reasoning": bool(reasoning_content),
                    "reasoning_length": len(reasoning_content),
                    "content_length": len(content)
                }
                
                return ThinkResult(
                    reasoning_content=reasoning_content,
                    content=content,
                    metadata=metadata
                )
    
    async def _stream_think(self,
                          messages: List[Message],
                          **kwargs) -> AsyncIterator[Dict[str, Any]]:
        """
        æµå¼æ¨ç†æ¥å£ - æ”¯æŒDeepSeek R1çš„æµå¼æ¨ç†è¾“å‡º
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
            
        Yields:
            Dict: åŒ…å«æ¨ç†è¿‡ç¨‹æˆ–æœ€ç»ˆç­”æ¡ˆçš„æµå¼æ•°æ®
        """
        # è·å–DeepSeek R1æ¨¡å‹åç§°
        deepseek_model = os.getenv('DOUBAO_MODEL_DEEPSEEKR1', 'deepseek-reasoner')
        
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        formatted_messages = self._convert_messages(messages)
        
        # æ„å»ºæµå¼è¯·æ±‚æ•°æ®
        data = {
            "model": deepseek_model,
            "messages": formatted_messages,
            "temperature": kwargs.get("temperature", 0.6),
            "max_tokens": kwargs.get("max_tokens", 16384),  # DeepSeek R1æœ€å¤§æ”¯æŒ16384 tokens
            "stream": True,
            "stream_options": {
                "include_usage": True
            }
        }
        
        # è¿‡æ»¤ä¸æ”¯æŒçš„å‚æ•°
        if self.config.extra_params:
            unsupported_params = ['top_p', 'presence_penalty', 'frequency_penalty']
            filtered_params = {k: v for k, v in self.config.extra_params.items() 
                             if k not in unsupported_params}
            data.update(filtered_params)
            
        # å‘é€æµå¼è¯·æ±‚
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.config.api_base}/chat/completions",
                headers=headers,
                json=data,
                timeout=aiohttp.ClientTimeout(total=self.config.timeout)
            ) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"è±†åŒ…APIè°ƒç”¨å¤±è´¥: {error_text}")
                
                # ç´¯ç§¯å†…å®¹ç”¨äºæ„å»ºå®Œæ•´ç»“æœ
                accumulated_reasoning = ""
                accumulated_content = ""
                
                # å¤„ç†æµå¼å“åº”
                async for line in response.content:
                    line_text = line.decode('utf-8').strip()
                    if not line_text:
                        continue
                        
                    if line_text.startswith("data: "):
                        line_text = line_text[6:]
                        
                    if line_text == "[DONE]":
                        # å‘é€æœ€ç»ˆå®Œæ•´ç»“æœ
                        yield {
                            "type": "think_complete",
                            "reasoning_content": accumulated_reasoning,
                            "content": accumulated_content,
                            "metadata": {
                                "model": deepseek_model,
                                "has_reasoning": bool(accumulated_reasoning),
                                "reasoning_length": len(accumulated_reasoning),
                                "content_length": len(accumulated_content)
                            }
                        }
                        break
                        
                    try:
                        chunk = json.loads(line_text)
                        
                        if chunk.get("choices") and len(chunk["choices"]) > 0:
                            choice = chunk["choices"][0]
                            delta = choice.get("delta", {})
                            
                            # å¤„ç†æ¨ç†å†…å®¹
                            if delta.get("reasoning_content"):
                                reasoning_chunk = delta["reasoning_content"]
                                accumulated_reasoning += reasoning_chunk
                                yield {
                                    "type": "reasoning_chunk",
                                    "content": reasoning_chunk,
                                    "accumulated_reasoning": accumulated_reasoning
                                }
                            
                            # å¤„ç†æœ€ç»ˆç­”æ¡ˆå†…å®¹
                            if delta.get("content"):
                                content_chunk = delta["content"]
                                accumulated_content += content_chunk
                                yield {
                                    "type": "content_chunk",
                                    "content": content_chunk,
                                    "accumulated_content": accumulated_content
                                }
                                
                    except json.JSONDecodeError as e:
                        # è·³è¿‡JSONè§£æé”™è¯¯ï¼Œä½†è¾“å‡ºè°ƒè¯•ä¿¡æ¯
                        print(f"[DoubaoLLM.stream_think] JSONè§£æå¤±è´¥: {line_text[:100]}")
                        continue
                    except Exception as e:
                        print(f"[DoubaoLLM.stream_think] å¤„ç†chunkæ—¶å‡ºé”™: {e}")
                        continue
            
    async def generate(self, 
                      messages: List[Message],
                      mode: str = "normal",
                      **kwargs) -> Message:
        """ç”Ÿæˆå›å¤
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            mode: ç”Ÿæˆæ¨¡å¼ï¼Œ'normal' æˆ– 'think'
            **kwargs: å…¶ä»–å‚æ•°
        """
        # å¦‚æœæ˜¯thinkæ¨¡å¼ï¼Œè°ƒç”¨thinkæ–¹æ³•å¹¶è¿”å›Messageæ ¼å¼
        if mode == "think":
            think_result = await self._think(messages, **kwargs)
            
            # åªè¿”å›æœ€ç»ˆå†…å®¹ï¼Œæ¨ç†è¿‡ç¨‹é€šè¿‡metadataä¼ é€’
            return Message(
                role=MessageRole.ASSISTANT,
                content=think_result.content,
                metadata={
                    **think_result.metadata,
                    "mode": "think",
                    "has_reasoning": bool(think_result.reasoning_content),
                    "reasoning_content": think_result.reasoning_content
                }
            )
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        formatted_messages = self._convert_messages(messages)
        
        # æ„å»ºè¯·æ±‚æ•°æ®
        data = {
            "model": self.config.model_name,
            "messages": formatted_messages,
            "temperature": kwargs.get("temperature", self.config.temperature),
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
        }
        
        # æ·»åŠ é¢å¤–å‚æ•°
        if self.config.extra_params:
            data.update(self.config.extra_params)
            
        # å‘é€è¯·æ±‚
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.config.api_base}/chat/completions",
                headers=headers,
                json=data,
                timeout=aiohttp.ClientTimeout(total=self.config.timeout)
            ) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"è±†åŒ…APIè°ƒç”¨å¤±è´¥: {error_text}")
                    
                result = await response.json()
                
                # æå–å›å¤å†…å®¹
                content = result["choices"][0]["message"]["content"]
                
                # åˆ›å»ºå“åº”æ¶ˆæ¯
                return Message(
                    role=MessageRole.ASSISTANT,
                    content=content,
                    metadata={
                        "model": self.config.model_name,
                        "usage": result.get("usage", {}),
                        "finish_reason": result["choices"][0].get("finish_reason")
                    }
                )
                
    async def stream_generate(self,
                                           messages: List[Message],
                                           mode: str = "normal",
                                           interrupt_checker=None,
                                           **kwargs) -> AsyncIterator[str]:
        """æ”¯æŒä¸­æ–­æ£€æŸ¥çš„æµå¼ç”Ÿæˆ
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            mode: ç”Ÿæˆæ¨¡å¼ï¼Œ'normal' æˆ– 'think'
            interrupt_checker: ä¸­æ–­æ£€æŸ¥å™¨ï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
        """
        # å¦‚æœæ˜¯thinkæ¨¡å¼ï¼Œä½¿ç”¨æµå¼think
        if mode == "think":
            accumulated_reasoning = ""
            accumulated_content = ""
            
            async for result in self._stream_think(messages, **kwargs):
                if result.get("type") == "reasoning_chunk":
                    reasoning_chunk = result.get("content", "")
                    accumulated_reasoning += reasoning_chunk
                    # è¾“å‡ºæ¨ç†è¿‡ç¨‹
                    if not accumulated_content:  # ç¬¬ä¸€æ¬¡è¾“å‡ºæ—¶æ·»åŠ æ ‡é¢˜
                        if not accumulated_reasoning.startswith("**ğŸ§  æ¨ç†è¿‡ç¨‹ï¼š**"):
                            yield "**ğŸ§  æ¨ç†è¿‡ç¨‹ï¼š**\n"
                    yield reasoning_chunk
                
                elif result.get("type") == "content_chunk":
                    content_chunk = result.get("content", "")
                    if not accumulated_content:  # ç¬¬ä¸€æ¬¡è¾“å‡ºå†…å®¹æ—¶æ·»åŠ åˆ†éš”ç¬¦
                        yield "\n\n**ç»“è®ºï¼š**\n"
                    accumulated_content += content_chunk
                    yield content_chunk
                
                elif result.get("type") == "think_complete":
                    # thinkå®Œæˆï¼Œä¸éœ€è¦é¢å¤–è¾“å‡º
                    break
                
                # ä¸­æ–­æ£€æŸ¥
                if interrupt_checker and interrupt_checker(accumulated_reasoning + accumulated_content):
                    break
            return
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        formatted_messages = self._convert_messages(messages)
        
        # æ„å»ºè¯·æ±‚æ•°æ®
        data = {
            "model": self.config.model_name,
            "messages": formatted_messages,
            "temperature": kwargs.get("temperature", self.config.temperature),
            "max_tokens": kwargs.get("max_tokens", self.config.max_tokens),
            "stream": True,
            "stream_options": {
                "include_usage": True
            }
        }
        
        # æ·»åŠ é¢å¤–å‚æ•°
        if self.config.extra_params:
            data.update(self.config.extra_params)
            
        # å‘é€æµå¼è¯·æ±‚
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.config.api_base}/chat/completions",
                headers=headers,
                json=data,
                timeout=aiohttp.ClientTimeout(total=self.config.timeout)
            ) as response:
                if response.status != 200:
                    error_text = await response.text()
                    raise Exception(f"è±†åŒ…APIè°ƒç”¨å¤±è´¥: {error_text}")
                    
                # ç´¯ç§¯å†…å®¹ç”¨äºä¸­æ–­æ£€æŸ¥
                accumulated_content = ""
                
                # å¤„ç†æµå¼å“åº”
                async for line in response.content:
                    line_text = line.decode('utf-8').strip()
                    if not line_text:
                        continue
                        
                    if line_text.startswith("data: "):
                        line_text = line_text[6:]
                        
                    if line_text == "[DONE]":
                        break
                        
                    try:
                        chunk = json.loads(line_text)
                        
                        # æå–å¢é‡å†…å®¹
                        if chunk.get("choices") and len(chunk["choices"]) > 0:
                            choice = chunk["choices"][0]
                            if choice.get("delta") and choice["delta"].get("content"):
                                content_chunk = choice["delta"]["content"]
                                accumulated_content += content_chunk
                                
                                # å¦‚æœæä¾›äº†ä¸­æ–­æ£€æŸ¥å™¨ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ä¸­æ–­
                                if interrupt_checker and interrupt_checker(accumulated_content):
                                    # å‘é€å½“å‰chunkåä¸­æ–­
                                    yield content_chunk
                                    break
                                
                                yield content_chunk
                                
                    except json.JSONDecodeError:
                        continue
                        
    async def call_llm(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.7) -> tuple[bool, str]:
        """ç»Ÿä¸€LLMè°ƒç”¨æ¥å£ï¼Œç”¨äºæ‰¹å¤„ç†å™¨"""
        try:
            # å°†promptè½¬æ¢ä¸ºMessageæ ¼å¼
            messages = [Message(role=MessageRole.USER, content=prompt)]
            
            # è°ƒç”¨generateæ–¹æ³•
            response = await self.generate(
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            return True, response.content
            
        except Exception as e:
            return False, str(e)
    
    def count_tokens(self, text: str) -> int:
        """ä¼°ç®—tokenæ•°é‡"""
        # ä¸­æ–‡å¤§çº¦1.5ä¸ªå­—ç¬¦ä¸€ä¸ªtokenï¼Œè‹±æ–‡å¤§çº¦4ä¸ªå­—ç¬¦ä¸€ä¸ªtoken
        # è¿™é‡Œä½¿ç”¨ç®€å•çš„ä¼°ç®—
        chinese_count = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        english_count = len(text) - chinese_count
        
        return int(chinese_count / 1.5 + english_count / 4)


# æ³¨å†Œåˆ°å·¥å‚
LLMFactory.register("doubao", DoubaoLLM) 