#!/usr/bin/env python3
"""
ç³»ç»Ÿçº§æ‰¹å¤„ç†å™¨ - ReactAgentçš„é€šç”¨æ‰¹å¤„ç†åŠŸèƒ½
æ”¯æŒå‰ç«¯é…ç½®ã€CSVè§£æã€LLMæŒ‡ä»¤ç”Ÿæˆã€å¹¶å‘Agentæ‰§è¡Œ
æ”¯æŒå¹¶è¡Œ/éå†ä¸¤ç§å¤„ç†æ¨¡å¼å’Œå®æ—¶è¿›åº¦å±•ç¤º
"""
import os
import csv
import json
import asyncio
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional, AsyncIterator
from dataclasses import dataclass
from pathlib import Path
from enum import Enum

logger = logging.getLogger(__name__)


class ProcessingMode(Enum):
    """å¤„ç†æ¨¡å¼æšä¸¾"""
    PARALLEL = "parallel"      # å¹¶è¡Œæ¨¡å¼ - å¿«é€Ÿé«˜æ•ˆ
    SEQUENTIAL = "sequential"  # éå†æ¨¡å¼ - é¡ºåºæ‰§è¡Œ


@dataclass
class BatchConfig:
    """æ‰¹å¤„ç†é…ç½®"""
    enabled: bool = False
    csv_file_path: Optional[str] = None
    batch_size: int = 20
    concurrent_tasks: int = 5
    max_rows: int = 1000  # æœ€å¤§å¤„ç†è¡Œæ•°é™åˆ¶
    processing_mode: ProcessingMode = ProcessingMode.PARALLEL  # å¤„ç†æ¨¡å¼


@dataclass
class BatchProgress:
    """æ‰¹å¤„ç†è¿›åº¦ä¿¡æ¯"""
    total_tasks: int = 0
    completed_tasks: int = 0
    successful_tasks: int = 0
    failed_tasks: int = 0
    current_batch: int = 0
    total_batches: int = 0
    current_task_description: str = ""
    start_time: Optional[datetime] = None
    estimated_completion: Optional[datetime] = None
    average_task_time: float = 0.0
    
    @property
    def progress_percentage(self) -> float:
        """è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”"""
        if self.total_tasks == 0:
            return 0.0
        return (self.completed_tasks / self.total_tasks) * 100
    
    @property
    def success_rate(self) -> float:
        """è®¡ç®—æˆåŠŸç‡"""
        if self.completed_tasks == 0:
            return 0.0
        return (self.successful_tasks / self.completed_tasks) * 100


@dataclass
class BatchInstruction:
    """æ‰¹å¤„ç†æŒ‡ä»¤"""
    task_type: str
    batch_description: str
    per_row_template: str
    total_rows: int
    expected_output: str


class CSVDataManager:
    """CSVæ•°æ®ç®¡ç†å™¨"""
    
    @staticmethod
    def detect_encoding(file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶ç¼–ç """
        try:
            import chardet
            with open(file_path, 'rb') as f:
                raw_data = f.read(10000)  # è¯»å–å‰10KBç”¨äºæ£€æµ‹
                result = chardet.detect(raw_data)
                detected_encoding = result.get('encoding', 'utf-8')
                confidence = result.get('confidence', 0)
                
                # å¦‚æœç½®ä¿¡åº¦å¤ªä½ï¼Œä½¿ç”¨é»˜è®¤ç¼–ç 
                if confidence < 0.5:
                    detected_encoding = 'utf-8'
                    
                logger.info(f"æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {detected_encoding} (ç½®ä¿¡åº¦: {confidence:.2f})")
                return detected_encoding
        except ImportError:
            logger.warning("chardetæœªå®‰è£…ï¼Œä½¿ç”¨utf-8ç¼–ç ")
            return 'utf-8'
        except Exception as e:
            logger.warning(f"ç¼–ç æ£€æµ‹å¤±è´¥: {e}ï¼Œä½¿ç”¨utf-8ç¼–ç ")
            return 'utf-8'
    
    @staticmethod
    def validate_and_parse_csv(file_path: str) -> tuple[bool, str, List[Dict[str, Any]], Dict[str, Any]]:
        """éªŒè¯å¹¶è§£æCSVæ–‡ä»¶ï¼Œè¿”å›æ•°æ®å’Œæ–‡ä»¶ç»“æ„ä¿¡æ¯"""
        try:
            if not os.path.exists(file_path):
                return False, f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}", [], {}
            
            # æ£€æµ‹ç¼–ç 
            detected_encoding = CSVDataManager.detect_encoding(file_path)
            
            # å°è¯•å¤šç§ç¼–ç è¯»å–CSV
            csv_data = []
            fieldnames = []
            successful_encoding = None
            
            encodings_to_try = [
                detected_encoding, 
                'utf-8-sig', 
                'utf-8', 
                'gbk', 
                'gb2312', 
                'gb18030',
                'big5',
                'latin1',
                'cp1252'
            ]
            
            # å»é‡å¹¶ä¿æŒé¡ºåº
            encodings_to_try = list(dict.fromkeys(encodings_to_try))
            
            for encoding in encodings_to_try:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        reader = csv.DictReader(f)
                        
                        # è·å–åˆ—å
                        fieldnames = reader.fieldnames
                        if not fieldnames:
                            continue
                        
                        # è¯»å–æ•°æ®è¡Œ
                        csv_data = []
                        for index, row in enumerate(reader):
                            # æ¸…ç†æ•°æ®ä¸­çš„BOMå’Œç‰¹æ®Šå­—ç¬¦
                            cleaned_row = {}
                            for key, value in row.items():
                                # æ¸…ç†åˆ—åä¸­çš„BOM
                                clean_key = key.replace('\ufeff', '').strip() if key else ''
                                # æ¸…ç†å€¼ä¸­çš„ç‰¹æ®Šå­—ç¬¦
                                clean_value = value.strip() if value else ''
                                cleaned_row[clean_key] = clean_value
                            
                            # æ·»åŠ è¡Œç´¢å¼•
                            cleaned_row['_row_index'] = index + 1
                            csv_data.append(cleaned_row)
                        
                        successful_encoding = encoding
                        break
                        
                except (UnicodeDecodeError, UnicodeError):
                    continue
                except Exception as e:
                    logger.warning(f"ä½¿ç”¨ç¼–ç  {encoding} è¯»å–å¤±è´¥: {e}")
                    continue
            
            if not successful_encoding:
                return False, "æ— æ³•ä½¿ç”¨ä»»ä½•ç¼–ç è§£æCSVæ–‡ä»¶", [], {}
            
            if not fieldnames:
                return False, "CSVæ–‡ä»¶æ²¡æœ‰åˆ—å¤´", [], {}
            
            if not csv_data:
                return False, "CSVæ–‡ä»¶æ²¡æœ‰æ•°æ®è¡Œ", [], {}
            
            # æ¸…ç†åˆ—åä¸­çš„BOMå’Œç‰¹æ®Šå­—ç¬¦
            clean_fieldnames = [col.replace('\ufeff', '').strip() for col in fieldnames]
            
            # ç”Ÿæˆæ–‡ä»¶ç»“æ„ä¿¡æ¯
            structure_info = CSVDataManager.get_csv_structure_info(csv_data, clean_fieldnames)
            structure_info['detected_encoding'] = successful_encoding
            structure_info['file_size'] = os.path.getsize(file_path)
            
            success_msg = f"æˆåŠŸè§£æ{len(csv_data)}è¡Œæ•°æ®ï¼Œä½¿ç”¨ç¼–ç : {successful_encoding}ï¼Œåˆ—: {clean_fieldnames}"
            logger.info(success_msg)
            
            return True, success_msg, csv_data, structure_info
            
        except Exception as e:
            error_msg = f"CSVè§£æé”™è¯¯: {str(e)}"
            logger.error(error_msg)
            return False, error_msg, [], {}
    
    @staticmethod
    def get_csv_structure_info(csv_data: List[Dict[str, Any]], fieldnames: List[str] = None) -> Dict[str, Any]:
        """è·å–CSVç»“æ„ä¿¡æ¯ç”¨äºLLMåˆ†æ"""
        if not csv_data:
            return {}
        
        # è·å–åˆ—ä¿¡æ¯
        sample_row = csv_data[0]
        if fieldnames:
            columns = fieldnames
        else:
            columns = [col for col in sample_row.keys() if not col.startswith('_')]
        
        # è·å–æ•°æ®æ ·ä¾‹å’Œæ•°æ®ç±»å‹åˆ†æ
        sample_data = {}
        column_types = {}
        for col in columns:
            values = [row.get(col, '') for row in csv_data[:5]]  # å–å‰5è¡Œä½œä¸ºæ ·ä¾‹
            sample_data[col] = values
            
            # ç®€å•çš„æ•°æ®ç±»å‹æ¨æ–­
            non_empty_values = [v for v in values if v and str(v).strip()]
            if non_empty_values:
                # æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å­—
                try:
                    float(non_empty_values[0])
                    column_types[col] = 'numeric'
                except ValueError:
                    # æ£€æŸ¥æ˜¯å¦ä¸ºæ—¥æœŸ
                    if any(keyword in str(non_empty_values[0]).lower() for keyword in ['æ—¥æœŸ', 'date', 'æ—¶é—´', 'time']):
                        column_types[col] = 'datetime'
                    else:
                        column_types[col] = 'text'
            else:
                column_types[col] = 'unknown'
        
        return {
            "total_rows": len(csv_data),
            "columns": columns,
            "column_types": column_types,
            "sample_data": sample_data,
            "first_row_example": {col: csv_data[0].get(col, '') for col in columns},
            # æ·»åŠ å­—æ®µé€‰æ‹©çš„é»˜è®¤é…ç½®
            "field_selection": {col: True for col in columns},  # é»˜è®¤å…¨é€‰
            "field_descriptions": {col: f"{col} - {column_types.get(col, 'unknown')}" for col in columns}
        }


class BatchInstructionGenerator:
    """æ‰¹å¤„ç†æŒ‡ä»¤ç”Ÿæˆå™¨ - ä½¿ç”¨LLMåˆ†æç”¨æˆ·æ„å›¾å¹¶ç”Ÿæˆæ‰¹å¤„ç†æŒ‡ä»¤"""
    
    def __init__(self, llm_caller=None):
        self.llm_caller = llm_caller
    
    async def generate_batch_instruction(self, user_message: str, csv_structure: Dict[str, Any]) -> BatchInstruction:
        """æ ¹æ®ç”¨æˆ·æ¶ˆæ¯å’ŒCSVç»“æ„ç”Ÿæˆæ‰¹å¤„ç†æŒ‡ä»¤"""
        
        # æ„å»ºLLMæç¤ºè¯
        prompt = self._build_instruction_prompt(user_message, csv_structure)
        
        try:
            if self.llm_caller:
                # è°ƒç”¨LLMç”ŸæˆæŒ‡ä»¤
                success, response = await self.llm_caller.call_llm(prompt,  temperature=0.3)
                
                if success:
                    # è§£æLLMå“åº”
                    logger.info(f"æ‰¹å¤„ç†LLMå“åº”çš„é€šç”¨æŒ‡ä»¤: {response}")
                    return self._parse_llm_response(response, csv_structure)
                else:
                    logger.warning(f"LLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æŒ‡ä»¤: {response}")
            
            # å¦‚æœLLMä¸å¯ç”¨ï¼Œç”Ÿæˆé»˜è®¤æŒ‡ä»¤
            return self._generate_default_instruction(user_message, csv_structure)
            
        except Exception as e:
            logger.error(f"æ‰¹å¤„ç†æŒ‡ä»¤ç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_default_instruction(user_message, csv_structure)
    
    def _build_instruction_prompt(self, user_message: str, csv_structure: Dict[str, Any]) -> str:
        """æ„å»ºLLMæç¤ºè¯"""
        return f"""ä½ æ˜¯ä¸€ä¸ªæ‰¹å¤„ç†ä»»åŠ¡åˆ†æä¸“å®¶ã€‚ç”¨æˆ·æƒ³è¦å¯¹CSVæ•°æ®è¿›è¡Œæ‰¹é‡å¤„ç†ï¼Œè¯·åˆ†æç”¨æˆ·æ„å›¾å¹¶ç”Ÿæˆæ‰¹å¤„ç†æŒ‡ä»¤ã€‚

ç”¨æˆ·æ¶ˆæ¯: "{user_message}"

CSVæ•°æ®ç»“æ„:
- æ€»è¡Œæ•°: {csv_structure.get('total_rows', 0)}
- åˆ—å: {csv_structure.get('columns', [])}
- æ•°æ®æ ·ä¾‹: {csv_structure.get('first_row_example', {})}

è¯·åˆ†æç”¨æˆ·æ„å›¾ï¼Œç”ŸæˆJSONæ ¼å¼çš„æ‰¹å¤„ç†æŒ‡ä»¤ï¼š
{{
    "task_type": "ä»»åŠ¡ç±»å‹(å¦‚: schedule_generation, content_creation, data_analysisç­‰)",
    "batch_description": "æ‰¹å¤„ç†ä»»åŠ¡çš„æ€»ä½“æè¿°",
    "per_row_template": "å•è¡Œå¤„ç†æ¨¡æ¿ï¼Œä½¿ç”¨{{åˆ—å}}å ä½ç¬¦ï¼Œå¦‚: ä¸ºè§’è‰²{{character_name}}ç”Ÿæˆ{{duration_days}}å¤©çš„æ—¥ç¨‹",
    "expected_output": "æœŸæœ›çš„è¾“å‡ºæ ¼å¼æè¿°"
}}

æ³¨æ„ï¼š
1. per_row_templateä¸­çš„å ä½ç¬¦å¿…é¡»ä¸CSVåˆ—åå®Œå…¨åŒ¹é…
2. è¦ä½“ç°ç”¨æˆ·çš„å…·ä½“éœ€æ±‚
3. ç¡®ä¿æŒ‡ä»¤æ¸…æ™°ã€å¯æ‰§è¡Œ
4. åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—

JSON:"""
    
    def _parse_llm_response(self, response: str, csv_structure: Dict[str, Any]) -> BatchInstruction:
        """è§£æLLMå“åº”"""
        try:
            # å°è¯•è§£æJSON
            response = response.strip()
            if response.startswith('```json'):
                response = response[7:]
            if response.endswith('```'):
                response = response[:-3]
            response = response.strip()
            
            instruction_data = json.loads(response)
            
            return BatchInstruction(
                task_type=instruction_data.get('task_type', 'general_processing'),
                batch_description=instruction_data.get('batch_description', 'æ‰¹é‡å¤„ç†ä»»åŠ¡'),
                per_row_template=instruction_data.get('per_row_template', 'å¤„ç†æ•°æ®è¡Œ'),
                total_rows=csv_structure.get('total_rows', 0),
                expected_output=instruction_data.get('expected_output', 'å¤„ç†ç»“æœ')
            )
            
        except Exception as e:
            logger.warning(f"LLMå“åº”è§£æå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æŒ‡ä»¤")
            return self._generate_default_instruction("æ‰¹é‡å¤„ç†", csv_structure)
    
    def _generate_default_instruction(self, user_message: str, csv_structure: Dict[str, Any]) -> BatchInstruction:
        """ç”Ÿæˆé»˜è®¤æ‰¹å¤„ç†æŒ‡ä»¤"""
        columns = csv_structure.get('columns', [])
        
        # æ ¹æ®å¸¸è§åˆ—åæ¨æµ‹ä»»åŠ¡ç±»å‹
        if any('name' in col.lower() for col in columns):
            task_type = 'character_processing'
            template = f"å¤„ç†{{{columns[0] if columns else 'data'}}}"
        else:
            task_type = 'general_processing'
            template = "å¤„ç†CSVæ•°æ®è¡Œ"
        
        return BatchInstruction(
            task_type=task_type,
            batch_description=f"æ ¹æ®ç”¨æˆ·éœ€æ±‚è¿›è¡Œæ‰¹é‡å¤„ç†: {user_message}",
            per_row_template=template,
            total_rows=csv_structure.get('total_rows', 0),
            expected_output="æ‰¹é‡å¤„ç†ç»“æœ"
        )


class ReactAgentTaskExecutor:
    """ReactAgentä»»åŠ¡æ‰§è¡Œå™¨ - æ¨¡æ‹ŸReactAgentå¤„ç†å•ä¸ªä»»åŠ¡"""
    
    def __init__(self, mcp_tool_manager=None):
        self.mcp_tool_manager = mcp_tool_manager
    
    async def execute_single_task(self, task_prompt: str, row_data: Dict[str, Any], row_index: int) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªReactAgentä»»åŠ¡"""
        start_time = datetime.now()
        
        try:
            logger.info(f"æ‰§è¡Œç¬¬{row_index}è¡Œä»»åŠ¡: {task_prompt[:50]}...")
            
            # æ¨¡æ‹ŸReactAgentçš„æ€è€ƒå’Œè¡ŒåŠ¨è¿‡ç¨‹
            result = await self._simulate_react_process(task_prompt, row_data)
            
            execution_time = (datetime.now() - start_time).total_seconds()
            
            return {
                "row_index": row_index,
                "success": True,
                "task_prompt": task_prompt,
                "result": result,
                "execution_time": execution_time,
                "processed_at": datetime.now().isoformat(),
                "row_data": row_data
            }
            
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            
            return {
                "row_index": row_index,
                "success": False,
                "task_prompt": task_prompt,
                "error": str(e),
                "execution_time": execution_time,
                "processed_at": datetime.now().isoformat(),
                "row_data": row_data
            }
    
    async def _simulate_react_process(self, task_prompt: str, row_data: Dict[str, Any]) -> str:
        """æ¨¡æ‹ŸReactAgentçš„Reactè¿‡ç¨‹"""
        
        # å¦‚æœæœ‰MCPå·¥å…·ç®¡ç†å™¨ï¼Œå°è¯•è°ƒç”¨ç›¸åº”çš„å·¥å…·
        if self.mcp_tool_manager:
            try:
                # æ£€æŸ¥æ˜¯å¦æ˜¯è§’è‰²æ—¥ç¨‹ç”Ÿæˆä»»åŠ¡
                if any(keyword in task_prompt.lower() for keyword in ['æ—¥ç¨‹', 'schedule', 'è®¡åˆ’', 'å®‰æ’']):
                    return await self._handle_schedule_task(task_prompt, row_data)
                
                # å…¶ä»–ç±»å‹çš„ä»»åŠ¡å¯ä»¥åœ¨è¿™é‡Œæ‰©å±•
                return await self._handle_general_task(task_prompt, row_data)
                
            except Exception as e:
                logger.warning(f"MCPå·¥å…·è°ƒç”¨å¤±è´¥ï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœ: {e}")
        
        # è¿”å›æ¨¡æ‹Ÿçš„å¤„ç†ç»“æœ
        return f"æ¨¡æ‹Ÿå¤„ç†ç»“æœ: åŸºäº '{task_prompt}' å¯¹æ•°æ® {row_data} çš„å¤„ç†å®Œæˆ"
    
    async def _handle_schedule_task(self, task_prompt: str, row_data: Dict[str, Any]) -> str:
        """å¤„ç†æ—¥ç¨‹ç”Ÿæˆä»»åŠ¡ï¼ˆå·²æ”¹ä¸ºé€šç”¨ä»»åŠ¡å¤„ç†ï¼‰"""
        # è§’è‰²æ‰®æ¼”æœåŠ¡å™¨å·²ç§»é™¤ï¼Œä½¿ç”¨é€šç”¨ä»»åŠ¡å¤„ç†
        return await self._handle_general_task(task_prompt, row_data)
    
    async def _handle_general_task(self, task_prompt: str, row_data: Dict[str, Any]) -> str:
        """å¤„ç†é€šç”¨ä»»åŠ¡"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒç”¨ä¸åŒçš„MCPå·¥å…·
        return f"é€šç”¨ä»»åŠ¡å¤„ç†: {task_prompt}"


class BatchProcessor:
    """ç³»ç»Ÿçº§æ‰¹å¤„ç†å™¨ä¸»æ§åˆ¶å™¨"""
    
    def __init__(self, llm_caller=None, mcp_tool_manager=None):
        self.config = BatchConfig()
        self.csv_data: List[Dict[str, Any]] = []
        self.csv_structure: Dict[str, Any] = {}  # å­˜å‚¨CSVç»“æ„ä¿¡æ¯
        self.instruction_generator = BatchInstructionGenerator(llm_caller)
        self.task_executor = ReactAgentTaskExecutor(mcp_tool_manager)
        self.current_batch_task = None
        self.current_progress = BatchProgress()  # å½“å‰è¿›åº¦çŠ¶æ€
    
    def configure_batch_mode(self, enabled: bool, csv_file_path: str = None, 
                           batch_size: int = 20, concurrent_tasks: int = 5,
                           processing_mode: str = "parallel") -> Dict[str, Any]:
        """é…ç½®æ‰¹å¤„ç†æ¨¡å¼"""
        self.config.enabled = enabled
        self.config.batch_size = batch_size
        self.config.concurrent_tasks = concurrent_tasks
        
        # è®¾ç½®å¤„ç†æ¨¡å¼
        try:
            self.config.processing_mode = ProcessingMode(processing_mode)
        except ValueError:
            self.config.processing_mode = ProcessingMode.PARALLEL
            logger.warning(f"æ— æ•ˆçš„å¤„ç†æ¨¡å¼: {processing_mode}ï¼Œä½¿ç”¨é»˜è®¤å¹¶è¡Œæ¨¡å¼")
        
        if enabled and csv_file_path:
            # éªŒè¯å’Œè§£æCSV
            success, message, csv_data, structure_info = CSVDataManager.validate_and_parse_csv(csv_file_path)
            
            if success:
                self.config.csv_file_path = csv_file_path
                self.csv_data = csv_data
                self.csv_structure = structure_info
                
                return {
                    "success": True,
                    "message": f"æ‰¹å¤„ç†æ¨¡å¼å·²å¯ç”¨: {message}",
                    "csv_rows": len(csv_data),
                    "csv_structure": structure_info,
                    "config": {
                        "batch_size": batch_size,
                        "concurrent_tasks": concurrent_tasks,
                        "processing_mode": processing_mode
                    }
                }
            else:
                self.config.enabled = False
                return {
                    "success": False,
                    "message": f"æ‰¹å¤„ç†æ¨¡å¼å¯ç”¨å¤±è´¥: {message}"
                }
        
        elif not enabled:
            self.config.csv_file_path = None
            self.csv_data = []
            return {
                "success": True,
                "message": "æ‰¹å¤„ç†æ¨¡å¼å·²å…³é—­"
            }
        
        return {
            "success": False,
            "message": "é…ç½®å‚æ•°ä¸å®Œæ•´"
        }
    
    def is_batch_mode_enabled(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ç”¨æ‰¹å¤„ç†æ¨¡å¼"""
        return self.config.enabled and bool(self.csv_data)
    
    async def process_batch_request_with_progress(self, user_message: str) -> AsyncIterator[Dict[str, Any]]:
        """å¤„ç†æ‰¹å¤„ç†è¯·æ±‚å¹¶æä¾›æµå¼è¿›åº¦æ›´æ–°"""
        if not self.is_batch_mode_enabled():
            yield {
                "type": "error",
                "content": "âŒ æ‰¹å¤„ç†æ¨¡å¼æœªå¯ç”¨æˆ–CSVæ•°æ®æœªåŠ è½½"
            }
            return
        
        try:
            # 1. ç”Ÿæˆæ‰¹å¤„ç†æŒ‡ä»¤
            yield {
                "type": "progress",
                "content": "ğŸ§  æ­£åœ¨åˆ†æä»»åŠ¡éœ€æ±‚å¹¶ç”Ÿæˆæ‰¹å¤„ç†æŒ‡ä»¤...",
                "stage": "instruction_generation"
            }
            
            csv_structure = CSVDataManager.get_csv_structure_info(self.csv_data)
            batch_instruction = await self.instruction_generator.generate_batch_instruction(
                user_message, csv_structure
            )
            
            logger.info(f"ç”Ÿæˆæ‰¹å¤„ç†æŒ‡ä»¤: {batch_instruction.batch_description}")
            
            # åˆå§‹åŒ–è¿›åº¦
            self.current_progress = BatchProgress(
                total_tasks=len(self.csv_data),
                start_time=datetime.now()
            )
            
            yield {
                "type": "instruction_generated",
                "content": f"ğŸ“‹ **æ‰¹å¤„ç†æŒ‡ä»¤å·²ç”Ÿæˆ**\n\n"
                          f"**ä»»åŠ¡ç±»å‹**: {batch_instruction.task_type}\n"
                          f"**ä»»åŠ¡æè¿°**: {batch_instruction.batch_description}\n"
                          f"**å¤„ç†æ¨¡æ¿**: {batch_instruction.per_row_template}\n"
                          f"**æ€»ä»»åŠ¡æ•°**: {self.current_progress.total_tasks}\n"
                          f"**å¤„ç†æ¨¡å¼**: {'å¹¶è¡Œæ¨¡å¼' if self.config.processing_mode == ProcessingMode.PARALLEL else 'é¡ºåºæ¨¡å¼'}\n\n"
                          f"ğŸš€ å¼€å§‹æ‰§è¡Œæ‰¹å¤„ç†ä»»åŠ¡...",
                "instruction": batch_instruction
            }
            
            # 2. æ ¹æ®æ¨¡å¼æ‰§è¡Œæ‰¹å¤„ç†ä»»åŠ¡
            if self.config.processing_mode == ProcessingMode.PARALLEL:
                async for progress_data in self._execute_batch_tasks_parallel(batch_instruction):
                    yield progress_data
            else:
                async for progress_data in self._execute_batch_tasks_sequential(batch_instruction):
                    yield progress_data
            
            # 3. ç”Ÿæˆæœ€ç»ˆæ±‡æ€»
            yield {
                "type": "final_summary",
                "content": self._generate_final_summary(),
                "progress": self.current_progress
            }
            
        except Exception as e:
            logger.error(f"æ‰¹å¤„ç†æ‰§è¡Œå¤±è´¥: {e}")
            yield {
                "type": "error",
                "content": f"âŒ æ‰¹å¤„ç†æ‰§è¡Œå¤±è´¥: {str(e)}"
            }
    
    async def process_batch_request(self, user_message: str) -> Dict[str, Any]:
        """å¤„ç†æ‰¹å¤„ç†è¯·æ±‚ - å…¼å®¹åŸæœ‰æ¥å£"""
        if not self.is_batch_mode_enabled():
            return {
                "success": False,
                "message": "æ‰¹å¤„ç†æ¨¡å¼æœªå¯ç”¨æˆ–CSVæ•°æ®æœªåŠ è½½"
            }
        
        try:
            # 1. ç”Ÿæˆæ‰¹å¤„ç†æŒ‡ä»¤
            csv_structure = CSVDataManager.get_csv_structure_info(self.csv_data)
            batch_instruction = await self.instruction_generator.generate_batch_instruction(
                user_message, csv_structure
            )
            
            logger.info(f"ç”Ÿæˆæ‰¹å¤„ç†æŒ‡ä»¤: {batch_instruction.batch_description}")
            
            # 2. æ‰§è¡Œæ‰¹å¤„ç†ä»»åŠ¡
            results = await self._execute_batch_tasks(batch_instruction)
            
            # 3. æ±‡æ€»ç»“æœ
            summary = self._summarize_results(results, batch_instruction)
            
            return {
                "success": True,
                "batch_instruction": {
                    "task_type": batch_instruction.task_type,
                    "description": batch_instruction.batch_description,
                    "template": batch_instruction.per_row_template
                },
                "execution_summary": summary,
                "detailed_results": results
            }
            
        except Exception as e:
            logger.error(f"æ‰¹å¤„ç†æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"æ‰¹å¤„ç†æ‰§è¡Œå¤±è´¥: {str(e)}"
            }
    
    async def _execute_batch_tasks_parallel(self, instruction: BatchInstruction) -> AsyncIterator[Dict[str, Any]]:
        """å¹¶è¡Œæ¨¡å¼æ‰§è¡Œæ‰¹å¤„ç†ä»»åŠ¡"""
        all_results = []
        
        # è®¡ç®—æ€»æ‰¹æ¬¡æ•°
        total_batches = (len(self.csv_data) + self.config.batch_size - 1) // self.config.batch_size
        self.current_progress.total_batches = total_batches
        
        # åˆ†æ‰¹å¤„ç†
        for batch_idx, batch_start in enumerate(range(0, len(self.csv_data), self.config.batch_size)):
            batch_end = min(batch_start + self.config.batch_size, len(self.csv_data))
            batch_data = self.csv_data[batch_start:batch_end]
            
            self.current_progress.current_batch = batch_idx + 1
            
            yield {
                "type": "batch_start",
                "content": f"ğŸ“¦ å¼€å§‹å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹æ¬¡ (ç¬¬{batch_start+1}-{batch_end}è¡Œ)",
                "batch_info": {
                    "batch_index": batch_idx + 1,
                    "total_batches": total_batches,
                    "batch_start": batch_start + 1,
                    "batch_end": batch_end,
                    "batch_size": len(batch_data)
                }
            }
            
            # å¹¶å‘æ‰§è¡Œå½“å‰æ‰¹æ¬¡
            semaphore = asyncio.Semaphore(self.config.concurrent_tasks)
            
            async def execute_with_semaphore(row_data):
                async with semaphore:
                    # ç”Ÿæˆå…·ä½“çš„ä»»åŠ¡æç¤ºè¯
                    task_prompt = self._generate_task_prompt(instruction.per_row_template, row_data)
                    
                    # æ‰§è¡Œå•ä¸ªä»»åŠ¡
                    return await self.task_executor.execute_single_task(
                        task_prompt, row_data, row_data.get('_row_index', 0)
                    )
            
            # å¹¶å‘æ‰§è¡Œå½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰ä»»åŠ¡
            batch_results = await asyncio.gather(
                *[execute_with_semaphore(row_data) for row_data in batch_data],
                return_exceptions=True
            )
            
            # å¤„ç†ç»“æœå¹¶æ›´æ–°è¿›åº¦
            for result in batch_results:
                if isinstance(result, Exception):
                    result_data = {
                        "success": False,
                        "error": str(result),
                        "processed_at": datetime.now().isoformat()
                    }
                    self.current_progress.failed_tasks += 1
                else:
                    result_data = result
                    if result.get('success', False):
                        self.current_progress.successful_tasks += 1
                    else:
                        self.current_progress.failed_tasks += 1
                
                self.current_progress.completed_tasks += 1
                all_results.append(result_data)
                
                # æ›´æ–°å¹³å‡è€—æ—¶
                if result_data.get('execution_time'):
                    total_time = self.current_progress.average_task_time * (self.current_progress.completed_tasks - 1) + result_data.get('execution_time', 0)
                    self.current_progress.average_task_time = total_time / self.current_progress.completed_tasks
            
            # å‘é€æ‰¹æ¬¡å®Œæˆè¿›åº¦
            yield {
                "type": "batch_completed",
                "content": f"âœ… ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹æ¬¡å®Œæˆ - "
                          f"è¿›åº¦: {self.current_progress.progress_percentage:.1f}% "
                          f"({self.current_progress.completed_tasks}/{self.current_progress.total_tasks})",
                "progress": {
                    "percentage": self.current_progress.progress_percentage,
                    "completed": self.current_progress.completed_tasks,
                    "total": self.current_progress.total_tasks,
                    "successful": self.current_progress.successful_tasks,
                    "failed": self.current_progress.failed_tasks,
                    "success_rate": self.current_progress.success_rate,
                    "average_time": self.current_progress.average_task_time
                }
            }
        
        # ä¿å­˜ç»“æœä¾›æœ€ç»ˆæ±‡æ€»ä½¿ç”¨
        self.current_progress.results = all_results
    
    async def _execute_batch_tasks_sequential(self, instruction: BatchInstruction) -> AsyncIterator[Dict[str, Any]]:
        """é¡ºåºæ¨¡å¼æ‰§è¡Œæ‰¹å¤„ç†ä»»åŠ¡"""
        all_results = []
        self.current_progress.total_batches = 1
        self.current_progress.current_batch = 1
        
        yield {
            "type": "sequential_start",
            "content": f"ğŸ”„ å¼€å§‹é¡ºåºå¤„ç† {len(self.csv_data)} ä¸ªä»»åŠ¡..."
        }
        
        # é¡ºåºå¤„ç†æ¯ä¸ªä»»åŠ¡
        for idx, row_data in enumerate(self.csv_data):
            # ç”Ÿæˆå…·ä½“çš„ä»»åŠ¡æç¤ºè¯
            task_prompt = self._generate_task_prompt(instruction.per_row_template, row_data)
            row_index = row_data.get('_row_index', idx + 1)
            
            # æ›´æ–°å½“å‰ä»»åŠ¡æè¿°
            task_preview = task_prompt[:50] + "..." if len(task_prompt) > 50 else task_prompt
            self.current_progress.current_task_description = task_preview
            
            yield {
                "type": "task_start",
                "content": f"ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ {idx + 1}/{len(self.csv_data)} ä¸ªä»»åŠ¡\n"
                          f"**ä»»åŠ¡å†…å®¹**: {task_preview}\n"
                          f"**è¿›åº¦**: {((idx) / len(self.csv_data) * 100):.1f}%",
                "task_info": {
                    "task_index": idx + 1,
                    "total_tasks": len(self.csv_data),
                    "task_prompt": task_prompt,
                    "row_data": row_data
                }
            }
            
            # æ‰§è¡Œå•ä¸ªä»»åŠ¡
            try:
                result = await self.task_executor.execute_single_task(task_prompt, row_data, row_index)
                
                if result.get('success', False):
                    self.current_progress.successful_tasks += 1
                    status_icon = "âœ…"
                else:
                    self.current_progress.failed_tasks += 1
                    status_icon = "âŒ"
                
                # æ›´æ–°è¿›åº¦
                self.current_progress.completed_tasks += 1
                
                # æ›´æ–°å¹³å‡è€—æ—¶
                if result.get('execution_time'):
                    total_time = self.current_progress.average_task_time * (self.current_progress.completed_tasks - 1) + result.get('execution_time', 0)
                    self.current_progress.average_task_time = total_time / self.current_progress.completed_tasks
                
                all_results.append(result)
                
                # å‘é€ä»»åŠ¡å®ŒæˆçŠ¶æ€
                result_preview = ""
                if result.get('success') and result.get('result'):
                    result_content = str(result.get('result', ''))
                    result_preview = (result_content[:100] + "...") if len(result_content) > 100 else result_content
                elif result.get('error'):
                    result_preview = f"é”™è¯¯: {result.get('error', '')[:50]}..."
                
                yield {
                    "type": "task_completed",
                    "content": f"{status_icon} ç¬¬ {idx + 1}/{len(self.csv_data)} ä¸ªä»»åŠ¡å®Œæˆ\n"
                              f"**æ‰§è¡Œæ—¶é—´**: {result.get('execution_time', 0):.2f}ç§’\n"
                              f"**ç»“æœé¢„è§ˆ**: {result_preview}\n"
                              f"**æ€»ä½“è¿›åº¦**: {self.current_progress.progress_percentage:.1f}%",
                    "result": result,
                    "progress": {
                        "percentage": self.current_progress.progress_percentage,
                        "completed": self.current_progress.completed_tasks,
                        "total": self.current_progress.total_tasks,
                        "successful": self.current_progress.successful_tasks,
                        "failed": self.current_progress.failed_tasks,
                        "success_rate": self.current_progress.success_rate,
                        "average_time": self.current_progress.average_task_time
                    }
                }
                
            except Exception as e:
                # å¤„ç†å¼‚å¸¸
                result = {
                    "row_index": row_index,
                    "success": False,
                    "task_prompt": task_prompt,
                    "error": str(e),
                    "execution_time": 0.0,
                    "processed_at": datetime.now().isoformat(),
                    "row_data": row_data
                }
                
                self.current_progress.failed_tasks += 1
                self.current_progress.completed_tasks += 1
                all_results.append(result)
                
                yield {
                    "type": "task_error",
                    "content": f"âŒ ç¬¬ {idx + 1}/{len(self.csv_data)} ä¸ªä»»åŠ¡å¤±è´¥\n"
                              f"**é”™è¯¯**: {str(e)}\n"
                              f"**è¿›åº¦**: {self.current_progress.progress_percentage:.1f}%",
                    "error": str(e),
                    "task_info": {
                        "task_index": idx + 1,
                        "task_prompt": task_prompt
                    }
                }
        
        # ä¿å­˜ç»“æœä¾›æœ€ç»ˆæ±‡æ€»ä½¿ç”¨
        self.current_progress.results = all_results
    
    async def _execute_batch_tasks(self, instruction: BatchInstruction) -> List[Dict[str, Any]]:
        """æ‰§è¡Œæ‰¹å¤„ç†ä»»åŠ¡ - å…¼å®¹åŸæœ‰æ¥å£ï¼ˆå¹¶è¡Œæ¨¡å¼ï¼‰"""
        all_results = []
        
        # åˆ†æ‰¹å¤„ç†
        for batch_start in range(0, len(self.csv_data), self.config.batch_size):
            batch_end = min(batch_start + self.config.batch_size, len(self.csv_data))
            batch_data = self.csv_data[batch_start:batch_end]
            
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_start+1}-{batch_end}")
            
            # å¹¶å‘æ‰§è¡Œå½“å‰æ‰¹æ¬¡
            semaphore = asyncio.Semaphore(self.config.concurrent_tasks)
            
            async def execute_with_semaphore(row_data):
                async with semaphore:
                    # ç”Ÿæˆå…·ä½“çš„ä»»åŠ¡æç¤ºè¯
                    task_prompt = self._generate_task_prompt(instruction.per_row_template, row_data)
                    
                    # æ‰§è¡Œå•ä¸ªä»»åŠ¡
                    return await self.task_executor.execute_single_task(
                        task_prompt, row_data, row_data.get('_row_index', 0)
                    )
            
            # å¹¶å‘æ‰§è¡Œå½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰ä»»åŠ¡
            batch_results = await asyncio.gather(
                *[execute_with_semaphore(row_data) for row_data in batch_data],
                return_exceptions=True
            )
            
            # å¤„ç†ç»“æœ
            for result in batch_results:
                if isinstance(result, Exception):
                    all_results.append({
                        "success": False,
                        "error": str(result),
                        "processed_at": datetime.now().isoformat()
                    })
                else:
                    all_results.append(result)
        
        return all_results
    
    def _generate_task_prompt(self, template: str, row_data: Dict[str, Any]) -> str:
        """æ ¹æ®æ¨¡æ¿å’Œè¡Œæ•°æ®ç”Ÿæˆå…·ä½“çš„ä»»åŠ¡æç¤ºè¯"""
        try:
            # ä½¿ç”¨formatæ–¹æ³•æ›¿æ¢å ä½ç¬¦
            return template.format(**row_data)
        except KeyError as e:
            # å¦‚æœæ¨¡æ¿ä¸­çš„å ä½ç¬¦åœ¨æ•°æ®ä¸­ä¸å­˜åœ¨ï¼Œè®°å½•è­¦å‘Šå¹¶è¿”å›åŸæ¨¡æ¿
            logger.warning(f"æ¨¡æ¿å ä½ç¬¦ {e} åœ¨CSVæ•°æ®ä¸­ä¸å­˜åœ¨")
            return template
        except Exception as e:
            logger.error(f"ä»»åŠ¡æç¤ºè¯ç”Ÿæˆå¤±è´¥: {e}")
            return template
    
    def _summarize_results(self, results: List[Dict[str, Any]], instruction: BatchInstruction) -> Dict[str, Any]:
        """æ±‡æ€»æ‰¹å¤„ç†ç»“æœ"""
        total_tasks = len(results)
        successful_tasks = sum(1 for r in results if r.get('success', False))
        failed_tasks = total_tasks - successful_tasks
        
        total_time = sum(r.get('execution_time', 0) for r in results)
        avg_time = total_time / total_tasks if total_tasks > 0 else 0
        
        return {
            "task_type": instruction.task_type,
            "batch_description": instruction.batch_description,
            "total_tasks": total_tasks,
            "successful_tasks": successful_tasks,
            "failed_tasks": failed_tasks,
            "success_rate": f"{(successful_tasks / total_tasks * 100):.1f}%" if total_tasks > 0 else "0%",
            "total_execution_time": f"{total_time:.2f}ç§’",
            "average_task_time": f"{avg_time:.2f}ç§’",
            "completed_at": datetime.now().isoformat()
        }
    
    def _generate_final_summary(self) -> str:
        """ç”Ÿæˆæœ€ç»ˆæ±‡æ€»æŠ¥å‘Š"""
        progress = self.current_progress
        end_time = datetime.now()
        total_duration = (end_time - progress.start_time).total_seconds() if progress.start_time else 0
        
        summary = f"""ğŸ‰ **æ‰¹å¤„ç†ä»»åŠ¡å®Œæˆï¼**

ğŸ“Š **æ‰§è¡Œç»Ÿè®¡**:
- æ€»ä»»åŠ¡æ•°: {progress.total_tasks}
- æˆåŠŸä»»åŠ¡: {progress.successful_tasks}
- å¤±è´¥ä»»åŠ¡: {progress.failed_tasks}
- æˆåŠŸç‡: {progress.success_rate:.1f}%

â±ï¸ **æ—¶é—´ç»Ÿè®¡**:
- æ€»è€—æ—¶: {total_duration:.2f}ç§’
- å¹³å‡è€—æ—¶: {progress.average_task_time:.2f}ç§’/ä»»åŠ¡
- å¤„ç†æ¨¡å¼: {'å¹¶è¡Œæ¨¡å¼' if self.config.processing_mode == ProcessingMode.PARALLEL else 'é¡ºåºæ¨¡å¼'}

ğŸ’¡ **æç¤º**: è¯¦ç»†ç»“æœå·²ä¿å­˜ï¼Œæ‚¨å¯ä»¥åœ¨æ‰§è¡Œè¯¦æƒ…ä¸­æŸ¥çœ‹å®Œæ•´çš„æ‰¹å¤„ç†ç»“æœã€‚"""

        if progress.failed_tasks > 0:
            summary += f"\n\nâš ï¸ **æ³¨æ„**: æœ‰ {progress.failed_tasks} ä¸ªä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ‰§è¡Œè¯¦æƒ…äº†è§£å¤±è´¥åŸå› ã€‚"
        
        return summary
    
    def update_field_selection(self, field_selection: Dict[str, bool]) -> Dict[str, Any]:
        """æ›´æ–°å­—æ®µé€‰æ‹©é…ç½®"""
        if not self.csv_structure:
            return {"success": False, "message": "æ²¡æœ‰åŠ è½½CSVç»“æ„ä¿¡æ¯"}
        
        # éªŒè¯å­—æ®µé€‰æ‹©
        available_fields = set(self.csv_structure.get("columns", []))
        selected_fields = set(field for field, selected in field_selection.items() if selected)
        
        if not selected_fields:
            return {"success": False, "message": "è‡³å°‘éœ€è¦é€‰æ‹©ä¸€ä¸ªå­—æ®µ"}
        
        invalid_fields = selected_fields - available_fields
        if invalid_fields:
            return {"success": False, "message": f"æ— æ•ˆå­—æ®µ: {invalid_fields}"}
        
        # æ›´æ–°å­—æ®µé€‰æ‹©
        self.csv_structure["field_selection"] = field_selection
        
        return {
            "success": True,
            "message": f"å·²æ›´æ–°å­—æ®µé€‰æ‹©ï¼Œé€‰ä¸­ {len(selected_fields)} ä¸ªå­—æ®µ",
            "selected_fields": list(selected_fields)
        }
    

    def save_results_to_csv(self, results: List[Dict[str, Any]], output_path: str = None) -> bool:
        """ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœåˆ°CSVæ–‡ä»¶"""
        try:
            import csv
            from datetime import datetime
            
            if not results:
                logger.warning("æ²¡æœ‰ç»“æœæ•°æ®éœ€è¦ä¿å­˜")
                return False
            
            # ç¡®å®šè¾“å‡ºè·¯å¾„
            if output_path is None:
                output_dir = Path("workspace/batch_schedule_output")
                output_dir.mkdir(parents=True, exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_path = output_dir / f"batch_results_{timestamp}.csv"
            
            output_path = Path(output_path)
            
            # è·å–æ‰€æœ‰å¯èƒ½çš„å­—æ®µå
            all_fields = set()
            for result in results:
                if isinstance(result, dict):
                    all_fields.update(result.keys())
            
            # å†™å…¥CSVæ–‡ä»¶
            with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=sorted(all_fields))
                writer.writeheader()
                
                for result in results:
                    if isinstance(result, dict):
                        writer.writerow(result)
            
            logger.info(f"âœ… æ‰¹é‡å¤„ç†ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            logger.info(f"ğŸ“Š å…±ä¿å­˜ {len(results)} æ¡è®°å½•")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœå¤±è´¥: {e}")
            return False

    def get_batch_status(self) -> Dict[str, Any]:
        """è·å–æ‰¹å¤„ç†çŠ¶æ€"""
        status = {
            "enabled": self.config.enabled,
            "csv_loaded": bool(self.csv_data),
            "csv_rows": len(self.csv_data),
            "csv_file": self.config.csv_file_path,
            "batch_size": self.config.batch_size,
            "concurrent_tasks": self.config.concurrent_tasks,
            "processing_mode": self.config.processing_mode.value
        }
        
        # å¦‚æœæœ‰CSVç»“æ„ä¿¡æ¯ï¼Œæ·»åŠ åˆ°çŠ¶æ€ä¸­
        if self.csv_structure:
            status.update({
                "csv_structure": self.csv_structure,
                "available_fields": self.csv_structure.get("columns", []),
                "selected_fields": [
                    field for field, selected in self.csv_structure.get("field_selection", {}).items() 
                    if selected
                ]
            })
        
        return status 