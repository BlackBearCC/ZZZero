#!/usr/bin/env python3
"""
æµ‹è¯•æµå¼è¾“å‡ºåŠŸèƒ½
"""
import asyncio
import os
import sys

# æ·»åŠ srcè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from llm.base import LLMFactory
from core.types import LLMConfig
from tools.mcp_tools import MCPToolManager
from agents.react_agent import ReactAgent


async def test_stream_output():
    """æµ‹è¯•æµå¼è¾“å‡ºåŠŸèƒ½"""
    print("ğŸ”¥ å¼€å§‹æµ‹è¯•æµå¼è¾“å‡ºåŠŸèƒ½")
    
    # åˆ›å»ºLLMé…ç½®
    llm_config = LLMConfig(
        provider="doubao",
        model_name="ep-20250221154410-vh78x",
        temperature=0.7
    )
    
    # åˆ›å»ºLLMå®ä¾‹
    llm = LLMFactory.create(llm_config)
    await llm.initialize()
    
    # åˆ›å»ºå·¥å…·ç®¡ç†å™¨
    tool_manager = MCPToolManager()
    await tool_manager.initialize()
    
    # åˆ›å»ºReAct Agent
    agent = ReactAgent(
        llm=llm,
        tool_manager=tool_manager,
        max_iterations=5
    )
    
    # æµ‹è¯•æŸ¥è¯¢
    query = "å¸®æˆ‘æŸ¥è¯¢ä¸€ä¸‹CSVè¡¨æ ¼ä¸­çš„æ•°æ®"
    
    print(f"\nğŸ“ æŸ¥è¯¢: {query}")
    print("\nğŸš€ å¼€å§‹æµå¼è¾“å‡º:")
    print("-" * 50)
    
    try:
        async for chunk in agent.stream_run(query):
            chunk_type = chunk.get("type", "")
            content = chunk.get("content", "")
            
            if chunk_type == "text_chunk":
                print(content, end="", flush=True)
            elif chunk_type == "tool_result":
                print(f"\nğŸ”§ [å·¥å…·è°ƒç”¨å®Œæˆ]", flush=True)
                print(content, end="", flush=True)
            elif chunk_type == "tool_error":
                print(f"\nâŒ [å·¥å…·é”™è¯¯]: {content}", flush=True)
            elif chunk_type == "stream_error":
                print(f"\nğŸ’¥ [æµå¼é”™è¯¯]: {content}", flush=True)
    
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "-" * 50)
    print("âœ… æµå¼è¾“å‡ºæµ‹è¯•å®Œæˆ")
    
    # æ¸…ç†èµ„æº
    await agent.cleanup()


async def test_simple_stream():
    """æµ‹è¯•ç®€å•æµå¼è¾“å‡ºï¼ˆä¸å¸¦å·¥å…·ï¼‰"""
    print("\nğŸ”¥ å¼€å§‹æµ‹è¯•ç®€å•æµå¼è¾“å‡º")
    
    # åˆ›å»ºLLMé…ç½®
    llm_config = LLMConfig(
        provider="doubao",
        model_name="ep-20250221154410-vh78x",
        temperature=0.7
    )
    
    # åˆ›å»ºLLMå®ä¾‹
    llm = LLMFactory.create(llm_config)
    await llm.initialize()
    
    # åˆ›å»ºReAct Agentï¼ˆä¸ä½¿ç”¨å·¥å…·ï¼‰
    agent = ReactAgent(
        llm=llm,
        tool_manager=None,
        max_iterations=5
    )
    
    # æµ‹è¯•æŸ¥è¯¢
    query = "è¯·ä»‹ç»ä¸€ä¸‹Pythonçš„è£…é¥°å™¨"
    
    print(f"\nğŸ“ æŸ¥è¯¢: {query}")
    print("\nğŸš€ å¼€å§‹æµå¼è¾“å‡º:")
    print("-" * 50)
    
    try:
        async for chunk in agent.stream_run(query):
            chunk_type = chunk.get("type", "")
            content = chunk.get("content", "")
            
            if chunk_type == "text_chunk":
                print(content, end="", flush=True)
            elif chunk_type == "final_result":
                print(content, end="", flush=True)
    
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "-" * 50)
    print("âœ… ç®€å•æµå¼è¾“å‡ºæµ‹è¯•å®Œæˆ")
    
    # æ¸…ç†èµ„æº
    await agent.cleanup()


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_simple_stream())
    print("\n" + "=" * 60)
    asyncio.run(test_stream_output()) 