#!/usr/bin/env python3
"""
æ‰¹é‡æ—¥ç¨‹ç”Ÿæˆå™¨ - æœ¬åœ°mainå¯åŠ¨è„šæœ¬
æ”¯æŒæŒ‰æ‰¹æ¬¡ç”Ÿæˆæ—¥ç¨‹ï¼Œæ¯æ‰¹æ¬¡éšæœºé…ç½®ï¼Œä¿å­˜ä¸ºCSVæ ¼å¼
"""

import asyncio
import random
import json
import os
import sys
import csv
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path
import logging



# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = Path(__file__).parent
src_path = current_dir / "src"
sys.path.insert(0, str(src_path))

# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„ä»¥æ”¯æŒç»å¯¹å¯¼å…¥
sys.path.insert(0, str(current_dir))

from src.workflow.schedule_workflow import ScheduleWorkflow
from src.llm.base import LLMFactory
from src.core.types import LLMConfig
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_schedule_generator.log', encoding='utf-8'),  # æ˜ç¡®æŒ‡å®šUTF-8ç¼–ç 
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class BatchScheduleGenerator:
    """æ‰¹é‡æ—¥ç¨‹ç”Ÿæˆå™¨"""
    
    def __init__(self, start_date: str = "2025-07-18", batch_count: int = 100):
        """
        åˆå§‹åŒ–æ‰¹é‡ç”Ÿæˆå™¨
        
        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            batch_count: æ‰¹æ¬¡æ•°é‡
        """
        self.start_date = datetime.strptime(start_date, '%Y-%m-%d')
        self.batch_count = batch_count
        self.current_date = self.start_date
        self.workflow = None
        self.llm = None
        self.batch_history = []  # å­˜å‚¨æ¯æ‰¹æ¬¡çš„æ€»ç»“ï¼Œç”¨äºè¿ç»­æ€§
        
        # è¾“å‡ºç›®å½•
        self.output_dir = Path("workspace/batch_schedule_output")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–LLMå’Œå·¥ä½œæµ
        self._init_workflow()
        
        logger.info(f"æ‰¹é‡æ—¥ç¨‹ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"å¼€å§‹æ—¥æœŸ: {start_date}")
        logger.info(f"æ‰¹æ¬¡æ•°é‡: {batch_count}")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
    
    def _init_workflow(self):
        """åˆå§‹åŒ–å·¥ä½œæµå’ŒLLM"""
        try:
            # åˆ›å»ºLLMå®ä¾‹
            llm_config = LLMConfig(
                provider="doubao",
                api_key=os.getenv('DOUBAO_API_KEY', 'b633a622-b5d0-4f16-a8a9-616239cf15d1'),
                model_name=os.getenv('DOUBAO_MODEL_DEEPSEEKR1', 'ep-20250221154107-c4qc7'),
                temperature=0.7,
                max_tokens=16384
            )
            
            llm_factory = LLMFactory()
            self.llm = llm_factory.create(llm_config)
            
            # åˆ›å»ºå·¥ä½œæµå®ä¾‹
            self.workflow = ScheduleWorkflow(llm=self.llm)
            
            logger.info("LLMå’Œå·¥ä½œæµåˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"LLMå’Œå·¥ä½œæµåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _get_random_config(self, batch_num: int) -> Dict[str, Any]:
        """ç”Ÿæˆéšæœºé…ç½® - æ¯æ¬¡ç”Ÿæˆè¾ƒçŸ­å‘¨æœŸï¼Œä¿æŒçµæ´»æ€§"""
        # éšæœºå¤©æ•° (æ¯æ¬¡ç”Ÿæˆ30-50å¤©çš„å‘¨æœŸï¼Œä¿æŒçµæ´»æ€§)
        total_days = random.randint(30, 50)  # æ¯ä¸ªæ‰¹æ¬¡30-50å¤©ï¼Œé¿å…è¿‡é•¿å‘¨æœŸ
        end_date = self.current_date + timedelta(days=total_days - 1)
        
        # è·å–å¯ç”¨è§’è‰²åˆ—è¡¨ï¼ˆæ’é™¤ä¸»è§’æ–¹çŸ¥è¡¡ï¼‰
        available_characters = list(self.workflow.characters_data.get("è§’è‰²åˆ—è¡¨", {}).keys())
        if 'æ–¹çŸ¥è¡¡' in available_characters:
            available_characters.remove('æ–¹çŸ¥è¡¡')
        
        # éšæœºé€‰æ‹©è§’è‰² (2-6)
        char_count = min(random.randint(2, 6), len(available_characters))
        selected_characters = random.sample(available_characters, char_count)
        
        # è·å–å¯ç”¨åœ°ç‚¹åˆ—è¡¨
        available_locations = []
        for district_name, district_info in self.workflow.locations_data.get("districts", {}).items():
            for loc_name, loc_info in district_info.get("locations", {}).items():
                available_locations.append(loc_info.get('name', loc_name))
        
        # éšæœºé€‰æ‹©åœ°ç‚¹ (3-9ä¸ª)
        loc_count = min(random.randint(2, 6), len(available_locations))
        selected_locations = random.sample(available_locations, loc_count)
        
        # ç”Ÿæˆé…ç½®
        config = {
            'protagonist': 'æ–¹çŸ¥è¡¡',
            'schedule_type': 'weekly',
            'start_date': self.current_date.strftime('%Y-%m-%d'),
            'end_date': end_date.strftime('%Y-%m-%d'),
            'total_days': total_days,
            'selected_characters': selected_characters,
            'selected_locations': selected_locations,
            'selected_stories': [],  # æš‚æ—¶ä¸ä½¿ç”¨å‰§æƒ…
            'time_slots_config': {
                'å¤œé—´': {'start': '23:00', 'end': '06:00'},
                'ä¸Šåˆ': {'start': '06:00', 'end': '11:00'},
                'ä¸­åˆ': {'start': '11:00', 'end': '14:00'},
                'ä¸‹åˆ': {'start': '14:00', 'end': '18:00'},
                'æ™šä¸Š': {'start': '18:00', 'end': '23:00'}
            },
            'character_distribution': 'balanced',
            'story_integration': 'moderate',
            'include_holidays': True,
            'include_lunar': True,
            'mood_variety': True,
            'location_variety': True,
            # å¯ç”¨å‘¨æœŸæ€»ç»“åŠŸèƒ½
            'enable_cycle_summary': True,
            # æ·»åŠ ä¸Šä¸€æ‰¹æ¬¡æ€»ç»“ä¿¡æ¯ç”¨äºè¿ç»­æ€§
            'previous_batch_summary': self._get_previous_summary() if batch_num > 1 else ""
        }
        
        return config
    
    def _get_previous_summary(self) -> str:
        """è·å–ä¸Šä¸€æ‰¹æ¬¡çš„æ€»ç»“ä¿¡æ¯ï¼Œç”¨äºä¿æŒè¿ç»­æ€§"""
        if not self.batch_history:
            # å°è¯•ä»CSVæ–‡ä»¶è¯»å–æœ€è¿‘çš„æ€»ç»“
            return self._get_latest_cycle_summary_from_csv()
        
        last_batch = self.batch_history[-1]
        summary = f"""
## ä¸Šä¸€æ‰¹æ¬¡æ€»ç»“ï¼ˆ{last_batch['start_date']} - {last_batch['end_date']}ï¼‰

**æ—¶é—´èŒƒå›´**: {last_batch['start_date']} è‡³ {last_batch['end_date']}ï¼ˆ{last_batch['total_days']}å¤©ï¼‰
**ä¸»è¦è§’è‰²**: {', '.join(last_batch['characters'])}
**ä¸»è¦åœ°ç‚¹**: {', '.join(last_batch['locations'])}
**é‡è¦äº‹ä»¶**: {last_batch.get('key_events', 'å·¥ä½œã€ç ”ç©¶ã€ç¤¾äº¤ç­‰æ—¥å¸¸æ´»åŠ¨')}
**æƒ…æ„Ÿå‘å±•**: {last_batch.get('emotional_progress', 'ä¸å„è§’è‰²ä¿æŒè‰¯å¥½å…³ç³»')}
**é—ç•™é—®é¢˜**: {last_batch.get('pending_issues', 'æ— ç‰¹åˆ«é—ç•™é—®é¢˜')}

è¯·ç¡®ä¿æ–°çš„æ—¥ç¨‹ä¸ä¸Šè¿°æƒ…å†µè‡ªç„¶è¡”æ¥ï¼Œé¿å…çªå…€çš„å˜åŒ–ã€‚
"""
        return summary

    def _get_latest_cycle_summary_from_csv(self) -> str:
        """ä»CSVæ–‡ä»¶ä¸­è·å–æœ€æ–°çš„å‘¨æœŸæ€»ç»“"""
        try:
            import pandas as pd
            
            # æŸ¥æ‰¾æœ€æ–°çš„CSVæ–‡ä»¶
            csv_files = [f for f in os.listdir(self.output_dir) if f.startswith('batch_schedules_') and f.endswith('.csv')]
            if not csv_files:
                logger.info("æœªæ‰¾åˆ°å†å²æ‰¹æ¬¡æ€»ç»“ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ‰¹æ¬¡æˆ–å†å²æ–‡ä»¶ä¸ºç©º")
                return ""
            
            # è¯»å–æœ€æ–°çš„CSVæ–‡ä»¶
            latest_csv = sorted(csv_files)[-1]
            csv_path = os.path.join(self.output_dir, latest_csv)
            
            df = pd.read_csv(csv_path)
            if df.empty:
                logger.info("CSVæ–‡ä»¶ä¸ºç©ºï¼Œæœªæ‰¾åˆ°å†å²æ‰¹æ¬¡æ€»ç»“")
                return ""
            
            # è·å–æœ€æ–°æ‰¹æ¬¡çš„å‘¨æœŸæ€»ç»“ï¼ˆéç©ºçš„ï¼‰
            latest_summaries = df[df['cycle_summary'].notna() & (df['cycle_summary'] != '')]['cycle_summary']
            if latest_summaries.empty:
                logger.info("æœªæ‰¾åˆ°å†å²æ‰¹æ¬¡æ€»ç»“ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ‰¹æ¬¡æˆ–å†å²æ–‡ä»¶ä¸ºç©º")
                return ""
            
            latest_summary = latest_summaries.iloc[-1]
            logger.info(f"ä»CSVæ–‡ä»¶ä¸­æ‰¾åˆ°å†å²æ€»ç»“ï¼Œé•¿åº¦: {len(latest_summary)} å­—ç¬¦")
            return latest_summary
            
        except Exception as e:
            logger.warning(f"è¯»å–CSVå†å²æ€»ç»“å¤±è´¥: {e}")
            logger.info("æœªæ‰¾åˆ°å†å²æ‰¹æ¬¡æ€»ç»“ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªæ‰¹æ¬¡æˆ–å†å²æ–‡ä»¶ä¸ºç©º")
            return ""
    
    async def _generate_single_batch(self, batch_num: int, retry_count: int = 0) -> Optional[Dict[str, Any]]:
        """ç”Ÿæˆå•ä¸ªæ‰¹æ¬¡çš„æ—¥ç¨‹ - æ”¯æŒå¼‚å¸¸é‡è¯•æœºåˆ¶"""
        max_retries = 3
        
        # ç”Ÿæˆé…ç½®ï¼ˆä¿æŒä¸€è‡´ï¼‰
        config = self._get_random_config(batch_num)
        
        for attempt in range(max_retries):
            try:
                current_attempt = retry_count + attempt + 1
                logger.info(f"å¼€å§‹ç”Ÿæˆç¬¬ {batch_num} æ‰¹æ¬¡æ—¥ç¨‹ï¼ˆç¬¬ {current_attempt} æ¬¡å°è¯•ï¼‰...")
                
                if attempt > 0:
                    logger.info(f"æ‰¹æ¬¡ {batch_num} é‡è¯•ç¬¬ {attempt} æ¬¡ï¼Œä½¿ç”¨ç›¸åŒé…ç½®")
                
                logger.info(f"æ‰¹æ¬¡ {batch_num} é…ç½®:")
                logger.info(f"  æ—¥æœŸèŒƒå›´: {config['start_date']} - {config['end_date']} ({config['total_days']}å¤©)")
                logger.info(f"  é¢„è®¡å‘¨æœŸæ•°: {(config['total_days'] + 10) // 11} ä¸ªå‘¨æœŸï¼ˆæ¯å‘¨æœŸ7-15å¤©ï¼‰")
                logger.info(f"  è§’è‰²æ•°é‡: {len(config['selected_characters'])}")
                logger.info(f"  åœ°ç‚¹æ•°é‡: {len(config['selected_locations'])}")
                logger.info(f"  é€‰æ‹©è§’è‰²: {', '.join(config['selected_characters'])}")
                logger.info(f"  é€‰æ‹©åœ°ç‚¹: {', '.join(config['selected_locations'])}")
                
                # åˆ›å»ºç®€åŒ–çš„å·¥ä½œæµèŠå¤©æ¥å£
                class SimpleWorkflowChat:
                    def __init__(self):
                        self.current_node = ""
                    
                    async def add_node_message(self, node_name: str, message: str, status: str):
                        # åªæ‰“å°é‡è¦çš„çŠ¶æ€ä¿¡æ¯
                        if status in ['success', 'error', 'warning']:
                            clean_message = message.replace('âœ…', '[æˆåŠŸ]').replace('âŒ', '[å¤±è´¥]').replace('âš ï¸', '[è­¦å‘Š]')
                            logger.info(f"[{node_name}] {clean_message}")
                    
                    def _create_workflow_progress(self):
                        return ""
                
                workflow_chat = SimpleWorkflowChat()
                
                # æ‰§è¡Œå¤šå‘¨æœŸå·¥ä½œæµ
                logger.info(f"å¼€å§‹æ‰§è¡Œå¤šå‘¨æœŸå·¥ä½œæµ...")
                
                progress_count = 0
                async for stream_event in self.workflow.execute_workflow_stream(config, workflow_chat):
                    progress_count += 1
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆè¾“å‡ºäº‹ä»¶
                    if isinstance(stream_event, tuple) and len(stream_event) >= 4:
                        html, content, message, is_complete = stream_event
                        if "å‘¨æœŸç”Ÿæˆå®Œæˆ" in message or "æ‰§è¡Œå®Œæˆ" in message:
                            logger.info(f"æ£€æµ‹åˆ°å‘¨æœŸå®Œæˆä¿¡å·: {message}")
                    
                logger.info(f"å¤šå‘¨æœŸå·¥ä½œæµæ‰§è¡Œå®Œæˆï¼Œå…±æ”¶åˆ° {progress_count} æ¬¡äº‹ä»¶")
                
                # ç­‰å¾…æ•°æ®åº“å†™å…¥å®Œæˆ
                logger.info("ç­‰å¾…æ•°æ®åº“å†™å…¥å®Œæˆ...")
                import time
                time.sleep(2)
                
                # ä»æ•°æ®åº“è·å–ç”Ÿæˆçš„æ—¥ç¨‹è®°å½•
                from database.managers.schedule_manager import ScheduleManager
                schedule_manager = ScheduleManager()
                
                # è·å–æ‰¹æ¬¡æ—¥æœŸèŒƒå›´å†…çš„æ‰€æœ‰æ—¥ç¨‹è®°å½•
                recent_schedules = schedule_manager.get_schedules_by_filter({}, limit=10)
                
                # ç­›é€‰å‡ºå½“å‰æ‰¹æ¬¡æ—¥æœŸèŒƒå›´å†…çš„æ—¥ç¨‹
                batch_schedules = []
                for schedule in recent_schedules:
                    schedule_start = schedule.get('start_date', '')
                    schedule_end = schedule.get('end_date', '')
                    
                    # æ£€æŸ¥æ˜¯å¦åœ¨å½“å‰æ‰¹æ¬¡çš„æ—¥æœŸèŒƒå›´å†…æˆ–æœ‰é‡å 
                    if (schedule_start <= config['end_date'] and schedule_end >= config['start_date']):
                        batch_schedules.append(schedule)
                        logger.info(f"æ‰¾åˆ°åŒ¹é…å‘¨æœŸ: {schedule_start} - {schedule_end}, daily_schedulesæ•°é‡: {len(schedule.get('daily_schedules', []))}")
                
                if batch_schedules:
                    logger.info(f"æ‰¹æ¬¡ {batch_num} æ‰¾åˆ° {len(batch_schedules)} ä¸ªå‘¨æœŸçš„æ—¥ç¨‹è®°å½•")
                    
                    # åˆå¹¶æ‰€æœ‰å‘¨æœŸçš„æ•°æ®
                    batch_info = self._merge_multiple_cycles_data(batch_schedules, batch_num, config)
                    
                    if batch_info:
                        # ä¿å­˜åˆ°å†å²è®°å½•
                        self.batch_history.append(batch_info)
                        logger.info(f"âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆï¼ˆç¬¬ {current_attempt} æ¬¡å°è¯•æˆåŠŸï¼‰")
                        return batch_info
                    else:
                        raise Exception("åˆå¹¶å‘¨æœŸæ•°æ®å¤±è´¥")
                else:
                    raise Exception("æ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°å½“å‰æ‰¹æ¬¡çš„æ—¥ç¨‹è®°å½•")
                        
            except Exception as e:
                current_attempt = retry_count + attempt + 1
                logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} ç¬¬ {current_attempt} æ¬¡å°è¯•å¤±è´¥: {e}")
                
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 2  # é€’å¢ç­‰å¾…æ—¶é—´ï¼š2s, 4s, 6s
                    logger.info(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} ç»è¿‡ {max_retries} æ¬¡å°è¯•ä»ç„¶å¤±è´¥ï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡")
                    import traceback
                    logger.error(traceback.format_exc())
                    return None
        
        return None
    
    def _extract_key_events(self, daily_schedules: List[Dict]) -> str:
        """ä»æ—¥ç¨‹ä¸­æå–å…³é”®äº‹ä»¶"""
        key_events = []
        for day in daily_schedules[:3]:  # åªå–å‰3å¤©çš„äº‹ä»¶ä½œä¸ºæ‘˜è¦
            for slot in day.get('time_slots', []):
                content = slot.get('story_content', '')
                if len(content) > 100:  # å†…å®¹è¾ƒä¸°å¯Œçš„äº‹ä»¶
                    key_events.append(f"{day.get('date', '')} {slot.get('slot_name', '')}: {content[:50]}...")
        return '; '.join(key_events[:3])  # æœ€å¤š3ä¸ªå…³é”®äº‹ä»¶
    
    def _extract_emotional_progress(self, daily_schedules: List[Dict]) -> str:
        """æå–æƒ…æ„Ÿå‘å±•çº¿"""
        # ç®€åŒ–æå–ï¼ŒæŸ¥æ‰¾åŒ…å«æƒ…æ„Ÿè¯æ±‡çš„å†…å®¹
        emotional_keywords = ['æ„ŸåŠ¨', 'å¼€å¿ƒ', 'æ‹…å¿ƒ', 'æœŸå¾…', 'æ»¡æ„', 'æ„Ÿè°¢', 'å‹è°Š', 'å…³ç³»', 'äº¤æµ']
        emotional_events = []
        
        for day in daily_schedules:
            for slot in day.get('time_slots', []):
                content = slot.get('story_content', '')
                for keyword in emotional_keywords:
                    if keyword in content:
                        emotional_events.append(f"ä¸{slot.get('assigned_character', '')}çš„{keyword}")
                        break
        
        return '; '.join(set(emotional_events[:3]))  # å»é‡å¹¶é™åˆ¶æ•°é‡
    
    def _extract_pending_issues(self, daily_schedules: List[Dict]) -> str:
        """æå–é—ç•™é—®é¢˜"""
        # ç®€åŒ–æå–ï¼ŒæŸ¥æ‰¾æœ€åä¸€å¤©çš„è®¡åˆ’æˆ–æœªå®Œæˆäº‹é¡¹
        if daily_schedules:
            last_day = daily_schedules[-1]
            daily_plan = last_day.get('daily_plan', '')
            if 'è®¡åˆ’' in daily_plan or 'å‡†å¤‡' in daily_plan:
                return daily_plan[:100] + "..." if len(daily_plan) > 100 else daily_plan
        return "æ— ç‰¹åˆ«é—ç•™é—®é¢˜"
    
    def _merge_multiple_cycles_data(self, batch_schedules: List[Dict], batch_num: int, config: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """åˆå¹¶å¤šä¸ªå‘¨æœŸçš„æ•°æ®ä¸ºä¸€ä¸ªæ‰¹æ¬¡ä¿¡æ¯ï¼Œå¹¶ä¿å­˜CSV"""
        try:
            if not batch_schedules:
                return None
            
            # åˆå¹¶æ‰€æœ‰å‘¨æœŸçš„æ¯æ—¥å®‰æ’
            all_daily_schedules = []
            all_cycle_summaries = []
            all_characters = set()
            all_locations = set()
            
            # æ‰¾åˆ°æœ€æ—©å’Œæœ€æ™šçš„æ—¥æœŸ
            start_dates = []
            end_dates = []
            total_days = 0
            
            for i, schedule in enumerate(batch_schedules):
                # æ”¶é›†æ—¥æœŸä¿¡æ¯
                start_dates.append(schedule.get('start_date', ''))
                end_dates.append(schedule.get('end_date', ''))
                total_days += schedule.get('total_days', 0)
                
                # åˆå¹¶æ¯æ—¥å®‰æ’
                daily_schedules = schedule.get('daily_schedules', [])
                logger.info(f"å‘¨æœŸ {i+1}: {schedule.get('start_date', '')} - {schedule.get('end_date', '')}, åŒ…å« {len(daily_schedules)} å¤©")
                all_daily_schedules.extend(daily_schedules)
                
                # æ”¶é›†å‘¨æœŸæ€»ç»“
                cycle_summary = schedule.get('cycle_summary', '')
                if cycle_summary:
                    all_cycle_summaries.append(cycle_summary)
                
                # æå–è§’è‰²å’Œåœ°ç‚¹
                for day in daily_schedules:
                    for slot in day.get('time_slots', []):
                        chars = slot.get('involved_characters', [])
                        for char in chars:
                            if char and char != 'æ–¹çŸ¥è¡¡':
                                all_characters.add(char)
                        
                        location = slot.get('location', '')
                        if location:
                            all_locations.add(location)
            
            # æŒ‰æ—¥æœŸæ’åº
            all_daily_schedules.sort(key=lambda x: x.get('date', ''))
            
            # ç›´æ¥ä¿å­˜ä¸ºCSVæ–‡ä»¶
            self._save_batch_to_csv(all_daily_schedules, batch_schedules, batch_num)
            
            # æ„å»ºæ‰¹æ¬¡ä¿¡æ¯
            batch_info = {
                'batch_number': batch_num,
                'schedule_ids': [s.get('schedule_id', '') for s in batch_schedules],
                'start_date': min(start_dates) if start_dates else config['start_date'],
                'end_date': max(end_dates) if end_dates else config['end_date'],
                'total_days': total_days,
                'cycles_count': len(batch_schedules),
                'characters': list(all_characters),
                'locations': list(all_locations),
                'daily_schedules': all_daily_schedules,
                'cycle_summaries': all_cycle_summaries,
                'key_events': self._extract_key_events(all_daily_schedules),
                'emotional_progress': self._extract_emotional_progress(all_daily_schedules),
                'pending_issues': self._extract_pending_issues(all_daily_schedules)
            }
            
            logger.info(f"æˆåŠŸåˆå¹¶æ‰¹æ¬¡ä¿¡æ¯:")
            logger.info(f"  åŒ…å« {len(batch_schedules)} ä¸ªå‘¨æœŸï¼Œ{len(all_daily_schedules)} å¤©å®‰æ’")
            logger.info(f"  æ¶‰åŠ {len(all_characters)} ä¸ªè§’è‰²: {', '.join(list(all_characters)[:3])}...")
            logger.info(f"  æ¶‰åŠ {len(all_locations)} ä¸ªåœ°ç‚¹: {', '.join(list(all_locations)[:3])}...")
            
            return batch_info
            
        except Exception as e:
            logger.error(f"åˆå¹¶å¤šå‘¨æœŸæ•°æ®å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None

    def _get_batch_info_from_database(self, schedule_id: str) -> Optional[Dict[str, Any]]:
        """ä»æ•°æ®åº“è·å–å®Œæ•´çš„æ‰¹æ¬¡ä¿¡æ¯"""
        try:
            from database.managers.schedule_manager import ScheduleManager
            
            # åˆ›å»ºæ•°æ®åº“ç®¡ç†å™¨
            schedule_manager = ScheduleManager()
            
            # è·å–å®Œæ•´çš„æ—¥ç¨‹æ•°æ®
            full_schedule = schedule_manager.get_schedule_by_id(schedule_id)
            
            if not full_schedule:
                logger.warning(f"æ•°æ®åº“ä¸­æœªæ‰¾åˆ°æ—¥ç¨‹: {schedule_id}")
                return None
            
            # æå–æ¯æ—¥å®‰æ’
            daily_schedules = full_schedule.get('daily_schedules', [])
            
            # æ„å»ºæ‰¹æ¬¡ä¿¡æ¯
            batch_info = {
                'batch_number': len(self.batch_history) + 1,  # åŸºäºå½“å‰å†å²æ•°é‡
                'schedule_id': schedule_id,
                'start_date': full_schedule.get('start_date', ''),
                'end_date': full_schedule.get('end_date', ''),
                'total_days': full_schedule.get('total_days', 0),
                'characters': [],  # ä»æ—¶é—´æ®µä¸­æå–
                'locations': [],   # ä»æ—¶é—´æ®µä¸­æå–
                'daily_schedules': daily_schedules,
                'schedule_summary': {},  # å¯ä»¥ä»æè¿°ä¸­è§£æ
                'weekly_plan': full_schedule.get('weekly_plan', ''),
                'key_events': self._extract_key_events(daily_schedules),
                'emotional_progress': self._extract_emotional_progress(daily_schedules),
                'pending_issues': self._extract_pending_issues(daily_schedules)
            }
            
            # ä»æ—¶é—´æ®µä¸­æå–å‚ä¸çš„è§’è‰²å’Œåœ°ç‚¹
            characters = set()
            locations = set()
            
            for day in daily_schedules:
                for slot in day.get('time_slots', []):
                    assigned_char = slot.get('assigned_character', '')
                    if assigned_char and assigned_char != 'æ–¹çŸ¥è¡¡':
                        characters.add(assigned_char)
                    
                    location = slot.get('location', '')
                    if location:
                        locations.add(location)
            
            batch_info['characters'] = list(characters)
            batch_info['locations'] = list(locations)
            
            logger.info(f"ä»æ•°æ®åº“æˆåŠŸè·å–æ‰¹æ¬¡ä¿¡æ¯: {schedule_id}")
            logger.info(f"  åŒ…å« {len(daily_schedules)} å¤©å®‰æ’")
            logger.info(f"  æ¶‰åŠ {len(characters)} ä¸ªè§’è‰²: {', '.join(list(characters)[:3])}...")
            logger.info(f"  æ¶‰åŠ {len(locations)} ä¸ªåœ°ç‚¹: {', '.join(list(locations)[:3])}...")
            
            return batch_info
            
        except Exception as e:
            logger.error(f"ä»æ•°æ®åº“è·å–æ‰¹æ¬¡ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return None
    

    
    def _check_holidays_in_batch(self, batch_info: Dict[str, Any]) -> Dict[str, str]:
        """éªŒè¯æ‰¹æ¬¡ä¸­çš„èŠ‚å‡æ—¥"""
        holidays = {}
        try:
            start_date = batch_info['start_date']
            end_date = batch_info['end_date']
            
            # ä½¿ç”¨å·¥ä½œæµçš„èŠ‚å‡æ—¥æ•°æ®
            holidays_data = self.workflow.get_holidays_in_range(start_date, end_date)
            
            if holidays_data:
                logger.info(f"æ‰¹æ¬¡ {batch_info['batch_number']} åŒ…å«èŠ‚å‡æ—¥: {list(holidays_data.keys())}")
                for date, holiday_info in holidays_data.items():
                    holidays[date] = holiday_info.get('name', '')
            else:
                logger.info(f"æ‰¹æ¬¡ {batch_info['batch_number']} æ— èŠ‚å‡æ—¥")
                
        except Exception as e:
            logger.warning(f"æ£€æŸ¥èŠ‚å‡æ—¥å¤±è´¥: {e}")
            
        return holidays
    
    def _get_season_from_date(self, date_str: str) -> str:
        """æ ¹æ®æ—¥æœŸç¡®å®šå­£èŠ‚"""
        try:
            from datetime import datetime
            date = datetime.strptime(date_str, '%Y-%m-%d')
            month = date.month
            
            if month in [12, 1, 2]:
                return 'å†¬å­£'
            elif month in [3, 4, 5]:
                return 'æ˜¥å­£'
            elif month in [6, 7, 8]:
                return 'å¤å­£'
            elif month in [9, 10, 11]:
                return 'ç§‹å­£'
            else:
                return 'æœªçŸ¥'
        except:
            return 'æœªçŸ¥'
    
    def _save_batch_to_csv(self, daily_schedules: List[Dict], batch_schedules: List[Dict], batch_num: int):
        """ä¿å­˜æ‰¹æ¬¡æ•°æ®åˆ°CSVæ–‡ä»¶"""
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            self.output_dir.mkdir(parents=True, exist_ok=True)
            
            # ä½¿ç”¨å›ºå®šCSVæ–‡ä»¶åï¼Œä¾¿äºå¢é‡æ›´æ–°
            csv_file_path = self.output_dir / "batch_schedules.csv"
            
            # å®šä¹‰CSVåˆ—å¤´
            csv_headers = [
                "æ—¥æœŸ", "æ˜ŸæœŸ", "èŠ‚æ—¥ä¿¡æ¯", "å­£èŠ‚", "å¤©æ°”", "ä¸»é¢˜", 
                "å‘¨æœŸè®¡åˆ’", "3å¤©æ€»ç»“", "æ¯æ—¥è®¡åˆ’", "æ¯æ—¥æ€»ç»“", "æ¶‰åŠè§’è‰²", "è§’è‰²ç®€ä»‹",
                "ä¸Šåˆ", "ä¸­åˆ", "ä¸‹åˆ", "æ™šä¸Š", "å¤œé—´"
            ]
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå†³å®šæ˜¯è¿½åŠ è¿˜æ˜¯åˆ›å»ºæ–°æ–‡ä»¶
            file_exists = csv_file_path.exists() and csv_file_path.stat().st_size > 0
            write_mode = 'a' if file_exists else 'w'
            
            # è·å–å‘¨æœŸè®¡åˆ’å’Œå‘¨æœŸæ€»ç»“
            cycle_theme = ""
            cycle_summary = ""
            if batch_schedules:
                first_schedule = batch_schedules[0]
                if isinstance(first_schedule, dict) and 'cycle_summary' in first_schedule:
                    cycle_summary = first_schedule.get('cycle_summary', '')
                    # ä»å‘¨æœŸæ•°æ®ä¸­æå–ä¸»é¢˜
                    cycle_info = first_schedule.get('cycle_info', {})
                    cycle_theme = cycle_info.get('cycle_theme', '')
            
            # å†™å…¥CSVæ–‡ä»¶
            with open(csv_file_path, write_mode, encoding='utf-8', newline='') as csvfile:
                writer = csv.writer(csvfile)
                
                # åªåœ¨æ–‡ä»¶ä¸å­˜åœ¨æ—¶å†™å…¥è¡¨å¤´
                if not file_exists:
                    writer.writerow(csv_headers)
                
                # è·å–LLMç”Ÿæˆçš„æ‰¹æ¬¡æ€»ç»“(batch_summary)
                llm_batch_summary = ""
                logger.info(f"æ­£åœ¨æŸ¥æ‰¾LLMç”Ÿæˆçš„batch_summary...")
                
                # ä»batch_schedulesä¸­æŸ¥æ‰¾LLMç”Ÿæˆçš„batch_summary
                for schedule in batch_schedules:
                    # ç›´æ¥ä»scheduleä¸­æŸ¥æ‰¾batch_summary
                    if 'batch_summary' in schedule and schedule['batch_summary']:
                        llm_batch_summary = schedule['batch_summary']
                        logger.info(f"âœ… ä»å‘¨æœŸæ•°æ®ä¸­æ‰¾åˆ°LLMç”Ÿæˆçš„batch_summary: {llm_batch_summary[:150]}...")
                        break
                    
                    # ä»daily_schedulesä¸­æŸ¥æ‰¾batch_summary
                    daily_schedules_in_cycle = schedule.get('daily_schedules', [])
                    for day in daily_schedules_in_cycle:
                        if 'batch_summary' in day and day['batch_summary']:
                            llm_batch_summary = day['batch_summary']
                            logger.info(f"âœ… ä»æ¯æ—¥æ•°æ®ä¸­æ‰¾åˆ°LLMç”Ÿæˆçš„batch_summary: {llm_batch_summary[:150]}...")
                            break
                    if llm_batch_summary:
                        break
                
                if not llm_batch_summary:
                    logger.warning("âš ï¸ æœªæ‰¾åˆ°LLMç”Ÿæˆçš„batch_summaryï¼Œå°†ä½¿ç”¨ç©ºå€¼")
                
                # éå†æ¯å¤©çš„æ—¥ç¨‹æ•°æ®
                for day_index, day_data in enumerate(daily_schedules):
                    date = day_data.get('date', '')
                    weekday = day_data.get('weekday_name', '')
                    weather = day_data.get('weather', '')
                    is_holiday = day_data.get('is_holiday', False)
                    holiday_name = day_data.get('holiday_name', '')
                    
                    # èŠ‚æ—¥ä¿¡æ¯å¤„ç†
                    holiday_info = holiday_name if is_holiday and holiday_name else "æ— "
                    
                    # æ ¹æ®æ—¥æœŸç¡®å®šå­£èŠ‚
                    season = self._get_season_from_date(date)
                    
                    daily_plan = day_data.get('daily_plan', '')
                    daily_summary = day_data.get('daily_summary', '')  # æ¯æ—¥æ€»ç»“
                    
                    # æå–æ¯æ—¥æ¶‰åŠè§’è‰²ä¿¡æ¯
                    daily_involved_characters = day_data.get('daily_involved_characters', [])
                    daily_characters_info = day_data.get('daily_characters_info', '')
                    
                    # å¦‚æœæ²¡æœ‰æä¾›å­—ç¬¦ä¸²æ ¼å¼çš„è§’è‰²ä¿¡æ¯ï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆ
                    if not daily_characters_info and daily_involved_characters:
                        # ä»è§’è‰²æ•°æ®ä¸­è·å–ç®€ä»‹
                        char_infos = []
                        char_list = self.workflow.characters_data.get("è§’è‰²åˆ—è¡¨", {})
                        for char_name in daily_involved_characters:
                            if char_name in char_list:
                                char_desc = char_list[char_name].get('ç®€ä»‹', '')
                                char_infos.append(f"{char_name}-{char_desc}")
                        daily_characters_info = 'ï¼›'.join(char_infos)
                    
                    # åˆå§‹åŒ–æ—¶é—´æ®µæ•°æ®
                    time_slots_data = {
                        'ä¸Šåˆ': '',
                        'ä¸­åˆ': '', 
                        'ä¸‹åˆ': '',
                        'æ™šä¸Š': '',
                        'å¤œé—´': ''
                    }
                    
                    # æå–æ—¶é—´æ®µæ•°æ®
                    time_slots = day_data.get('time_slots', [])
                    for slot in time_slots:
                        slot_name = slot.get('slot_name', '')
                        if slot_name in time_slots_data:
                            time_slots_data[slot_name] = slot.get('story_content', '')
                    
                    # 3å¤©æ€»ç»“ï¼šåªåœ¨æ¯3å¤©çš„ç¬¬ä¸€å¤©æ˜¾ç¤ºLLMç”Ÿæˆçš„batch_summaryï¼Œå…¶ä»–å¤©ä¸ºç©º
                    day_batch_summary = ""
                    if day_index % 3 == 0:  # æ¯3å¤©çš„ç¬¬ä¸€å¤©æ˜¾ç¤ºLLMç”Ÿæˆçš„æ€»ç»“
                        day_batch_summary = llm_batch_summary
                    
                    # æ„å»ºCSVè¡Œæ•°æ®
                    row_data = [
                        date,                          # æ—¥æœŸ
                        weekday,                       # æ˜ŸæœŸ
                        holiday_info,                  # èŠ‚æ—¥ä¿¡æ¯
                        season,                        # å­£èŠ‚
                        weather,                       # å¤©æ°”
                        cycle_theme,                   # ä¸»é¢˜
                        cycle_summary,                 # å‘¨æœŸè®¡åˆ’
                        day_batch_summary,             # 3å¤©æ€»ç»“
                        daily_plan,                    # æ¯æ—¥è®¡åˆ’
                        daily_summary,                 # æ¯æ—¥æ€»ç»“
                        ', '.join(daily_involved_characters),  # æ¶‰åŠè§’è‰²
                        daily_characters_info,         # è§’è‰²ç®€ä»‹
                        time_slots_data['ä¸Šåˆ'],        # ä¸Šåˆ
                        time_slots_data['ä¸­åˆ'],        # ä¸­åˆ
                        time_slots_data['ä¸‹åˆ'],        # ä¸‹åˆ
                        time_slots_data['æ™šä¸Š'],        # æ™šä¸Š
                        time_slots_data['å¤œé—´']         # å¤œé—´
                    ]
                    
                    writer.writerow(row_data)
            
            logger.info(f"æ‰¹æ¬¡ {batch_num} CSVæ•°æ®å·²{'è¿½åŠ åˆ°' if file_exists else 'ä¿å­˜ä¸ºæ–°'}æ–‡ä»¶: {csv_file_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ‰¹æ¬¡ {batch_num} CSVæ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
    

    
    def _save_detailed_json(self, batch_info: Dict[str, Any]):
        """ä¿å­˜è¯¦ç»†çš„JSONæ•°æ®ï¼ˆå¯é€‰ï¼‰"""
        try:
            json_file = self.output_dir / f"batch_{batch_info['batch_number']:03d}_{batch_info['start_date']}.json"
            
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(batch_info, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ‰¹æ¬¡ {batch_info['batch_number']} è¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ° {json_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜è¯¦ç»†JSONæ•°æ®å¤±è´¥: {e}")
    
    async def generate_all_batches(self):
        """è¿ç»­ç”Ÿæˆæ‰€æœ‰æ‰¹æ¬¡çš„æ—¥ç¨‹ï¼Œå®ç°æ—¶é—´è¿ç»­æ€§å’Œå¼‚å¸¸é‡è¯•"""
        logger.info(f"å¼€å§‹è¿ç»­ç”Ÿæˆ {self.batch_count} ä¸ªæ‰¹æ¬¡çš„æ—¥ç¨‹...")
        
        success_count = 0
        failed_count = 0
        total_attempts = 0
        
        for batch_num in range(1, self.batch_count + 1):
            logger.info(f"\n{'='*60}")
            logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {batch_num}/{self.batch_count} æ‰¹æ¬¡")
            logger.info(f"å½“å‰å¼€å§‹æ—¥æœŸ: {self.current_date.strftime('%Y-%m-%d')}")
            logger.info(f"{'='*60}")
            
            # ç”Ÿæˆå•ä¸ªæ‰¹æ¬¡ï¼ˆå†…å«é‡è¯•æœºåˆ¶ï¼‰
            batch_info = await self._generate_single_batch(batch_num)
            total_attempts += 1  # ç»Ÿè®¡æ€»å°è¯•æ¬¡æ•°
            
            if batch_info:
                # ç¡®ä¿æ—¥æœŸè¿ç»­æ€§ï¼šä¸‹ä¸€æ‰¹æ¬¡ä»å½“å‰æ‰¹æ¬¡ç»“æŸæ—¥æœŸ+1å¤©å¼€å§‹
                batch_end_date = batch_info['end_date']
                next_start_date = datetime.strptime(batch_end_date, '%Y-%m-%d') + timedelta(days=1)
                self.current_date = next_start_date
                
                success_count += 1
                logger.info(f"ğŸ‰ æ‰¹æ¬¡ {batch_num} æœ€ç»ˆæˆåŠŸ")
                logger.info(f"   ğŸ“Š è¦†ç›–å¤©æ•°: {batch_info['total_days']} å¤©")
                logger.info(f"   ğŸ”„ æ¶‰åŠå‘¨æœŸ: {batch_info.get('cycles_count', 1)} ä¸ª")
                logger.info(f"   ğŸ“… ä¸‹æ‰¹æ¬¡å¼€å§‹: {self.current_date.strftime('%Y-%m-%d')}")
                logger.info(f"   âœ“ æ—¥æœŸè¿ç»­æ€§: {batch_end_date} â†’ {self.current_date.strftime('%Y-%m-%d')}")
            else:
                failed_count += 1
                logger.error(f"ğŸ’¥ æ‰¹æ¬¡ {batch_num} æœ€ç»ˆå¤±è´¥ï¼ˆå·²é‡è¯•3æ¬¡ï¼‰")
                
                # å¤±è´¥æ—¶æ¨è¿›éšæœºå¤©æ•°ï¼Œä¿æŒæ—¶é—´å‰è¿›
                try:
                    config = self._get_random_config(batch_num)
                    skip_days = config['total_days']
                except Exception as config_error:
                    logger.warning(f"è·å–é…ç½®å¤±è´¥: {config_error}ï¼Œä½¿ç”¨é»˜è®¤è·³è¿‡å¤©æ•°")
                    skip_days = random.randint(30, 50)
                
                self.current_date += timedelta(days=skip_days)
                logger.info(f"   â­ï¸ è·³è¿‡ {skip_days} å¤©åˆ°: {self.current_date.strftime('%Y-%m-%d')}")
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯ï¼Œç»™ç³»ç»Ÿæ¢å¤æ—¶é—´
            logger.info(f"â¸ï¸ æ‰¹æ¬¡é—´ä¼‘æ¯ 3 ç§’...")
            await asyncio.sleep(3)
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        self._generate_summary_report(success_count, failed_count, total_attempts)
        
        logger.info(f"\nğŸ è¿ç»­æ‰¹é‡ç”Ÿæˆå®Œæˆ!")
        logger.info(f"âœ… æˆåŠŸæ‰¹æ¬¡: {success_count}/{self.batch_count}")
        logger.info(f"âŒ å¤±è´¥æ‰¹æ¬¡: {failed_count}/{self.batch_count}")
        logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {success_count/self.batch_count*100:.1f}%")
        logger.info(f"ğŸ”„ æ€»å°è¯•æ¬¡æ•°: {total_attempts}")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        logger.info(f"ğŸ“… æœ€ç»ˆæ—¥æœŸ: {self.current_date.strftime('%Y-%m-%d')}")
        
        return success_count, failed_count
    
    def _generate_summary_report(self, success_count: int, failed_count: int, total_attempts: int = None):
        """ç”Ÿæˆå¸¦é‡è¯•ç»Ÿè®¡çš„æ€»ç»“æŠ¥å‘Š"""
        try:
            report_file = self.output_dir / f"batch_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(f"ğŸ“Š æ‰¹é‡æ—¥ç¨‹ç”Ÿæˆæ€»ç»“æŠ¥å‘Šï¼ˆå¼‚å¸¸é‡è¯•ç‰ˆï¼‰\n")
                f.write(f"{'='*60}\n\n")
                f.write(f"ğŸ• ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ğŸ“… å¼€å§‹æ—¥æœŸ: {self.start_date.strftime('%Y-%m-%d')}\n")
                f.write(f"ğŸ“… ç»“æŸæ—¥æœŸ: {self.current_date.strftime('%Y-%m-%d')}\n")
                f.write(f"ğŸ¯ è®¡åˆ’æ‰¹æ¬¡: {self.batch_count}\n")
                f.write(f"âœ… æˆåŠŸæ‰¹æ¬¡: {success_count}\n")
                f.write(f"âŒ å¤±è´¥æ‰¹æ¬¡: {failed_count}\n")
                f.write(f"ğŸ“ˆ æˆåŠŸç‡: {success_count/self.batch_count*100:.1f}%\n")
                if total_attempts:
                    f.write(f"ğŸ”„ æ€»å°è¯•æ¬¡æ•°: {total_attempts}\n")
                    f.write(f"ğŸ’ª å¹³å‡æ¯æ‰¹æ¬¡å°è¯•: {total_attempts/self.batch_count:.1f} æ¬¡\n")
                f.write(f"\n")
                
                f.write("ğŸ“‹ æ‰¹æ¬¡è¯¦æƒ…:\n")
                f.write("-" * 40 + "\n")
                for batch in self.batch_history:
                    cycle_count = batch.get('cycles_count', 1)
                    f.write(f"æ‰¹æ¬¡ {batch['batch_number']}: {batch['start_date']} - {batch['end_date']} "
                           f"({batch['total_days']}å¤©, {cycle_count}å‘¨æœŸ, {len(batch['characters'])}è§’è‰², {len(batch['locations'])}åœ°ç‚¹)\n")
                
                if self.batch_history:
                    total_days = sum(batch['total_days'] for batch in self.batch_history)
                    total_cycles = sum(batch.get('cycles_count', 1) for batch in self.batch_history)
                    f.write(f"\nğŸ“Š ç»Ÿè®¡æ±‡æ€»:\n")
                    f.write(f"   ğŸ“… æ€»ç”Ÿæˆå¤©æ•°: {total_days} å¤©\n")
                    f.write(f"   ğŸ”„ æ€»å‘¨æœŸæ•°: {total_cycles} ä¸ª\n")
                    f.write(f"   ğŸ“Š å¹³å‡æ¯æ‰¹æ¬¡å¤©æ•°: {total_days/len(self.batch_history):.1f} å¤©\n")
                    f.write(f"   ğŸ“Š å¹³å‡æ¯æ‰¹æ¬¡å‘¨æœŸ: {total_cycles/len(self.batch_history):.1f} ä¸ª\n")
                    f.write(f"   ğŸ“Š å¹³å‡æ¯å‘¨æœŸå¤©æ•°: {total_days/total_cycles:.1f} å¤©\n")
                
                f.write(f"\nğŸ‰ æŠ¥å‘Šç”Ÿæˆå®Œæˆ!\n")
                f.write(f"ğŸ“ CSVè¾“å‡ºæ–‡ä»¶: workspace/batch_schedule_output/batch_schedules.csv\n")
            
            logger.info(f"ğŸ“‹ æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ€»ç»“æŠ¥å‘Šå¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ‰¹é‡æ—¥ç¨‹ç”Ÿæˆå™¨')
    parser.add_argument('--start-date', default='2025-07-03', help='å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--batch-count', type=int, default=30, help='æ‰¹æ¬¡æ•°é‡')
    
    args = parser.parse_args()
    
    print(f"æ‰¹é‡æ—¥ç¨‹ç”Ÿæˆå™¨å¯åŠ¨")
    print(f"å¼€å§‹æ—¥æœŸ: {args.start_date}")
    print(f"æ‰¹æ¬¡æ•°é‡: {args.batch_count}")
    print(f"è¾“å‡ºç›®å½•: workspace/batch_schedule_output/")
    
    try:
        generator = BatchScheduleGenerator(
            start_date=args.start_date,
            batch_count=args.batch_count
        )
        
        success_count, failed_count = await generator.generate_all_batches()
        
        print(f"æ‰¹é‡ç”Ÿæˆå®Œæˆ!")
        print(f"è¾“å‡ºç›®å½•: {generator.output_dir}")
        print(f"æˆåŠŸç‡: {success_count}/{generator.batch_count} ({success_count/generator.batch_count*100:.1f}%)")
        
    except KeyboardInterrupt:
        print(f"\nç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print(f"\nç¨‹åºé€€å‡º")
        sys.exit(0)


if __name__ == "__main__":
    # è®¾ç½®Windowså¼‚æ­¥äº‹ä»¶å¾ªç¯ç­–ç•¥
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    asyncio.run(main()) 