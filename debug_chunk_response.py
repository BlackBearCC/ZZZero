#!/usr/bin/env python3
"""
è°ƒè¯•DoubaoLLMçš„åŸå§‹chunkå“åº”
"""
import asyncio
import os
import sys
import json
import aiohttp
from typing import List

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from core.types import Message, MessageRole, LLMConfig
from llm.doubao import DoubaoLLM

async def test_raw_api_response():
    """æµ‹è¯•åŸå§‹APIå“åº”"""
    print("ğŸ” æµ‹è¯•åŸå§‹APIå“åº”...")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    api_key = os.getenv('ARK_API_KEY') or os.getenv('DOUBAO_API_KEY')
    if not api_key:
        print("âŒ æœªè®¾ç½®APIå¯†é’¥")
        return
    
    base_url = os.getenv('DOUBAO_BASE_URL', 'https://ark.cn-beijing.volces.com/api/v3')
    deepseek_model = os.getenv('DOUBAO_MODEL_DEEPSEEKR1', 'deepseek-reasoner')
    
    print(f"âœ… APIå¯†é’¥: {'*' * 10}...{api_key[-4:]}")
    print(f"âœ… åŸºç¡€URL: {base_url}")
    print(f"âœ… æ¨¡å‹åç§°: {deepseek_model}")
    
    # æ„å»ºè¯·æ±‚
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": deepseek_model,
        "messages": [
            {"role": "user", "content": "ç®€å•å›ç­”ï¼š1+1ç­‰äºå¤šå°‘ï¼Ÿ"}
        ],
        "temperature": 0.6,
        "max_tokens": 1000,
        "stream": True,
        "stream_options": {
            "include_usage": True
        }
    }
    
    print(f"\nğŸ“¤ å‘é€è¯·æ±‚æ•°æ®: {json.dumps(data, ensure_ascii=False, indent=2)}")
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{base_url}/chat/completions",
                headers=headers,
                json=data,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                print(f"\nğŸ“¥ å“åº”çŠ¶æ€: {response.status}")
                print(f"ğŸ“¥ å“åº”å¤´: {dict(response.headers)}")
                
                if response.status != 200:
                    error_text = await response.text()
                    print(f"âŒ APIè°ƒç”¨å¤±è´¥: {error_text}")
                    return
                
                chunk_count = 0
                reasoning_chunks = 0
                content_chunks = 0
                
                print("\nğŸ“Š åŸå§‹chunkæ•°æ®:")
                print("-" * 80)
                
                # å¤„ç†æµå¼å“åº”
                async for line in response.content:
                    line_text = line.decode('utf-8').strip()
                    if not line_text:
                        continue
                    
                    chunk_count += 1
                    print(f"[Chunk {chunk_count}] åŸå§‹è¡Œ: {repr(line_text)}")
                    
                    if line_text.startswith("data: "):
                        line_text = line_text[6:]
                        print(f"[Chunk {chunk_count}] å»é™¤å‰ç¼€å: {repr(line_text)}")
                        
                    if line_text == "[DONE]":
                        print(f"[Chunk {chunk_count}] æµç»“æŸæ ‡è®°")
                        break
                        
                    try:
                        chunk = json.loads(line_text)
                        print(f"[Chunk {chunk_count}] JSONè§£ææˆåŠŸ:")
                        print(f"  å®Œæ•´chunk: {json.dumps(chunk, ensure_ascii=False, indent=2)}")
                        
                        if chunk.get("choices") and len(chunk["choices"]) > 0:
                            choice = chunk["choices"][0]
                            delta = choice.get("delta", {})
                            
                            print(f"  deltaå†…å®¹: {json.dumps(delta, ensure_ascii=False, indent=2)}")
                            
                            # æ£€æŸ¥æ¨ç†å†…å®¹
                            if delta.get("reasoning_content"):
                                reasoning_chunks += 1
                                reasoning_chunk = delta["reasoning_content"]
                                print(f"  ğŸ§  æ¨ç†å†…å®¹ ({len(reasoning_chunk)} å­—ç¬¦): {repr(reasoning_chunk[:100])}")
                            
                            # æ£€æŸ¥æœ€ç»ˆç­”æ¡ˆå†…å®¹
                            if delta.get("content"):
                                content_chunks += 1
                                content_chunk = delta["content"]
                                print(f"  ğŸ’¬ ç­”æ¡ˆå†…å®¹ ({len(content_chunk)} å­—ç¬¦): {repr(content_chunk[:100])}")
                            
                            # æ£€æŸ¥å…¶ä»–å­—æ®µ
                            for key, value in delta.items():
                                if key not in ["reasoning_content", "content"]:
                                    print(f"  ğŸ” å…¶ä»–å­—æ®µ {key}: {repr(value)}")
                                    
                    except json.JSONDecodeError as e:
                        print(f"[Chunk {chunk_count}] JSONè§£æå¤±è´¥: {e}")
                        print(f"  åŸå§‹å†…å®¹: {repr(line_text)}")
                    
                    print("-" * 40)
                
                print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
                print(f"  æ€»chunkæ•°: {chunk_count}")
                print(f"  æ¨ç†chunkæ•°: {reasoning_chunks}")
                print(f"  å†…å®¹chunkæ•°: {content_chunks}")
                
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

async def test_llm_stream_think():
    """æµ‹è¯•LLMçš„stream_thinkæ–¹æ³•"""
    print("\nğŸ” æµ‹è¯•LLMçš„stream_thinkæ–¹æ³•...")
    
    try:
        # åˆ›å»ºLLMå®ä¾‹
        config = LLMConfig(
            provider="doubao",
            model_name="test",  # è¿™é‡Œä¸é‡è¦ï¼Œstream_thinkä¼šç”¨ç¯å¢ƒå˜é‡
            api_key=os.getenv('ARK_API_KEY') or os.getenv('DOUBAO_API_KEY'),
            api_base=os.getenv('DOUBAO_BASE_URL', 'https://ark.cn-beijing.volces.com/api/v3')
        )
        
        llm = DoubaoLLM(config)
        
        # åˆ›å»ºæµ‹è¯•æ¶ˆæ¯
        messages = [Message(role=MessageRole.USER, content="ç®€å•å›ç­”ï¼š1+1ç­‰äºå¤šå°‘ï¼Ÿ")]
        
        chunk_count = 0
        reasoning_chunks = 0
        content_chunks = 0
        
        print("\nğŸ“Š stream_thinkè¾“å‡º:")
        print("-" * 80)
        
        async for chunk_data in llm.stream_think(messages):
            chunk_count += 1
            chunk_type = chunk_data.get("type")
            
            print(f"[è¾“å‡º {chunk_count}] ç±»å‹: {chunk_type}")
            print(f"  å®Œæ•´æ•°æ®: {json.dumps(chunk_data, ensure_ascii=False, indent=2)}")
            
            if chunk_type == "reasoning_chunk":
                reasoning_chunks += 1
                content = chunk_data.get("content", "")
                print(f"  ğŸ§  æ¨ç†å†…å®¹ ({len(content)} å­—ç¬¦): {repr(content[:100])}")
                
            elif chunk_type == "content_chunk":
                content_chunks += 1
                content = chunk_data.get("content", "")
                print(f"  ğŸ’¬ ç­”æ¡ˆå†…å®¹ ({len(content)} å­—ç¬¦): {repr(content[:100])}")
                
            elif chunk_type == "think_complete":
                reasoning_content = chunk_data.get("reasoning_content", "")
                final_content = chunk_data.get("content", "")
                print(f"  âœ… å®Œæ•´æ¨ç† ({len(reasoning_content)} å­—ç¬¦): {repr(reasoning_content[:100])}")
                print(f"  âœ… å®Œæ•´ç­”æ¡ˆ ({len(final_content)} å­—ç¬¦): {repr(final_content[:100])}")
            
            print("-" * 40)
        
        print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
        print(f"  æ€»è¾“å‡ºæ•°: {chunk_count}")
        print(f"  æ¨ç†è¾“å‡ºæ•°: {reasoning_chunks}")
        print(f"  å†…å®¹è¾“å‡ºæ•°: {content_chunks}")
        
    except Exception as e:
        print(f"âŒ stream_thinkæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ DoubaoLLM chunkå“åº”è°ƒè¯•")
    print("=" * 80)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    api_key = os.getenv('ARK_API_KEY') or os.getenv('DOUBAO_API_KEY')
    if not api_key:
        print("âŒ æœªè®¾ç½®APIå¯†é’¥ï¼Œè¯·è®¾ç½® ARK_API_KEY æˆ– DOUBAO_API_KEY ç¯å¢ƒå˜é‡")
        return
    
    # æµ‹è¯•åŸå§‹APIå“åº”
    await test_raw_api_response()
    
    # æµ‹è¯•LLMçš„stream_thinkæ–¹æ³•
    await test_llm_stream_think()
    
    print("\nğŸ‰ è°ƒè¯•å®Œæˆï¼")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc() 