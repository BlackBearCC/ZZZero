#!/usr/bin/env python3
"""
è§’è‰²æ‰®æ¼”æ•°æ®ç”ŸæˆæœåŠ¡å™¨ - åŸºäºMCPåè®®çš„AIé©±åŠ¨çš„è§’è‰²æ‰®æ¼”æ•°æ®ç”ŸæˆæœåŠ¡
æ”¯æŒè®¡åˆ’æ—¥ç¨‹è¡¨ç”Ÿæˆã€è¯¦ç»†æ—¥ç¨‹ç”Ÿæˆç­‰åŠŸèƒ½
"""
import os
import sys

# é¦–å…ˆæ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import json
import asyncio
import logging
import random
import csv
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from enum import Enum

# ä½¿ç”¨æœ¬åœ°mcpæ¨¡å—
from mcp.server.stdio_server import StdioMCPServer
from mcp.types import Tool, Resource, JSONSchema, ToolInputSchema

# å¯¼å…¥é¡¹ç›®çš„LLMç³»ç»Ÿ
from src.llm.base import LLMFactory
from src.core.types import LLMConfig, Message, MessageRole

# å¯¼å…¥è§’è‰²æ’ä»¶ç³»ç»Ÿ
from src.core.plugins import get_role_plugin_manager, RolePluginManager

# ç¡®ä¿LLMæä¾›å•†å·²æ³¨å†Œ
try:
    from src.llm.doubao import DoubaoLLM  # æ³¨å†Œè±†åŒ…
except ImportError:
    logger.warning("è±†åŒ…LLMæ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œå°†ä¸æ”¯æŒè±†åŒ…")

try:
    from src.llm.openai import OpenAILLM  # æ³¨å†ŒOpenAIï¼ˆå¦‚æœå­˜åœ¨ï¼‰
except ImportError:
    logger.info("OpenAI LLMæ¨¡å—æœªæ‰¾åˆ°ï¼Œå¦‚éœ€ä½¿ç”¨è¯·å®ç°ç›¸åº”æ¨¡å—")

logger = logging.getLogger(__name__)


class TimePhase(Enum):
    """æ—¶é—´é˜¶æ®µæšä¸¾"""
    MORNING = ("ä¸Šåˆ", "06:00-11:00", "06:00", "11:00")
    NOON = ("ä¸­åˆ", "11:00-14:00", "11:00", "14:00")
    AFTERNOON = ("ä¸‹åˆ", "14:00-18:00", "14:00", "18:00")
    EVENING = ("æ™šä¸Š", "18:00-23:00", "18:00", "23:00")
    NIGHT = ("å¤œé—´", "23:00-06:00", "23:00", "06:00")
    
    def __init__(self, name: str, time_range: str, start_time: str, end_time: str):
        self.phase_name = name
        self.time_range = time_range
        self.start_time = start_time
        self.end_time = end_time


class AnnualScheduleData:
    """å¹´åº¦æ—¥ç¨‹æ•°æ®ç»“æ„"""
    
    def __init__(self):
        self.csv_events = []  # CSVä¸­çš„å¹´åº¦äº‹ä»¶
        self.daily_summaries = {}  # æ¯æ—¥æ‘˜è¦ {day_index: summary}
        self.weekly_compressions = {}  # æ¯å‘¨å‹ç¼©æ‘˜è¦ {week_index: compressed_summary}
        self.generation_progress = {
            "current_day": 0,
            "total_days": 365,
            "completed_days": 0,
            "started_at": None,
            "estimated_completion": None,
            "status": "not_started"  # not_started, in_progress, completed, paused, error
        }
        self.character_description = ""
        self.csv_file_path = ""


class AnnualScheduleManager:
    """å¹´åº¦æ—¥ç¨‹ç®¡ç†å™¨ - è´Ÿè´£CSVè¯»å–å’Œæ—¥ç¨‹æ•°æ®ç®¡ç†"""
    
    def __init__(self):
        self.current_schedule_data = None
        self.output_dir = Path("workspace/annual_schedule_output")
        self.output_dir.mkdir(exist_ok=True)
        
    def load_csv_schedule(self, csv_file_path: str) -> Tuple[bool, str, List[Dict[str, Any]]]:
        """åŠ è½½CSVå¹´åº¦æ—¥ç¨‹æ–‡ä»¶"""
        try:
            csv_path = Path(csv_file_path)
            if not csv_path.exists():
                return False, f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file_path}", []
            
            events = []
            with open(csv_path, 'r', encoding='utf-8') as file:
                reader = csv.DictReader(file)
                for row_idx, row in enumerate(reader, 1):
                    # éªŒè¯å¿…è¦å­—æ®µ
                    required_fields = ['æœˆä»½', 'æ—¥æœŸ', 'æ´»åŠ¨ç±»å‹', 'å…·ä½“å®‰æ’']
                    missing_fields = [field for field in required_fields if not row.get(field, '').strip()]
                    
                    if missing_fields:
                        logger.warning(f"ç¬¬{row_idx}è¡Œç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")
                        continue
                    
                    # è§£ææ—¥æœŸèŒƒå›´
                    date_range = row.get('æ—¥æœŸ', '').strip()
                    start_date, end_date = self._parse_date_range(row.get('æœˆä»½', ''), date_range)
                    
                    if not start_date:
                        logger.warning(f"ç¬¬{row_idx}è¡Œæ—¥æœŸè§£æå¤±è´¥: {date_range}")
                        continue
                    
                    event = {
                        'row_index': row_idx,
                        'month': row.get('æœˆä»½', '').strip(),
                        'date_range': date_range,
                        'start_date': start_date,
                        'end_date': end_date,
                        'activity_type': row.get('æ´»åŠ¨ç±»å‹', '').strip(),
                        'activity_name': row.get('å…·ä½“å®‰æ’', '').strip(),
                        'location': row.get('åœ°ç‚¹', '').strip(),
                        'remarks': row.get('å¤‡æ³¨', '').strip(),
                        'duration_days': (end_date - start_date).days + 1
                    }
                    events.append(event)
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½CSVæ—¥ç¨‹: {len(events)} ä¸ªäº‹ä»¶")
            return True, f"æˆåŠŸåŠ è½½ {len(events)} ä¸ªå¹´åº¦äº‹ä»¶", events
            
        except Exception as e:
            error_msg = f"åŠ è½½CSVæ–‡ä»¶å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return False, error_msg, []
    
    def _parse_date_range(self, month_str: str, date_range: str) -> Tuple[Optional[datetime], Optional[datetime]]:
        """è§£ææ—¥æœŸèŒƒå›´ï¼Œè¿”å›å¼€å§‹å’Œç»“æŸæ—¥æœŸ"""
        try:
            # è§£ææœˆä»½
            month = int(month_str.replace('æœˆ', ''))
            year = 2024  # é»˜è®¤å¹´ä»½
            
            # è§£ææ—¥æœŸèŒƒå›´ "01-05" æˆ– "15-17"
            if '-' in date_range:
                start_day, end_day = date_range.split('-')
                start_day = int(start_day)
                end_day = int(end_day)
            else:
                # å•æ—¥äº‹ä»¶
                start_day = end_day = int(date_range)
            
            start_date = datetime(year, month, start_day)
            end_date = datetime(year, month, end_day)
            
            return start_date, end_date
            
        except Exception as e:
            logger.error(f"æ—¥æœŸè§£æå¤±è´¥ {month_str}-{date_range}: {e}")
            return None, None
    
    def get_day_events(self, day_index: int, base_date: datetime) -> List[Dict[str, Any]]:
        """è·å–æŒ‡å®šå¤©çš„äº‹ä»¶ï¼ˆday_index: 0-364ï¼‰"""
        if not self.current_schedule_data:
            return []
        
        current_date = base_date + timedelta(days=day_index)
        day_events = []
        
        for event in self.current_schedule_data.csv_events:
            if event['start_date'] <= current_date <= event['end_date']:
                # è®¡ç®—äº‹ä»¶åœ¨å½“å¤©çš„é˜¶æ®µï¼ˆç¬¬å‡ å¤©ï¼‰
                event_day_offset = (current_date - event['start_date']).days
                
                day_event = event.copy()
                day_event['event_day_offset'] = event_day_offset
                day_event['is_event_start'] = (event_day_offset == 0)
                day_event['is_event_end'] = (current_date == event['end_date'])
                day_event['current_date'] = current_date
                
                day_events.append(day_event)
        
        return day_events
    
    def save_daily_schedule(self, day_index: int, daily_data: Dict[str, Any]) -> bool:
        """ä¿å­˜å•æ—¥è¯¦ç»†æ—¥ç¨‹"""
        try:
            filename = f"day_{day_index + 1:03d}_{daily_data.get('date', 'unknown')}.json"
            filepath = self.output_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(daily_data, f, ensure_ascii=False, indent=2)
            
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜ç¬¬{day_index + 1}å¤©æ—¥ç¨‹å¤±è´¥: {e}")
            return False
    
    def save_weekly_compression(self, week_index: int, compression_data: Dict[str, Any]) -> bool:
        """ä¿å­˜æ¯å‘¨å‹ç¼©æ‘˜è¦"""
        try:
            filename = f"week_{week_index + 1:02d}_compression.json"
            filepath = self.output_dir / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(compression_data, f, ensure_ascii=False, indent=2)
            
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜ç¬¬{week_index + 1}å‘¨å‹ç¼©æ‘˜è¦å¤±è´¥: {e}")
            return False
    
    def get_progress_summary(self) -> Dict[str, Any]:
        """è·å–ç”Ÿæˆè¿›åº¦æ‘˜è¦"""
        if not self.current_schedule_data:
            return {"error": "æœªåŠ è½½å¹´åº¦æ—¥ç¨‹æ•°æ®"}
        
        progress = self.current_schedule_data.generation_progress
        return {
            "current_day": progress["current_day"],
            "completed_days": progress["completed_days"],
            "total_days": progress["total_days"],
            "progress_percentage": (progress["completed_days"] / progress["total_days"]) * 100,
            "status": progress["status"],
            "started_at": progress["started_at"],
            "estimated_completion": progress["estimated_completion"]
        }


class ConfigManager:
    """é…ç½®ç®¡ç†å™¨ - ç®¡ç†åœ°ç‚¹ã€å¤©æ°”ã€æƒ…ç»ªé…ç½®æ•°æ®"""
    
    def __init__(self):
        # åœ°ç‚¹é…ç½®
        self.locations = {
            "å±…ä½åœºæ‰€": ["å§å®¤", "å®¢å…", "å¨æˆ¿", "é˜³å°", "èŠ±å›­", "ä¹¦æˆ¿", "å·¥ä½œå®¤"],
            "å·¥ä½œåœºæ‰€": ["åŠå…¬å®¤", "ä¼šè®®å®¤", "å®éªŒå®¤", "å·¥å‚", "å•†åº—", "é¤å…", "å­¦æ ¡"],
            "ä¼‘é—²åœºæ‰€": ["å…¬å›­", "å’–å•¡å…", "å›¾ä¹¦é¦†", "å¥èº«æˆ¿", "ç”µå½±é™¢", "å•†åœº", "æµ·è¾¹"],
            "ç¤¾äº¤åœºæ‰€": ["æœ‹å‹å®¶", "ç¤¾åŒºä¸­å¿ƒ", "ä¿±ä¹éƒ¨", "èšä¼šåœºæ‰€", "å®´ä¼šå…"],
            "æˆ·å¤–åœºæ‰€": ["å±±æ—", "æ¹–æ³Š", "åŸå¸‚å¹¿åœº", "è¡—é“", "æ™¯åŒº", "è¿åŠ¨åœº"]
        }
        
        # å¤©æ°”é…ç½®
        self.weather = {
            "æ™´æœ—": ["é˜³å…‰æ˜åªš", "å¾®é£å¾å¾", "ä¸‡é‡Œæ— äº‘", "æ¸©æš–èˆ’é€‚"],
            "é˜´å¤©": ["å¤šäº‘", "å‡‰çˆ½", "å¾®é£", "é€‚åˆæˆ·å¤–æ´»åŠ¨"],
            "é›¨å¤©": ["å°é›¨", "ä¸­é›¨", "å¤§é›¨", "é›·é›¨", "æ¯›æ¯›é›¨", "é˜µé›¨"],
            "ç‰¹æ®Šå¤©æ°”": ["é›ªå¤©", "é›¾å¤©", "æ²™å°˜", "ç‚çƒ­", "å¯’å†·"]
        }
        
        # æƒ…ç»ªæ°›å›´é…ç½®
        self.emotions = {
            "ç§¯ææƒ…ç»ª": ["å…´å¥‹", "æ„‰æ‚¦", "ä¸“æ³¨", "å……æ»¡æ´»åŠ›", "æ»¡è¶³", "å¹³é™"],
            "ä¸­æ€§æƒ…ç»ª": ["å¹³å¸¸", "æ·¡å®š", "æ€è€ƒ", "è§‚å¯Ÿ", "ç­‰å¾…", "å‡†å¤‡"],
            "æŒ‘æˆ˜æƒ…ç»ª": ["ç´§å¼ ", "å¿™ç¢Œ", "å‹åŠ›", "æœŸå¾…", "ä¸å®‰", "å›°æƒ‘"]
        }
    
    def get_random_location(self, category: str = None) -> str:
        """éšæœºè·å–åœ°ç‚¹"""
        if category and category in self.locations:
            return random.choice(self.locations[category])
        # éšæœºé€‰æ‹©åˆ†ç±»å’Œåœ°ç‚¹
        category = random.choice(list(self.locations.keys()))
        return random.choice(self.locations[category])
    
    def get_random_weather(self, category: str = None) -> str:
        """éšæœºè·å–å¤©æ°”"""
        if category and category in self.weather:
            return random.choice(self.weather[category])
        # éšæœºé€‰æ‹©åˆ†ç±»å’Œå¤©æ°”
        category = random.choice(list(self.weather.keys()))
        return random.choice(self.weather[category])
    
    def get_random_emotion(self, category: str = None) -> str:
        """éšæœºè·å–æƒ…ç»ª"""
        if category and category in self.emotions:
            return random.choice(self.emotions[category])
        # éšæœºé€‰æ‹©åˆ†ç±»å’Œæƒ…ç»ª
        category = random.choice(list(self.emotions.keys()))
        return random.choice(self.emotions[category])
    
    def get_all_config(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰é…ç½®"""
        return {
            "locations": self.locations,
            "weather": self.weather,
            "emotions": self.emotions
        }


class PromptManager:
    """æç¤ºè¯ç®¡ç†å™¨ - ç®¡ç†é¢„ç½®çš„æ ‡å‡†æç¤ºè¯æ¨¡æ¿"""
    
    def __init__(self):
        # å•æ—¥è¯¦ç»†æ—¥ç¨‹ç”Ÿæˆæç¤ºè¯
        self.daily_schedule_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§’è‰²æ‰®æ¼”æ—¥ç¨‹è§„åˆ’ä¸“å®¶ã€‚è¯·ä¸ºæŒ‡å®šè§’è‰²ç”Ÿæˆä»Šå¤©çš„è¯¦ç»†5é˜¶æ®µæ—¥ç¨‹å®‰æ’ã€‚

ã€è§’è‰²è®¾å®šã€‘
{character_description}

ã€ä»Šæ—¥åŸºæœ¬ä¿¡æ¯ã€‘
- æ—¥æœŸ: {current_date}
- æ˜ŸæœŸ: {weekday}
- å¤©æ•°: ç¬¬{day_index}å¤©

ã€ä»Šæ—¥é¢„å®šæ´»åŠ¨ã€‘
{scheduled_events}

ã€æ˜¨æ—¥æ´»åŠ¨æ‘˜è¦ã€‘
{previous_day_summary}

ã€è¿‘æœŸèƒŒæ™¯ä¿¡æ¯ã€‘
{recent_context}

ã€çŸ¥è¯†åº“å‚è€ƒä¿¡æ¯ã€‘
{knowledge_references}

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼ç”Ÿæˆä»Šæ—¥è¯¦ç»†5é˜¶æ®µæ—¥ç¨‹ï¼š

```json
{{
  "daily_summary": "ä»Šæ—¥æ•´ä½“å®‰æ’çš„ç®€è¦æ¦‚è¿°ï¼ˆä¸è¶…è¿‡100å­—ï¼‰",
  "character_state": "è§’è‰²ä»Šæ—¥çš„å¿ƒç†çŠ¶æ€å’Œç²¾ç¥çŠ¶å†µæè¿°",
  "morning": [
    {{
      "activity_name": "æ´»åŠ¨åç§°",
      "time_detail": "å…·ä½“æ—¶é—´å®‰æ’",
      "location": "æ´»åŠ¨åœ°ç‚¹",
      "details": "æ´»åŠ¨çš„è¯¦ç»†æè¿°ï¼ŒåŒ…æ‹¬èƒŒæ™¯åŸå› ã€ç›®çš„ã€å…·ä½“è¡Œä¸ºã€è§’è‰²å¿ƒç†ç­‰"
    }}
  ],
  "noon": [
    {{
      "activity_name": "æ´»åŠ¨åç§°",
      "time_detail": "å…·ä½“æ—¶é—´å®‰æ’", 
      "location": "æ´»åŠ¨åœ°ç‚¹",
      "details": "æ´»åŠ¨çš„è¯¦ç»†æè¿°"
    }}
  ],
  "afternoon": [
    {{
      "activity_name": "æ´»åŠ¨åç§°",
      "time_detail": "å…·ä½“æ—¶é—´å®‰æ’",
      "location": "æ´»åŠ¨åœ°ç‚¹", 
      "details": "æ´»åŠ¨çš„è¯¦ç»†æè¿°"
    }}
  ],
  "evening": [
    {{
      "activity_name": "æ´»åŠ¨åç§°",
      "time_detail": "å…·ä½“æ—¶é—´å®‰æ’",
      "location": "æ´»åŠ¨åœ°ç‚¹",
      "details": "æ´»åŠ¨çš„è¯¦ç»†æè¿°"
    }}
  ],
  "night": [
    {{
      "activity_name": "æ´»åŠ¨åç§°",
      "time_detail": "å…·ä½“æ—¶é—´å®‰æ’",
      "location": "æ´»åŠ¨åœ°ç‚¹",
      "details": "æ´»åŠ¨çš„è¯¦ç»†æè¿°"
    }}
  ]
}}
```

ç”Ÿæˆè¦æ±‚ï¼š
1. **ä¸¥æ ¼éµå¾ªè§’è‰²è®¾å®š**ï¼šæ‰€æœ‰æ´»åŠ¨å®‰æ’å¿…é¡»ç¬¦åˆè§’è‰²çš„æ€§æ ¼ç‰¹ç‚¹ã€ç”Ÿæ´»ä¹ æƒ¯ã€èŒä¸šç‰¹å¾
2. **èåˆé¢„å®šæ´»åŠ¨**ï¼šå·§å¦™åœ°å°†ä»Šæ—¥çš„é¢„å®šæ´»åŠ¨èå…¥åˆ°5ä¸ªæ—¶é—´æ®µä¸­ï¼Œç¡®ä¿æ´»åŠ¨çš„åˆç†æ€§å’Œè¿è´¯æ€§
3. **è€ƒè™‘æ˜¨æ—¥è¡”æ¥**ï¼šå‚è€ƒæ˜¨æ—¥æ´»åŠ¨æ‘˜è¦ï¼Œç¡®ä¿ä»Šæ—¥å®‰æ’çš„å»¶ç»­æ€§å’Œé€»è¾‘æ€§
4. **åˆ©ç”¨çŸ¥è¯†åº“ä¿¡æ¯**ï¼šç»“åˆçŸ¥è¯†åº“ä¸­çš„è§’è‰²èƒŒæ™¯ä¿¡æ¯ï¼Œä¸°å¯Œæ´»åŠ¨çš„ç»†èŠ‚å’Œæ·±åº¦
5. **æ—¶é—´å®‰æ’åˆç†**ï¼šæ¯ä¸ªæ—¶é—´æ®µçš„æ´»åŠ¨è¦ç¬¦åˆè¯¥æ—¶æ®µçš„ç‰¹ç‚¹ï¼Œæ´»åŠ¨é—´æœ‰è‡ªç„¶è¿‡æ¸¡
6. **ç»†èŠ‚ç”ŸåŠ¨å…·ä½“**ï¼šdetailså­—æ®µè¦åŒ…å«è§’è‰²çš„å¿ƒç†æ´»åŠ¨ã€å…·ä½“è¡Œä¸ºã€ç¯å¢ƒæè¿°ç­‰
7. **ä¿æŒè§’è‰²ä¸€è‡´æ€§**ï¼šæ•´å¤©çš„å®‰æ’è¦ä½“ç°è§’è‰²çš„ä¸ªäººé£æ ¼å’Œç”Ÿæ´»èŠ‚å¥
- ä½¿ç”¨ä¸­æ–‡å›å¤
- å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡º"""

        # å‘¨åº¦å‹ç¼©æç¤ºè¯
        self.weekly_compression_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§’è‰²æ‰®æ¼”æ•°æ®åˆ†æå¸ˆã€‚è¯·å¯¹æŒ‡å®šè§’è‰²è¿‡å»7å¤©çš„æ—¥ç¨‹å®‰æ’è¿›è¡Œæ™ºèƒ½å‹ç¼©æ€»ç»“ã€‚

ã€è§’è‰²è®¾å®šã€‘
{character_description}

ã€æœ¬å‘¨æ—¶é—´èŒƒå›´ã€‘
ç¬¬{week_index}å‘¨ ({start_date} è‡³ {end_date})

ã€æœ¬å‘¨æ¯æ—¥æ‘˜è¦ã€‘
{daily_summaries}

ã€ä¸Šå‘¨å‹ç¼©æ‘˜è¦ã€‘
{previous_week_summary}

ã€çŸ¥è¯†åº“éªŒè¯ä¿¡æ¯ã€‘
{knowledge_verification}

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼ç”Ÿæˆæœ¬å‘¨å‹ç¼©æ‘˜è¦ï¼š

```json
{{
  "week_summary": "æœ¬å‘¨æ•´ä½“æƒ…å†µçš„ç»¼åˆæ¦‚è¿°ï¼ˆ200-300å­—ï¼‰",
  "character_development": "è§’è‰²åœ¨æœ¬å‘¨çš„å¿ƒç†çŠ¶æ€å˜åŒ–å’Œæˆé•¿è½¨è¿¹",
  "key_activities": [
    {{
      "activity_type": "æ´»åŠ¨ç±»å‹",
      "frequency": "å‡ºç°é¢‘æ¬¡",
      "importance": "é‡è¦ç¨‹åº¦è¯„çº§ï¼ˆ1-5ï¼‰",
      "description": "æ´»åŠ¨æè¿°å’Œå½±å“"
    }}
  ],
  "relationship_dynamics": "è§’è‰²ä¸ä»–äººçš„äº’åŠ¨æƒ…å†µå’Œå…³ç³»å˜åŒ–",
  "habit_patterns": "å‘ç°çš„ç”Ÿæ´»ä¹ æƒ¯æ¨¡å¼å’Œè¡Œä¸ºè§„å¾‹",
  "emotional_trends": "æƒ…ç»ªå˜åŒ–è¶‹åŠ¿å’Œä¸»è¦æ„Ÿå—",
  "continuity_notes": "éœ€è¦åœ¨ä¸‹å‘¨å»¶ç»­æˆ–å…³æ³¨çš„é‡è¦äº‹é¡¹",
  "character_consistency_check": "è§’è‰²è¡Œä¸ºä¸è®¾å®šçš„ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ"
}}
```

åˆ†æè¦æ±‚ï¼š
1. **ä¿æŒè§’è‰²ä¸€è‡´æ€§**ï¼šæ£€æŸ¥æœ¬å‘¨çš„æ´»åŠ¨å®‰æ’æ˜¯å¦ç¬¦åˆè§’è‰²è®¾å®šï¼Œå‘ç°å¹¶è®°å½•ä»»ä½•åå·®
2. **æå–å…³é”®æ¨¡å¼**ï¼šè¯†åˆ«è§’è‰²çš„è¡Œä¸ºæ¨¡å¼ã€æƒ…æ„Ÿå˜åŒ–ã€ç¤¾äº¤çŠ¶å†µç­‰é‡è¦è¶‹åŠ¿
3. **ç¡®ä¿è¿è´¯æ€§**ï¼šåˆ†ææœ¬å‘¨ä¸ä¸Šå‘¨çš„è¿æ¥ç‚¹ï¼Œä¸ºä¸‹å‘¨çš„å®‰æ’æä¾›èƒŒæ™¯æ”¯æ’‘
4. **æ·±åº¦åˆ†æ**ï¼šä¸ä»…è®°å½•å‘ç”Ÿäº†ä»€ä¹ˆï¼Œæ›´è¦åˆ†æä¸ºä»€ä¹ˆå‘ç”Ÿï¼Œå¯¹è§’è‰²çš„æ„ä¹‰æ˜¯ä»€ä¹ˆ
5. **å‰ç»è§„åˆ’**ï¼šåŸºäºæœ¬å‘¨çš„å‘å±•ï¼Œé¢„åˆ¤ä¸‹å‘¨å¯èƒ½çš„å‘å±•æ–¹å‘å’Œéœ€è¦å…³æ³¨çš„é‡ç‚¹
6. **éªŒè¯åˆç†æ€§**ï¼šç»“åˆçŸ¥è¯†åº“ä¿¡æ¯ï¼ŒéªŒè¯è§’è‰²è¡Œä¸ºçš„åˆç†æ€§å’Œä¸“ä¸šæ€§
- ä½¿ç”¨ä¸­æ–‡å›å¤
- å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡º"""

        # è§’è‰²éªŒè¯æç¤ºè¯  
        self.character_verification_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§’è‰²æ‰®æ¼”ä¸€è‡´æ€§æ£€æŸ¥ä¸“å®¶ã€‚è¯·æ ¹æ®è§’è‰²è®¾å®šå’ŒçŸ¥è¯†åº“ä¿¡æ¯ï¼Œå¯¹è¿‘æœŸçš„è¡Œä¸ºå®‰æ’è¿›è¡ŒéªŒè¯ã€‚

ã€è§’è‰²è®¾å®šã€‘
{character_description}

ã€æ£€æŸ¥æ—¶é—´èŒƒå›´ã€‘
{time_range}

ã€è¿‘æœŸè¡Œä¸ºæ‘˜è¦ã€‘
{behavior_summary}

ã€çŸ¥è¯†åº“æ ¸å¿ƒä¿¡æ¯ã€‘
{knowledge_core}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿›è¡ŒéªŒè¯åˆ†æï¼š

```json
{{
  "consistency_score": "ä¸€è‡´æ€§è¯„åˆ†ï¼ˆ0-100ï¼‰",
  "verification_result": "æ€»ä½“éªŒè¯ç»“æœï¼ˆé€šè¿‡/è­¦å‘Š/ä¸é€šè¿‡ï¼‰",
  "analysis_details": {{
    "personality_match": "æ€§æ ¼ç‰¹å¾åŒ¹é…åº¦åˆ†æ",
    "lifestyle_match": "ç”Ÿæ´»æ–¹å¼åŒ¹é…åº¦åˆ†æ", 
    "professional_match": "èŒä¸šç‰¹å¾åŒ¹é…åº¦åˆ†æ",
    "relationship_match": "äººé™…å…³ç³»åŒ¹é…åº¦åˆ†æ"
  }},
  "identified_issues": [
    {{
      "issue_type": "é—®é¢˜ç±»å‹",
      "severity": "ä¸¥é‡ç¨‹åº¦ï¼ˆé«˜/ä¸­/ä½ï¼‰",
      "description": "é—®é¢˜æè¿°",
      "suggestion": "æ”¹è¿›å»ºè®®"
    }}
  ],
  "positive_highlights": [
    "è¡¨ç°è‰¯å¥½çš„è§’è‰²ç‰¹å¾ä½“ç°"
  ],
  "adjustment_recommendations": "ä¸‹ä¸€é˜¶æ®µçš„è°ƒæ•´å»ºè®®"
}}
```

éªŒè¯è¦æ±‚ï¼š
1. **å…¨é¢æ€§æ£€æŸ¥**ï¼šä»æ€§æ ¼ã€ç”Ÿæ´»æ–¹å¼ã€èŒä¸šç‰¹å¾ã€äººé™…å…³ç³»ç­‰å¤šç»´åº¦éªŒè¯
2. **ä¸“ä¸šæ€§åˆ¤æ–­**ï¼šç»“åˆçŸ¥è¯†åº“ä¸­çš„ä¸“ä¸šä¿¡æ¯ï¼ŒéªŒè¯è§’è‰²è¡Œä¸ºçš„ä¸“ä¸šå‡†ç¡®æ€§
3. **è¿è´¯æ€§åˆ†æ**ï¼šæ£€æŸ¥è§’è‰²è¡Œä¸ºçš„å‰åä¸€è‡´æ€§å’Œå‘å±•çš„åˆç†æ€§
4. **å»ºè®¾æ€§åé¦ˆ**ï¼šä¸ä»…æŒ‡å‡ºé—®é¢˜ï¼Œæ›´è¦æä¾›å…·ä½“çš„æ”¹è¿›æ–¹æ¡ˆ
5. **åŠ¨æ€é€‚åº”æ€§**ï¼šè€ƒè™‘è§’è‰²å¯èƒ½çš„åˆç†å˜åŒ–å’Œæˆé•¿è½¨è¿¹
- ä½¿ç”¨ä¸­æ–‡å›å¤
- å¿…é¡»ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡º"""
    
    def get_daily_schedule_prompt(self, character_description: str, current_date: str, 
                                 weekday: str, day_index: int, scheduled_events: str, 
                                 previous_day_summary: str, recent_context: str, 
                                 knowledge_references: str) -> str:
        """è·å–å•æ—¥è¯¦ç»†æ—¥ç¨‹ç”Ÿæˆæç¤ºè¯"""
        return self.daily_schedule_prompt.format(
            character_description=character_description or "æœªæŒ‡å®šè§’è‰²è®¾å®š",
            current_date=current_date,
            weekday=weekday,
            day_index=day_index,
            scheduled_events=scheduled_events or "ä»Šæ—¥æ— ç‰¹åˆ«é¢„å®šæ´»åŠ¨",
            previous_day_summary=previous_day_summary or "æ˜¨æ—¥ä¿¡æ¯ä¸å¯ç”¨",
            recent_context=recent_context or "æ— è¿‘æœŸèƒŒæ™¯ä¿¡æ¯",
            knowledge_references=knowledge_references or "æ— ç›¸å…³çŸ¥è¯†åº“ä¿¡æ¯"
        )
    
    def get_weekly_compression_prompt(self, character_description: str, week_index: int,
                                    start_date: str, end_date: str, daily_summaries: str,
                                    previous_week_summary: str, knowledge_verification: str) -> str:
        """è·å–å‘¨åº¦å‹ç¼©æç¤ºè¯"""
        return self.weekly_compression_prompt.format(
            character_description=character_description or "æœªæŒ‡å®šè§’è‰²è®¾å®š",
            week_index=week_index,
            start_date=start_date,
            end_date=end_date,
            daily_summaries=daily_summaries or "æœ¬å‘¨æ—¥ç¨‹æ‘˜è¦ä¸å¯ç”¨",
            previous_week_summary=previous_week_summary or "ä¸Šå‘¨æ‘˜è¦ä¸å¯ç”¨",
            knowledge_verification=knowledge_verification or "æ— çŸ¥è¯†åº“éªŒè¯ä¿¡æ¯"
        )
        
    def get_character_verification_prompt(self, character_description: str, time_range: str,
                                        behavior_summary: str, knowledge_core: str) -> str:
        """è·å–è§’è‰²éªŒè¯æç¤ºè¯"""
        return self.character_verification_prompt.format(
            character_description=character_description or "æœªæŒ‡å®šè§’è‰²è®¾å®š",
            time_range=time_range,
            behavior_summary=behavior_summary or "æ— è¡Œä¸ºæ‘˜è¦",
            knowledge_core=knowledge_core or "æ— çŸ¥è¯†åº“æ ¸å¿ƒä¿¡æ¯"
        )


class LLMCaller:
    """LLMè°ƒç”¨å™¨ - ä½¿ç”¨é¡¹ç›®çš„ç»Ÿä¸€LLMæ¡†æ¶"""
    
    def __init__(self):
        self.llm_provider = None
        self._initialize_llm()
        
    def _initialize_llm(self):
        """åˆå§‹åŒ–LLMæä¾›è€…"""
        # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        provider = os.getenv("LLM_PROVIDER", "doubao")  # é»˜è®¤ä½¿ç”¨è±†åŒ…
        model_name = os.getenv("LLM_MODEL_NAME", "ep-20250221154410-vh78x")  # è±†åŒ…é»˜è®¤æ¨¡å‹
        api_key = os.getenv("ARK_API_KEY") or os.getenv("DOUBAO_API_KEY") or os.getenv("OPENAI_API_KEY")
        api_base = os.getenv("DOUBAO_BASE_URL") or os.getenv("OPENAI_BASE_URL")
        
        # æ ¹æ®providerè‡ªåŠ¨è®¾ç½®é»˜è®¤å€¼
        if provider == "doubao":
            api_base = api_base or "https://ark.cn-beijing.volces.com/api/v3"
            model_name = model_name or "ep-20250221154410-vh78x"
        elif provider == "openai":
            api_base = api_base or "https://api.openai.com/v1"
            model_name = model_name or "gpt-3.5-turbo"
        
        try:
            # åˆ›å»ºLLMé…ç½®
            llm_config = LLMConfig(
                provider=provider,
                model_name=model_name,
                api_key=api_key,
                api_base=api_base,
                temperature=0.4,
                timeout=600  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°60ç§’
            )
            
            # åˆ›å»ºLLMå®ä¾‹
            self.llm_provider = LLMFactory.create(llm_config)
            logger.info(f"âœ… LLMè°ƒç”¨å™¨åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨æä¾›å•†: {provider}ï¼Œæ¨¡å‹: {model_name}")
            
        except Exception as e:
            logger.error(f"âŒ LLMè°ƒç”¨å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.error(f"æä¾›å•†: {provider}, æ¨¡å‹: {model_name}, APIå¯†é’¥å·²è®¾ç½®: {bool(api_key)}")
            self.llm_provider = None
    
    async def call_llm(self, prompt: str, max_tokens: int = 2000, 
                       temperature: float = 0.7) -> Tuple[bool, str]:
        """ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£"""
        if not self.llm_provider:
            return False, "LLMæœåŠ¡æœªæ­£ç¡®åˆå§‹åŒ–"
        
        try:
            # ç¡®ä¿LLMå·²åˆå§‹åŒ–
            await self.llm_provider.initialize()
            
            # æ„å»ºæ¶ˆæ¯
            messages = [
                Message(role=MessageRole.SYSTEM, 
                       content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—¥ç¨‹è§„åˆ’åŠ©æ‰‹ï¼Œèƒ½å¤Ÿç”Ÿæˆè¯¦ç»†ã€å®ç”¨çš„æ—¥ç¨‹å®‰æ’ã€‚"),
                Message(role=MessageRole.USER, content=prompt)
            ]
            
            # è°ƒç”¨LLMç”Ÿæˆ
            response = await self.llm_provider.generate(
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            return True, response.content
            
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False, f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"
            
    async def cleanup(self):
        """æ¸…ç†LLMèµ„æº"""
        if self.llm_provider:
            try:
                await self.llm_provider.cleanup()
            except Exception as e:
                logger.warning(f"LLMæ¸…ç†å¤±è´¥: {e}")


class RolePlayDataGenerator:
    """è§’è‰²æ‰®æ¼”æ•°æ®ç”Ÿæˆå™¨ - æ ¸å¿ƒç”Ÿæˆé€»è¾‘"""
    
    def __init__(self):
        self.prompt_manager = PromptManager()
        self.llm_caller = LLMCaller()
        self.config_manager = ConfigManager()
        self.generation_history = []
        
        # åˆå§‹åŒ–å¹´åº¦æ—¥ç¨‹ç®¡ç†å™¨
        self.annual_manager = AnnualScheduleManager()
        
        # åˆå§‹åŒ–è§’è‰²æ’ä»¶ç®¡ç†å™¨
        self.role_plugin_manager = get_role_plugin_manager()
        logger.info("è§’è‰²æ’ä»¶ç®¡ç†å™¨å·²é›†æˆåˆ°è§’è‰²æ‰®æ¼”æ•°æ®ç”Ÿæˆå™¨")
        
        # è‡ªåŠ¨åˆå§‹åŒ–çŸ¥è¯†åº“
        asyncio.create_task(self._initialize_knowledge_base())
    
    async def _initialize_knowledge_base(self):
        """è‡ªåŠ¨åˆå§‹åŒ–çŸ¥è¯†åº“ï¼ŒåŠ è½½roleplay_data_README.md"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰çŸ¥è¯†åº“é…ç½®
            status = self.role_plugin_manager.get_status()
            kb_info = status.get('knowledge_base_plugin', {})
            
            if kb_info.get('enabled') and kb_info.get('available'):
                logger.info("çŸ¥è¯†åº“å·²å­˜åœ¨ä¸”å¯ç”¨ï¼Œè·³è¿‡è‡ªåŠ¨åˆå§‹åŒ–")
                return
            
            # è·å–READMEæ–‡ä»¶è·¯å¾„
            readme_path = Path(__file__).parent / "roleplay_data_README.md"
            
            if not readme_path.exists():
                logger.warning(f"READMEæ–‡ä»¶ä¸å­˜åœ¨: {readme_path}")
                return
            
            # é…ç½®çŸ¥è¯†åº“
            await self.role_plugin_manager.configure_knowledge_base(
                name="è§’è‰²æ‰®æ¼”æ•°æ®ç”ŸæˆæœåŠ¡çŸ¥è¯†åº“",
                source_file=str(readme_path),
                description="åŒ…å«è§’è‰²æ‰®æ¼”æ•°æ®ç”ŸæˆæœåŠ¡çš„åŠŸèƒ½è¯´æ˜ã€ä½¿ç”¨æ–¹æ³•ã€é…ç½®ä¿¡æ¯ç­‰",
                search_limit=5,
                enabled=True,
                process_immediately=True
            )
            
            logger.info("âœ… å·²è‡ªåŠ¨é…ç½®è§’è‰²æ‰®æ¼”æ•°æ®ç”ŸæˆæœåŠ¡çŸ¥è¯†åº“")
            
        except Exception as e:
            logger.error(f"è‡ªåŠ¨åˆå§‹åŒ–çŸ¥è¯†åº“å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸æœåŠ¡ç»§ç»­è¿è¡Œ
    
    async def _generate_search_keywords(self, character_description: str = "", requirements: str = "") -> List[str]:
        """ä½¿ç”¨LLMç”Ÿæˆæœç´¢å…³é”®è¯"""
        try:
            # æ„å»ºå…³é”®è¯ç”Ÿæˆæç¤ºè¯
            keyword_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹è§’è‰²è®¾å®šå’Œéœ€æ±‚æè¿°ï¼Œç”Ÿæˆ5-10ä¸ªé€‚åˆæœç´¢çŸ¥è¯†åº“çš„å…³é”®è¯ã€‚
å…³é”®è¯åº”è¯¥æ¶µç›–è§’è‰²ç‰¹ç‚¹ã€æ´»åŠ¨ç±»å‹ã€ä¸“ä¸šé¢†åŸŸç­‰æ–¹é¢ã€‚

è§’è‰²è®¾å®šï¼š
{character_description if character_description else "æœªæä¾›è§’è‰²è®¾å®š"}

éœ€æ±‚æè¿°ï¼š
{requirements if requirements else "æœªæä¾›å…·ä½“éœ€æ±‚"}

è¯·åªè¿”å›å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
ä¾‹å¦‚ï¼šæ—¥ç¨‹è§„åˆ’,æ—¶é—´ç®¡ç†,å·¥ä½œå®‰æ’,ä¼‘é—²æ´»åŠ¨,ä¸ªäººçˆ±å¥½

å…³é”®è¯ï¼š"""

            # è°ƒç”¨LLMç”Ÿæˆå…³é”®è¯
            success, content = await self.llm_caller.call_llm(
                keyword_prompt, max_tokens=100, temperature=0.3
            )
            
            if success and content:
                # è§£æç”Ÿæˆçš„å…³é”®è¯
                keywords = [kw.strip() for kw in content.strip().split(",") if kw.strip()]
                # é™åˆ¶å…³é”®è¯æ•°é‡
                keywords = keywords[:10]
                logger.info(f"âœ… LLMç”Ÿæˆæœç´¢å…³é”®è¯: {keywords}")
                return keywords
            else:
                logger.warning("LLMå…³é”®è¯ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å…³é”®è¯")
                
        except Exception as e:
            logger.error(f"LLMå…³é”®è¯ç”Ÿæˆå‡ºé”™: {e}")
        
        # å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨é»˜è®¤å…³é”®è¯
        default_keywords = ["æ—¥ç¨‹", "è®¡åˆ’", "å®‰æ’", "æ—¶é—´ç®¡ç†", "è§’è‰²æ‰®æ¼”"]
        if requirements:
            # ç®€å•æå–ä¸€äº›æ˜æ˜¾çš„å…³é”®è¯ä½œä¸ºè¡¥å……
            simple_keywords = [word.strip() for word in requirements.replace("ï¼Œ", ",").split(",") if word.strip()]
            default_keywords.extend(simple_keywords[:5])
        
        return default_keywords[:10]
    
    async def _enhance_with_role_plugins(self, character_description: str = "", requirements: str = "") -> Tuple[str, str]:
        """ä½¿ç”¨è§’è‰²æ’ä»¶å¢å¼ºå‚æ•° - åˆ†ç¦»è§’è‰²äººè®¾å’Œå‚è€ƒèµ„æ–™"""
        enhanced_character = character_description
        enhanced_requirements = requirements
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šç¡®å®šæœ€ç»ˆçš„è§’è‰²äººè®¾ä¿¡æ¯
            role_context = await self.role_plugin_manager.get_role_context([])  # å…ˆä¸ç”¨å…³é”®è¯ï¼Œç›´æ¥è·å–åŸºç¡€è§’è‰²èµ„æ–™
            
            # å¤„ç†è§’è‰²äººè®¾ - ç¡®å®šè§’è‰²çš„åŸºç¡€èº«ä»½ä¿¡æ¯
            final_character_profile = ""
            if "profile" in role_context and role_context["profile"]:
                plugin_profile = role_context["profile"]
                
                if enhanced_character and enhanced_character.strip():
                    # ç”¨æˆ·æä¾›äº†è§’è‰²æè¿°ï¼Œä»¥ç”¨æˆ·æè¿°ä¸ºå‡†
                    final_character_profile = enhanced_character
                    logger.info("âœ… ä½¿ç”¨ç”¨æˆ·æä¾›çš„è§’è‰²æè¿°ä½œä¸ºäººè®¾")
                else:
                    # ç”¨æˆ·æœªæä¾›è§’è‰²æè¿°ï¼Œä½¿ç”¨æ’ä»¶ä¸­çš„è§’è‰²äººè®¾
                    final_character_profile = plugin_profile
                    enhanced_character = plugin_profile
                    logger.info("âœ… ä½¿ç”¨æ’ä»¶ä¸­çš„è§’è‰²äººè®¾ä½œä¸ºåŸºç¡€")
            else:
                # æ²¡æœ‰æ’ä»¶è§’è‰²èµ„æ–™ï¼Œä½¿ç”¨ç”¨æˆ·æä¾›çš„æè¿°
                final_character_profile = enhanced_character
                logger.info("âœ… ä»…ä½¿ç”¨ç”¨æˆ·è§’è‰²æè¿°ï¼Œæ— æ’ä»¶äººè®¾")
            
            # ç¬¬äºŒæ­¥ï¼šåŸºäºç¡®å®šçš„è§’è‰²äººè®¾ç”Ÿæˆæœç´¢å…³é”®è¯ï¼Œè·å–å‚è€ƒèµ„æ–™
            if final_character_profile:
                keywords = await self._generate_search_keywords(final_character_profile, enhanced_requirements)
                
                # é‡æ–°è·å–è§’è‰²ä¸Šä¸‹æ–‡ï¼Œè¿™æ¬¡å¸¦ä¸Šå…³é”®è¯æœç´¢çŸ¥è¯†åº“
                role_context_with_search = await self.role_plugin_manager.get_role_context(keywords)
                
                # å¤„ç†çŸ¥è¯†åº“å‚è€ƒèµ„æ–™ - ä»…ä½œä¸ºèƒŒæ™¯å‚è€ƒï¼Œä¸å½±å“è§’è‰²äººè®¾
                if "knowledge" in role_context_with_search and role_context_with_search["knowledge"]:
                    knowledge_results = role_context_with_search["knowledge"]
                    
                    # ç­›é€‰å’Œæ ¼å¼åŒ–å‚è€ƒèµ„æ–™
                    reference_materials = []
                    for i, result in enumerate(knowledge_results[:3], 1):  # æœ€å¤š3æ¡å‚è€ƒèµ„æ–™
                        content = result['content']
                        # ç®€åŒ–å‚è€ƒèµ„æ–™ï¼Œçªå‡ºå…³é”®ä¿¡æ¯
                        if len(content) > 120:
                            content = content[:120] + "..."
                        reference_materials.append(f"å‚è€ƒèµ„æ–™{i}: {content}")
                    
                    if reference_materials:
                        reference_section = f"\n\nã€èƒŒæ™¯å‚è€ƒèµ„æ–™ã€‘\n" + "\n".join(reference_materials)
                        enhanced_requirements = enhanced_requirements + reference_section if enhanced_requirements else f"è¯·å‚è€ƒä»¥ä¸‹èƒŒæ™¯èµ„æ–™ï¼š{reference_section}"
                        logger.info(f"âœ… å·²æ·»åŠ  {len(reference_materials)} æ¡èƒŒæ™¯å‚è€ƒèµ„æ–™")
            
            # è®°å½•å¤„ç†ç»“æœ
            if enhanced_character != character_description:
                logger.info(f"âœ… è§’è‰²äººè®¾å·²ç¡®å®šï¼š{enhanced_character[:50]}...")
            if enhanced_requirements != requirements:
                logger.info(f"âœ… éœ€æ±‚å·²å¢å¼ºï¼Œæ·»åŠ äº†å‚è€ƒèµ„æ–™ï¼ˆæ–°é•¿åº¦: {len(enhanced_requirements)}ï¼‰")
            
            return enhanced_character, enhanced_requirements
            
        except Exception as e:
            logger.error(f"è§’è‰²æ’ä»¶å¢å¼ºå¤±è´¥: {e}")
            # å¦‚æœæ’ä»¶å¢å¼ºå¤±è´¥ï¼Œè¿”å›åŸå§‹å‚æ•°
            return character_description, requirements
    
    async def generate_annual_schedule(self, csv_file_path: str, character_description: str = "", 
                                     start_from_day: int = 0, max_days: int = 365) -> Dict[str, Any]:
        """
        åŸºäºCSVå¹´åº¦æ—¥ç¨‹è§„åˆ’ç”Ÿæˆ365å¤©è¯¦ç»†æ—¥ç¨‹
        
        Args:
            csv_file_path: CSVå¹´åº¦æ—¥ç¨‹æ–‡ä»¶è·¯å¾„
            character_description: è§’è‰²è®¾å®šæè¿°ï¼ŒåŒ…å«è§’è‰²çš„æ€§æ ¼ç‰¹ç‚¹ã€ç”Ÿæ´»æ–¹å¼ç­‰
            start_from_day: ä»ç¬¬å‡ å¤©å¼€å§‹ç”Ÿæˆï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼Œ0-364ï¼‰
            max_days: æœ€å¤§ç”Ÿæˆå¤©æ•°ï¼ˆç”¨äºæµ‹è¯•æˆ–åˆ†æ®µç”Ÿæˆï¼‰
            
        Returns:
            ç”Ÿæˆç»“æœå­—å…¸
        """
        generation_id = f"annual_{int(datetime.now().timestamp())}"
        start_time = datetime.now()
        
        logger.info(f"ğŸš€ å¼€å§‹ç”Ÿæˆ365å¤©è¯¦ç»†æ—¥ç¨‹ï¼Œç”ŸæˆID: {generation_id}")
        logger.info(f"ğŸ“‹ CSVæ–‡ä»¶è·¯å¾„: {csv_file_path}")
        logger.info(f"ğŸ“ è§’è‰²æè¿°é•¿åº¦: {len(character_description)} å­—ç¬¦")
        logger.info(f"ğŸ¯ ç”ŸæˆèŒƒå›´: ç¬¬{start_from_day + 1}å¤© è‡³ ç¬¬{min(start_from_day + max_days, 365)}å¤©")
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šåŠ è½½CSVå¹´åº¦æ—¥ç¨‹
            logger.info("ğŸ“Š ç¬¬ä¸€æ­¥ï¼šåŠ è½½CSVå¹´åº¦æ—¥ç¨‹æ–‡ä»¶...")
            success, message, events = self.annual_manager.load_csv_schedule(csv_file_path)
            if not success:
                logger.error(f"âŒ CSVåŠ è½½å¤±è´¥: {message}")
                return {
                    "generation_id": generation_id,
                    "type": "annual_schedule",
                    "success": False,
                    "error": f"CSVåŠ è½½å¤±è´¥: {message}",
                    "started_at": start_time.isoformat()
                }
            
            logger.info(f"âœ… CSVåŠ è½½æˆåŠŸï¼Œå…±è§£æåˆ° {len(events)} ä¸ªå¹´åº¦äº‹ä»¶")
            
            # ç¬¬äºŒæ­¥ï¼šåˆå§‹åŒ–å¹´åº¦æ—¥ç¨‹æ•°æ®
            schedule_data = AnnualScheduleData()
            schedule_data.csv_events = events
            schedule_data.character_description = character_description
            schedule_data.csv_file_path = csv_file_path
            schedule_data.generation_progress["started_at"] = start_time.isoformat()
            schedule_data.generation_progress["status"] = "in_progress"
            
            self.annual_manager.current_schedule_data = schedule_data
            
            # ç¬¬ä¸‰æ­¥ï¼šè·å–å¢å¼ºçš„è§’è‰²æè¿°
            logger.info("ğŸ”§ ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨è§’è‰²æ’ä»¶å¢å¼ºè§’è‰²è®¾å®š...")
            enhanced_character, _ = await self._enhance_with_role_plugins(character_description, "")
            
            if enhanced_character != character_description:
                logger.info(f"âœ… è§’è‰²è®¾å®šå·²é€šè¿‡æ’ä»¶å¢å¼ºï¼Œå¢å¼ºåé•¿åº¦: {len(enhanced_character)} å­—ç¬¦")
                schedule_data.character_description = enhanced_character
            
            # ç¬¬å››æ­¥ï¼šå¼€å§‹é€æ—¥ç”Ÿæˆ
            logger.info("ğŸ“… ç¬¬å››æ­¥ï¼šå¼€å§‹é€æ—¥ç”Ÿæˆè¯¦ç»†æ—¥ç¨‹...")
            base_date = datetime(2024, 1, 1)  # åŸºå‡†æ—¥æœŸ
            
            total_generated = 0
            total_errors = 0
            generation_results = []
            
            end_day = min(start_from_day + max_days, 365)
            
            for day_index in range(start_from_day, end_day):
                current_date = base_date + timedelta(days=day_index)
                weekday_name = ["æ˜ŸæœŸä¸€", "æ˜ŸæœŸäºŒ", "æ˜ŸæœŸä¸‰", "æ˜ŸæœŸå››", "æ˜ŸæœŸäº”", "æ˜ŸæœŸå…­", "æ˜ŸæœŸæ—¥"][current_date.weekday()]
                
                logger.info(f"ğŸ“… æ­£åœ¨ç”Ÿæˆç¬¬{day_index + 1}å¤©æ—¥ç¨‹: {current_date.strftime('%Y-%m-%d')} {weekday_name}")
                
                try:
                    # ç”Ÿæˆå½“æ—¥è¯¦ç»†æ—¥ç¨‹
                    daily_result = await self._generate_single_day_schedule(
                        day_index, current_date, weekday_name, schedule_data
                    )
                    
                    if daily_result["success"]:
                        total_generated += 1
                        schedule_data.generation_progress["completed_days"] = total_generated
                        logger.info(f"âœ… ç¬¬{day_index + 1}å¤©æ—¥ç¨‹ç”ŸæˆæˆåŠŸ")
                        
                        # ä¿å­˜åˆ°æ–‡ä»¶
                        save_success = self.annual_manager.save_daily_schedule(day_index, daily_result)
                        if save_success:
                            logger.info(f"ğŸ’¾ ç¬¬{day_index + 1}å¤©æ—¥ç¨‹å·²ä¿å­˜åˆ°æ–‡ä»¶")
                        else:
                            logger.warning(f"âš ï¸ ç¬¬{day_index + 1}å¤©æ—¥ç¨‹ä¿å­˜å¤±è´¥")
                        
                    else:
                        total_errors += 1
                        logger.error(f"âŒ ç¬¬{day_index + 1}å¤©æ—¥ç¨‹ç”Ÿæˆå¤±è´¥: {daily_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    
                    generation_results.append(daily_result)
                    
                    # æ¯7å¤©è¿›è¡Œä¸€æ¬¡å‘¨åº¦å‹ç¼©
                    if (day_index + 1) % 7 == 0:
                        week_index = day_index // 7
                        logger.info(f"ğŸ“Š ç¬¬{week_index + 1}å‘¨ç»“æŸï¼Œå¼€å§‹å‘¨åº¦å‹ç¼©...")
                        
                        compression_result = await self._perform_weekly_compression(
                            week_index, schedule_data
                        )
                        
                        if compression_result["success"]:
                            logger.info(f"âœ… ç¬¬{week_index + 1}å‘¨å‹ç¼©æ‘˜è¦ç”ŸæˆæˆåŠŸ")
                            
                            # ä¿å­˜å‘¨åº¦å‹ç¼©æ‘˜è¦
                            save_success = self.annual_manager.save_weekly_compression(week_index, compression_result)
                            if save_success:
                                logger.info(f"ğŸ’¾ ç¬¬{week_index + 1}å‘¨å‹ç¼©æ‘˜è¦å·²ä¿å­˜åˆ°æ–‡ä»¶")
                            
                            # è¿›è¡Œè§’è‰²ä¸€è‡´æ€§éªŒè¯
                            verification_result = await self._perform_character_verification(
                                week_index, schedule_data
                            )
                            
                            if verification_result["success"]:
                                logger.info(f"ğŸ” ç¬¬{week_index + 1}å‘¨è§’è‰²ä¸€è‡´æ€§éªŒè¯å®Œæˆ")
                                logger.info(f"ğŸ“Š éªŒè¯ç»“æœ: {verification_result.get('content', {}).get('verification_result', 'N/A')}")
                            else:
                                logger.warning(f"âš ï¸ ç¬¬{week_index + 1}å‘¨è§’è‰²ä¸€è‡´æ€§éªŒè¯å¤±è´¥")
                        else:
                            logger.error(f"âŒ ç¬¬{week_index + 1}å‘¨å‹ç¼©æ‘˜è¦ç”Ÿæˆå¤±è´¥")
                    
                    # æ›´æ–°è¿›åº¦
                    schedule_data.generation_progress["current_day"] = day_index + 1
                    
                    # é˜²æ­¢è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œé€‚å½“å»¶è¿Ÿ
                    await asyncio.sleep(0.1)
                    
                except Exception as e:
                    total_errors += 1
                    logger.error(f"âŒ ç¬¬{day_index + 1}å¤©ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
                    import traceback
                    logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                    
                    error_result = {
                        "day_index": day_index,
                        "date": current_date.strftime('%Y-%m-%d'),
                        "success": False,
                        "error": str(e)
                    }
                    generation_results.append(error_result)
            
            # ç¬¬äº”æ­¥ï¼šå®Œæˆç”Ÿæˆï¼Œæ›´æ–°çŠ¶æ€
            end_time = datetime.now()
            total_time = (end_time - start_time).total_seconds()
            
            schedule_data.generation_progress["status"] = "completed" if total_errors == 0 else "completed_with_errors"
            schedule_data.generation_progress["estimated_completion"] = end_time.isoformat()
            
            logger.info(f"ğŸ‰ å¹´åº¦æ—¥ç¨‹ç”Ÿæˆå®Œæˆï¼")
            logger.info(f"ğŸ“Š æ€»å…±ç”Ÿæˆ: {total_generated} å¤©")
            logger.info(f"âŒ ç”Ÿæˆå¤±è´¥: {total_errors} å¤©")
            logger.info(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’")
            logger.info(f"ğŸ“‚ è¾“å‡ºç›®å½•: {self.annual_manager.output_dir}")
            
            result = {
                "generation_id": generation_id,
                "type": "annual_schedule",
                "success": total_errors == 0,
                "csv_file_path": csv_file_path,
                "character_description": character_description[:200] + "..." if len(character_description) > 200 else character_description,
                "enhanced_character_used": enhanced_character != character_description,
                "generation_stats": {
                    "total_days_requested": end_day - start_from_day,
                    "total_days_generated": total_generated,
                    "total_errors": total_errors,
                    "success_rate": (total_generated / (total_generated + total_errors)) * 100 if (total_generated + total_errors) > 0 else 0
                },
                "csv_events": {
                    "total_events": len(events),
                    "event_summary": [{"date_range": e["date_range"], "activity": e["activity_name"][:50]} for e in events[:5]]
                },
                "progress": schedule_data.generation_progress,
                "output_directory": str(self.annual_manager.output_dir),
                "generation_time": total_time,
                "started_at": start_time.isoformat(),
                "completed_at": end_time.isoformat(),
                "daily_results_sample": generation_results[:3] if generation_results else []
            }
            
            # æ·»åŠ åˆ°å†å²è®°å½•
            self._add_to_history(result)
            
            return result
            
        except Exception as e:
            end_time = datetime.now()
            error_msg = f"å¹´åº¦æ—¥ç¨‹ç”Ÿæˆå¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            
            return {
                "generation_id": generation_id,
                "type": "annual_schedule",
                "success": False,
                "error": error_msg,
                "csv_file_path": csv_file_path,
                "character_description": character_description[:200] + "..." if len(character_description) > 200 else character_description,
                "started_at": start_time.isoformat(),
                "failed_at": end_time.isoformat()
            }
    
    async def _generate_single_day_schedule(self, day_index: int, current_date: datetime, 
                                           weekday_name: str, schedule_data: AnnualScheduleData) -> Dict[str, Any]:
        """ç”Ÿæˆå•æ—¥è¯¦ç»†æ—¥ç¨‹"""
        try:
            # è·å–å½“æ—¥é¢„å®šäº‹ä»¶
            day_events = self.annual_manager.get_day_events(day_index, datetime(2024, 1, 1))
            
            # æ ¼å¼åŒ–é¢„å®šäº‹ä»¶ä¿¡æ¯
            if day_events:
                events_text = "\n".join([
                    f"- {event['activity_name']} ({event['activity_type']}) "
                    f"{'[äº‹ä»¶å¼€å§‹]' if event['is_event_start'] else ''}"
                    f"{'[äº‹ä»¶ç»“æŸ]' if event['is_event_end'] else ''}"
                    f" å¤‡æ³¨: {event['remarks']}" if event['remarks'] else ""
                    for event in day_events
                ])
            else:
                events_text = "ä»Šæ—¥æ— ç‰¹åˆ«é¢„å®šæ´»åŠ¨ï¼Œå®‰æ’å¸¸è§„æ—¥ç¨‹"
            
            # è·å–æ˜¨æ—¥æ‘˜è¦
            previous_summary = schedule_data.daily_summaries.get(day_index - 1, "æ˜¨æ—¥ä¿¡æ¯ä¸å¯ç”¨") if day_index > 0 else "è¿™æ˜¯ç¬¬ä¸€å¤©"
            
            # è·å–è¿‘æœŸèƒŒæ™¯ä¿¡æ¯ï¼ˆæœ€è¿‘3å¤©çš„ç®€è¦æ‘˜è¦ï¼‰
            recent_context = self._get_recent_context(day_index, schedule_data)
            
            # æœç´¢çŸ¥è¯†åº“è·å–ç›¸å…³ä¿¡æ¯
            knowledge_references = await self._search_knowledge_for_day(day_events, schedule_data.character_description)
            
            # æ„å»ºæç¤ºè¯
            prompt = self.prompt_manager.get_daily_schedule_prompt(
                character_description=schedule_data.character_description,
                current_date=current_date.strftime('%Y-%m-%d'),
                weekday=weekday_name,
                day_index=day_index + 1,
                scheduled_events=events_text,
                previous_day_summary=previous_summary,
                recent_context=recent_context,
                knowledge_references=knowledge_references
            )
            
            # è°ƒç”¨LLMç”Ÿæˆ
            success, content = await self.llm_caller.call_llm(
                prompt, max_tokens=4000, temperature=0.7
            )
            
            if success:
                # è§£æç”Ÿæˆçš„JSONå†…å®¹
                daily_data = self._parse_daily_schedule_json(content)
                if daily_data:
                    # ä¿å­˜å½“æ—¥æ‘˜è¦
                    daily_summary = daily_data.get("daily_summary", "å½“æ—¥æ—¥ç¨‹æ‘˜è¦æœªç”Ÿæˆ")
                    schedule_data.daily_summaries[day_index] = daily_summary
                    
                    return {
                        "day_index": day_index,
                        "date": current_date.strftime('%Y-%m-%d'),
                        "weekday": weekday_name,
                        "success": True,
                        "scheduled_events": day_events,
                        "daily_data": daily_data,
                        "knowledge_references_used": len(knowledge_references.split('\n')) if knowledge_references else 0,
                        "generated_at": datetime.now().isoformat()
                    }
                else:
                    return {
                        "day_index": day_index,
                        "date": current_date.strftime('%Y-%m-%d'),
                        "success": False,
                        "error": "JSONè§£æå¤±è´¥ï¼Œç”Ÿæˆå†…å®¹æ ¼å¼ä¸æ­£ç¡®"
                    }
            else:
                return {
                    "day_index": day_index,
                    "date": current_date.strftime('%Y-%m-%d'),
                    "success": False,
                    "error": f"LLMç”Ÿæˆå¤±è´¥: {content}"
                }
                
        except Exception as e:
            return {
                "day_index": day_index,
                "date": current_date.strftime('%Y-%m-%d') if current_date else "unknown",
                "success": False,
                "error": f"å•æ—¥ç”Ÿæˆå¼‚å¸¸: {str(e)}"
            }
    
    def _parse_daily_schedule_json(self, content: str) -> Optional[Dict[str, Any]]:
        """è§£æå•æ—¥æ—¥ç¨‹JSONå†…å®¹"""
        try:
            # å°è¯•ä»å†…å®¹ä¸­æå–JSON
            json_start = content.find('{')
            json_end = content.rfind('}')
            
            if json_start != -1 and json_end != -1:
                json_str = content[json_start:json_end + 1]
                schedule_data = json.loads(json_str)
                
                # éªŒè¯å¿…è¦å­—æ®µ
                required_phases = ["morning", "noon", "afternoon", "evening", "night"]
                for phase in required_phases:
                    if phase not in schedule_data:
                        logger.warning(f"ç¼ºå°‘æ—¶é—´æ®µ: {phase}")
                        schedule_data[phase] = []
                
                return schedule_data
            else:
                logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")
                return None
                
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            return None
        except Exception as e:
            logger.error(f"è§£ææ—¥ç¨‹JSONå¤±è´¥: {e}")
            return None
    
    def _get_recent_context(self, day_index: int, schedule_data: AnnualScheduleData) -> str:
        """è·å–è¿‘æœŸèƒŒæ™¯ä¿¡æ¯"""
        if day_index <= 0:
            return "è¿™æ˜¯ç¬¬ä¸€å¤©ï¼Œæ²¡æœ‰è¿‘æœŸèƒŒæ™¯ä¿¡æ¯"
        
        # è·å–æœ€è¿‘3å¤©çš„æ‘˜è¦
        recent_summaries = []
        for i in range(max(0, day_index - 3), day_index):
            summary = schedule_data.daily_summaries.get(i)
            if summary:
                date = (datetime(2024, 1, 1) + timedelta(days=i)).strftime('%m-%d')
                recent_summaries.append(f"{date}: {summary}")
        
        if recent_summaries:
            return "æœ€è¿‘å‡ å¤©çš„æ´»åŠ¨æ‘˜è¦:\n" + "\n".join(recent_summaries)
        else:
            return "è¿‘æœŸèƒŒæ™¯ä¿¡æ¯ä¸å¯ç”¨"
    
    async def _search_knowledge_for_day(self, day_events: List[Dict[str, Any]], character_description: str) -> str:
        """ä¸ºå½“æ—¥æœç´¢ç›¸å…³çŸ¥è¯†åº“ä¿¡æ¯"""
        try:
            # æ ¹æ®å½“æ—¥äº‹ä»¶å’Œè§’è‰²æè¿°ç”Ÿæˆæœç´¢å…³é”®è¯
            keywords = []
            
            # ä»äº‹ä»¶ä¸­æå–å…³é”®è¯
            for event in day_events:
                activity_type = event.get('activity_type', '')
                activity_name = event.get('activity_name', '')
                keywords.extend([activity_type, activity_name])
            
            # æ·»åŠ è§’è‰²ç›¸å…³çš„é€šç”¨å…³é”®è¯
            if 'å¤©æ–‡' in character_description:
                keywords.extend(['å¤©æ–‡', 'è§‚æµ‹', 'ç ”ç©¶'])
            if 'æ•™æˆ' in character_description:
                keywords.extend(['æ•™å­¦', 'å­¦æœ¯', 'è¯¾ç¨‹'])
            
            # è¿‡æ»¤å’Œå»é‡
            keywords = list(set([kw.strip() for kw in keywords if kw and len(kw.strip()) > 1]))[:8]
            
            if not keywords:
                return "æ— ç›¸å…³çŸ¥è¯†åº“ä¿¡æ¯"
            
            # æœç´¢çŸ¥è¯†åº“
            kb_result = await self.search_role_knowledge(keywords, limit=3, min_score=0.1)
            
            if kb_result["success"] and kb_result.get("results"):
                references = []
                for i, result in enumerate(kb_result["results"][:3], 1):
                    content = result.get("content", "")
                    if len(content) > 150:
                        content = content[:150] + "..."
                    references.append(f"å‚è€ƒ{i}: {content}")
                
                return "\n".join(references)
            else:
                return "çŸ¥è¯†åº“æœç´¢æ— ç›¸å…³ç»“æœ"
                
        except Exception as e:
            logger.error(f"çŸ¥è¯†åº“æœç´¢å¤±è´¥: {e}")
            return "çŸ¥è¯†åº“æœç´¢å¤±è´¥"
    
    async def _perform_weekly_compression(self, week_index: int, schedule_data: AnnualScheduleData) -> Dict[str, Any]:
        """æ‰§è¡Œå‘¨åº¦å‹ç¼©"""
        try:
            logger.info(f"ğŸ“Š å¼€å§‹ç¬¬{week_index + 1}å‘¨å‹ç¼©æ‘˜è¦ç”Ÿæˆ...")
            
            # è·å–æœ¬å‘¨çš„æ¯æ—¥æ‘˜è¦
            start_day = week_index * 7
            end_day = min(start_day + 7, 365)
            
            daily_summaries = []
            for day_idx in range(start_day, end_day):
                if day_idx in schedule_data.daily_summaries:
                    date = (datetime(2024, 1, 1) + timedelta(days=day_idx)).strftime('%m-%d')
                    summary = schedule_data.daily_summaries[day_idx]
                    daily_summaries.append(f"{date}: {summary}")
            
            daily_summaries_text = "\n".join(daily_summaries) if daily_summaries else "æœ¬å‘¨æ—¥ç¨‹æ‘˜è¦ä¸å¯ç”¨"
            
            # è·å–ä¸Šå‘¨å‹ç¼©æ‘˜è¦
            previous_week_summary = schedule_data.weekly_compressions.get(week_index - 1, "ä¸Šå‘¨æ‘˜è¦ä¸å¯ç”¨") if week_index > 0 else "è¿™æ˜¯ç¬¬ä¸€å‘¨"
            
            # æœç´¢çŸ¥è¯†åº“è¿›è¡ŒéªŒè¯
            verification_keywords = ["å‘¨åº¦æ€»ç»“", "è¡Œä¸ºæ¨¡å¼", "æ€§æ ¼ç‰¹å¾", "ç”Ÿæ´»ä¹ æƒ¯"]
            kb_result = await self.search_role_knowledge(verification_keywords, limit=2, min_score=0.1)
            
            knowledge_verification = "æ— çŸ¥è¯†åº“éªŒè¯ä¿¡æ¯"
            if kb_result["success"] and kb_result.get("results"):
                verification_texts = [result.get("content", "")[:200] for result in kb_result["results"][:2]]
                knowledge_verification = "\n".join(verification_texts)
            
            # æ„å»ºå‹ç¼©æç¤ºè¯
            start_date = (datetime(2024, 1, 1) + timedelta(days=start_day)).strftime('%Y-%m-%d')
            end_date = (datetime(2024, 1, 1) + timedelta(days=end_day - 1)).strftime('%Y-%m-%d')
            
            prompt = self.prompt_manager.get_weekly_compression_prompt(
                character_description=schedule_data.character_description,
                week_index=week_index + 1,
                start_date=start_date,
                end_date=end_date,
                daily_summaries=daily_summaries_text,
                previous_week_summary=previous_week_summary,
                knowledge_verification=knowledge_verification
            )
            
            # è°ƒç”¨LLMç”Ÿæˆå‹ç¼©æ‘˜è¦
            success, content = await self.llm_caller.call_llm(
                prompt, max_tokens=3000, temperature=0.5
            )
            
            if success:
                # è§£æå‹ç¼©ç»“æœ
                compression_data = self._parse_weekly_compression_json(content)
                if compression_data:
                    # ä¿å­˜åˆ°å‘¨åº¦å‹ç¼©è®°å½•
                    week_summary = compression_data.get("week_summary", "æœ¬å‘¨æ‘˜è¦æœªç”Ÿæˆ")
                    schedule_data.weekly_compressions[week_index] = week_summary
                    
                    logger.info(f"âœ… ç¬¬{week_index + 1}å‘¨å‹ç¼©æ‘˜è¦ç”ŸæˆæˆåŠŸ")
                    
                    return {
                        "week_index": week_index,
                        "start_date": start_date,
                        "end_date": end_date,
                        "success": True,
                        "compression_data": compression_data,
                        "daily_count": len(daily_summaries),
                        "generated_at": datetime.now().isoformat()
                    }
                else:
                    return {
                        "week_index": week_index,
                        "success": False,
                        "error": "å‘¨åº¦å‹ç¼©JSONè§£æå¤±è´¥"
                    }
            else:
                return {
                    "week_index": week_index,
                    "success": False,
                    "error": f"å‘¨åº¦å‹ç¼©LLMç”Ÿæˆå¤±è´¥: {content}"
                }
                
        except Exception as e:
            logger.error(f"å‘¨åº¦å‹ç¼©æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "week_index": week_index,
                "success": False,
                "error": f"å‘¨åº¦å‹ç¼©å¼‚å¸¸: {str(e)}"
            }
    
    def _parse_weekly_compression_json(self, content: str) -> Optional[Dict[str, Any]]:
        """è§£æå‘¨åº¦å‹ç¼©JSONå†…å®¹"""
        try:
            # å°è¯•ä»å†…å®¹ä¸­æå–JSON
            json_start = content.find('{')
            json_end = content.rfind('}')
            
            if json_start != -1 and json_end != -1:
                json_str = content[json_start:json_end + 1]
                compression_data = json.loads(json_str)
                return compression_data
            else:
                logger.warning("å‘¨åº¦å‹ç¼©æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")
                return None
                
        except json.JSONDecodeError as e:
            logger.error(f"å‘¨åº¦å‹ç¼©JSONè§£æå¤±è´¥: {e}")
            return None
        except Exception as e:
            logger.error(f"è§£æå‘¨åº¦å‹ç¼©JSONå¤±è´¥: {e}")
            return None
    
    async def _perform_character_verification(self, week_index: int, schedule_data: AnnualScheduleData) -> Dict[str, Any]:
        """æ‰§è¡Œè§’è‰²ä¸€è‡´æ€§éªŒè¯"""
        try:
            logger.info(f"ğŸ” å¼€å§‹ç¬¬{week_index + 1}å‘¨è§’è‰²ä¸€è‡´æ€§éªŒè¯...")
            
            # è·å–æœ¬å‘¨è¡Œä¸ºæ‘˜è¦
            current_week_summary = schedule_data.weekly_compressions.get(week_index, "æœ¬å‘¨æ‘˜è¦ä¸å¯ç”¨")
            
            # æ„å»ºæ—¶é—´èŒƒå›´
            start_day = week_index * 7
            end_day = min(start_day + 7, 365)
            start_date = (datetime(2024, 1, 1) + timedelta(days=start_day)).strftime('%Y-%m-%d')
            end_date = (datetime(2024, 1, 1) + timedelta(days=end_day - 1)).strftime('%Y-%m-%d')
            time_range = f"ç¬¬{week_index + 1}å‘¨ ({start_date} è‡³ {end_date})"
            
            # æœç´¢çŸ¥è¯†åº“è·å–æ ¸å¿ƒè§’è‰²ä¿¡æ¯
            core_keywords = ["è§’è‰²è®¾å®š", "æ€§æ ¼ç‰¹å¾", "èŒä¸šç‰¹ç‚¹", "ç”Ÿæ´»æ–¹å¼", "ä¸ªäººçˆ±å¥½"]
            kb_result = await self.search_role_knowledge(core_keywords, limit=3, min_score=0.2)
            
            knowledge_core = "æ— çŸ¥è¯†åº“æ ¸å¿ƒä¿¡æ¯"
            if kb_result["success"] and kb_result.get("results"):
                core_texts = [result.get("content", "")[:300] for result in kb_result["results"][:3]]
                knowledge_core = "\n".join(core_texts)
            
            # æ„å»ºéªŒè¯æç¤ºè¯
            prompt = self.prompt_manager.get_character_verification_prompt(
                character_description=schedule_data.character_description,
                time_range=time_range,
                behavior_summary=current_week_summary,
                knowledge_core=knowledge_core
            )
            
            # è°ƒç”¨LLMè¿›è¡ŒéªŒè¯
            success, content = await self.llm_caller.call_llm(
                prompt, max_tokens=2000, temperature=0.3
            )
            
            if success:
                # è§£æéªŒè¯ç»“æœ
                verification_data = self._parse_character_verification_json(content)
                if verification_data:
                    logger.info(f"âœ… ç¬¬{week_index + 1}å‘¨è§’è‰²éªŒè¯å®Œæˆ")
                    
                    return {
                        "week_index": week_index,
                        "time_range": time_range,
                        "success": True,
                        "content": verification_data,
                        "generated_at": datetime.now().isoformat()
                    }
                else:
                    return {
                        "week_index": week_index,
                        "success": False,
                        "error": "è§’è‰²éªŒè¯JSONè§£æå¤±è´¥"
                    }
            else:
                return {
                    "week_index": week_index,
                    "success": False,
                    "error": f"è§’è‰²éªŒè¯LLMç”Ÿæˆå¤±è´¥: {content}"
                }
                
        except Exception as e:
            logger.error(f"è§’è‰²éªŒè¯æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "week_index": week_index,
                "success": False,
                "error": f"è§’è‰²éªŒè¯å¼‚å¸¸: {str(e)}"
            }
    
    def _parse_character_verification_json(self, content: str) -> Optional[Dict[str, Any]]:
        """è§£æè§’è‰²éªŒè¯JSONå†…å®¹"""
        try:
            # å°è¯•ä»å†…å®¹ä¸­æå–JSON
            json_start = content.find('{')
            json_end = content.rfind('}')
            
            if json_start != -1 and json_end != -1:
                json_str = content[json_start:json_end + 1]
                verification_data = json.loads(json_str)
                return verification_data
            else:
                logger.warning("è§’è‰²éªŒè¯æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ ¼å¼")
                return None
                
        except json.JSONDecodeError as e:
            logger.error(f"è§’è‰²éªŒè¯JSONè§£æå¤±è´¥: {e}")
            return None
        except Exception as e:
            logger.error(f"è§£æè§’è‰²éªŒè¯JSONå¤±è´¥: {e}")
            return None
    
    def _add_to_history(self, result: Dict[str, Any]):
        """æ·»åŠ ç”Ÿæˆç»“æœåˆ°å†å²è®°å½•"""
        # ç®€åŒ–å†å²è®°å½•ï¼Œåªä¿ç•™å…³é”®ä¿¡æ¯
        history_entry = {
            "generation_id": result["generation_id"],
            "type": result["type"],
            "success": result["success"],
            "generated_at": result["generated_at"],
            "generation_time": result.get("generation_time", 0)
        }
        
        self.generation_history.append(history_entry)
        
        # é™åˆ¶å†å²è®°å½•æ•°é‡
        if len(self.generation_history) > 50:
            self.generation_history = self.generation_history[-50:]
    
    def get_generation_history(self, limit: int = 20) -> List[Dict[str, Any]]:
        """è·å–ç”Ÿæˆå†å²"""
        return self.generation_history[-limit:]
    
    def clear_generation_history(self) -> Dict[str, Any]:
        """æ¸…ç©ºç”Ÿæˆå†å²"""
        count = len(self.generation_history)
        self.generation_history.clear()
        return {
            "cleared_count": count,
            "cleared_at": datetime.now().isoformat()
        }
    
    async def query_role_profile(self, include_metadata: bool = False) -> Dict[str, Any]:
        """
        æŸ¥è¯¢è§’è‰²èµ„æ–™ä¿¡æ¯
        
        Args:
            include_metadata: æ˜¯å¦åŒ…å«å…ƒæ•°æ®ä¿¡æ¯
            
        Returns:
            è§’è‰²èµ„æ–™æŸ¥è¯¢ç»“æœå­—å…¸
        """
        try:
            # æ£€æŸ¥è§’è‰²èµ„æ–™æ’ä»¶æ˜¯å¦å¯ç”¨
            profile_plugin = self.role_plugin_manager.get_plugin("role_profile")
            if not profile_plugin:
                return {
                    "success": False,
                    "error": "è§’è‰²èµ„æ–™æ’ä»¶æœªæ‰¾åˆ°",
                    "available": False
                }
            
            # æ£€æŸ¥æ’ä»¶æ˜¯å¦å¯ç”¨ä¸”æœ‰æ•°æ®
            is_available = await profile_plugin.is_available()
            if not is_available:
                return {
                    "success": False,
                    "error": "è§’è‰²èµ„æ–™æ’ä»¶æœªå¯ç”¨æˆ–æ— å¯ç”¨æ•°æ®",
                    "available": False,
                    "enabled": profile_plugin.enabled
                }
            
            # è·å–è§’è‰²èµ„æ–™å†…å®¹
            profile_content = await profile_plugin.get_data()
            result = {
                "success": True,
                "available": True,
                "enabled": profile_plugin.enabled,
                "content": profile_content,
                "content_length": len(profile_content) if profile_content else 0,
                "queried_at": datetime.now().isoformat()
            }
            
            # å¦‚æœéœ€è¦åŒ…å«å…ƒæ•°æ®ï¼Œæ·»åŠ è¯¦ç»†ä¿¡æ¯
            if include_metadata:
                profile_info = profile_plugin.get_profile_info()
                if profile_info:
                    result.update({
                        "metadata": profile_info,
                        "name": profile_info.get("name"),
                        "tags": profile_info.get("tags", []),
                        "created_at": profile_info.get("created_at"),
                        "updated_at": profile_info.get("updated_at")
                    })
            
            logger.info(f"âœ… è§’è‰²èµ„æ–™æŸ¥è¯¢æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {result['content_length']}")
            return result
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢è§’è‰²èµ„æ–™å¤±è´¥: {e}")
            return {
                "success": False,
                "error": f"æŸ¥è¯¢è§’è‰²èµ„æ–™æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "available": False,
                "queried_at": datetime.now().isoformat()
            }
    
    async def search_role_knowledge(self, keywords: List[str], limit: int = 5, min_score: float = 0.0) -> Dict[str, Any]:
        """
        æœç´¢è§’è‰²çŸ¥è¯†åº“
        
        Args:
            keywords: æœç´¢å…³é”®è¯åˆ—è¡¨
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            min_score: æœ€å°ç›¸ä¼¼åº¦åˆ†æ•°é˜ˆå€¼
            
        Returns:
            çŸ¥è¯†åº“æœç´¢ç»“æœå­—å…¸
        """
        try:
            # æ£€æŸ¥å‚æ•°
            if not keywords or not isinstance(keywords, list):
                return {
                    "success": False,
                    "error": "æœç´¢å…³é”®è¯ä¸èƒ½ä¸ºç©ºä¸”å¿…é¡»æ˜¯åˆ—è¡¨æ ¼å¼",
                    "keywords": keywords
                }
            
            # è¿‡æ»¤ç©ºå…³é”®è¯
            valid_keywords = [kw.strip() for kw in keywords if kw and kw.strip()]
            if not valid_keywords:
                return {
                    "success": False,
                    "error": "æ²¡æœ‰æœ‰æ•ˆçš„æœç´¢å…³é”®è¯",
                    "keywords": keywords
                }
            
            # æ£€æŸ¥çŸ¥è¯†åº“æ’ä»¶æ˜¯å¦å¯ç”¨
            kb_plugin = self.role_plugin_manager.get_plugin("role_knowledge_base")
            if not kb_plugin:
                return {
                    "success": False,
                    "error": "è§’è‰²çŸ¥è¯†åº“æ’ä»¶æœªæ‰¾åˆ°",
                    "available": False,
                    "keywords": valid_keywords
                }
            
            # æ£€æŸ¥æ’ä»¶æ˜¯å¦å¯ç”¨ä¸”æœ‰æ•°æ®
            is_available = await kb_plugin.is_available()
            if not is_available:
                kb_info = kb_plugin.get_knowledge_base_info()
                return {
                    "success": False,
                    "error": "è§’è‰²çŸ¥è¯†åº“æ’ä»¶æœªå¯ç”¨æˆ–çŸ¥è¯†åº“æœªé…ç½®",
                    "available": False,
                    "enabled": kb_plugin.enabled,
                    "knowledge_base_info": kb_info,
                    "keywords": valid_keywords
                }
            
            # æ‰§è¡Œæœç´¢
            search_results = await kb_plugin.search_knowledge(valid_keywords, limit)
            
            # æ ¹æ®æœ€å°åˆ†æ•°è¿‡æ»¤ç»“æœ
            filtered_results = []
            if search_results:
                for result in search_results:
                    score = result.get("score", 0.0)
                    if score >= min_score:
                        filtered_results.append(result)
            
            # æ„å»ºè¿”å›ç»“æœ
            result = {
                "success": True,
                "available": True,
                "enabled": kb_plugin.enabled,
                "keywords": valid_keywords,
                "total_results": len(search_results),
                "filtered_results": len(filtered_results),
                "results": filtered_results,
                "search_params": {
                    "limit": limit,
                    "min_score": min_score
                },
                "searched_at": datetime.now().isoformat()
            }
            
            # æ·»åŠ çŸ¥è¯†åº“åŸºæœ¬ä¿¡æ¯
            kb_info = kb_plugin.get_knowledge_base_info()
            if kb_info:
                result["knowledge_base_info"] = {
                    "name": kb_info.get("name"),
                    "description": kb_info.get("description"),
                    "data_count": kb_info.get("data_count", 0),
                    "vector_count": kb_info.get("vector_count", 0)
                }
            
            logger.info(f"âœ… è§’è‰²çŸ¥è¯†åº“æœç´¢å®Œæˆ: {valid_keywords} -> {len(filtered_results)} ä¸ªç»“æœ")
            return result
            
        except Exception as e:
            logger.error(f"æœç´¢è§’è‰²çŸ¥è¯†åº“å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return {
                "success": False,
                "error": f"æœç´¢è§’è‰²çŸ¥è¯†åº“æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "available": False,
                "keywords": keywords,
                "searched_at": datetime.now().isoformat()
            }
    
    def get_role_plugin_status(self) -> Dict[str, Any]:
        """
        è·å–è§’è‰²æ’ä»¶ç³»ç»ŸçŠ¶æ€
        
        Returns:
            è§’è‰²æ’ä»¶ç³»ç»ŸçŠ¶æ€å­—å…¸
        """
        try:
            # è·å–æ’ä»¶ç®¡ç†å™¨çŠ¶æ€
            status = self.role_plugin_manager.get_status()
            
            # æ·»åŠ é¢å¤–çš„çŠ¶æ€ä¿¡æ¯
            result = {
                "success": True,
                "status": status,
                "checked_at": datetime.now().isoformat(),
                "summary": {
                    "profile_enabled": status.get("profile_plugin", {}).get("enabled", False),
                    "profile_available": status.get("profile_plugin", {}).get("available", False),
                    "knowledge_base_enabled": status.get("knowledge_base_plugin", {}).get("enabled", False),
                    "knowledge_base_available": status.get("knowledge_base_plugin", {}).get("available", False)
                }
            }
            
            # æ·»åŠ å¯ç”¨æ€§æè¿°
            profile_status = "å¯ç”¨" if result["summary"]["profile_available"] else "ä¸å¯ç”¨"
            kb_status = "å¯ç”¨" if result["summary"]["knowledge_base_available"] else "ä¸å¯ç”¨"
            
            result["description"] = f"è§’è‰²èµ„æ–™: {profile_status}, çŸ¥è¯†åº“: {kb_status}"
            
            logger.info(f"âœ… è§’è‰²æ’ä»¶çŠ¶æ€æŸ¥è¯¢å®Œæˆ: {result['description']}")
            return result
            
        except Exception as e:
            logger.error(f"è·å–è§’è‰²æ’ä»¶çŠ¶æ€å¤±è´¥: {e}")
            return {
                "success": False,
                "error": f"è·å–è§’è‰²æ’ä»¶çŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
                "checked_at": datetime.now().isoformat()
            }
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.llm_caller:
            await self.llm_caller.cleanup()


class RolePlayDataServer(StdioMCPServer):
    """è§’è‰²æ‰®æ¼”æ•°æ®ç”ŸæˆMCPæœåŠ¡å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æœåŠ¡å™¨"""
        super().__init__("roleplay-data-server")
        self.generator = RolePlayDataGenerator()
        self._register_roleplay_tools()
    
    def _register_roleplay_tools(self):
        """æ³¨å†Œè§’è‰²æ‰®æ¼”æ•°æ®ç”Ÿæˆå·¥å…·"""
        
        # ç”Ÿæˆ365å¤©å¹´åº¦è¯¦ç»†æ—¥ç¨‹å·¥å…·
        self.register_tool(Tool(
            name="generate_annual_schedule",
            description="åŸºäºCSVå¹´åº¦æ—¥ç¨‹è§„åˆ’ç”Ÿæˆ365å¤©è¯¦ç»†çš„æ¯æ—¥5é˜¶æ®µæ—¥ç¨‹å®‰æ’ã€‚è‡ªåŠ¨ä½¿ç”¨é¢„è®¾çš„è§’è‰²é…ç½®å’ŒCSVæ–‡ä»¶ï¼Œæ— éœ€é¢å¤–å‚æ•°",
            inputSchema=ToolInputSchema(
                type="object",
                properties={}
            )
        ))
        
        # è·å–ç”Ÿæˆå†å²å·¥å…·
        self.register_tool(Tool(
            name="get_generation_history",
            description="è·å–è§’è‰²æ‰®æ¼”æ•°æ®ç”Ÿæˆå†å²è®°å½•",
            inputSchema=ToolInputSchema(
                type="object",
                properties={
                    "limit": {
                        "type": "integer",
                        "description": "è¿”å›çš„å†å²è®°å½•æ•°é‡é™åˆ¶ï¼Œé»˜è®¤20",
                        "default": 20,
                        "minimum": 1,
                        "maximum": 50
                    }
                }
            )
        ))
        
        # æ¸…ç©ºç”Ÿæˆå†å²å·¥å…·
        self.register_tool(Tool(
            name="clear_generation_history",
            description="æ¸…ç©ºè§’è‰²æ‰®æ¼”æ•°æ®ç”Ÿæˆå†å²è®°å½•",
            inputSchema=ToolInputSchema(
                type="object",
                properties={}
            )
        ))
        
        # è·å–æ—¶é—´é˜¶æ®µä¿¡æ¯å·¥å…·
        self.register_tool(Tool(
            name="get_time_phases",
            description="è·å–5é˜¶æ®µæ—¶é—´è§„åˆ’çš„è¯¦ç»†ä¿¡æ¯",
            inputSchema=ToolInputSchema(
                type="object",
                properties={}
            )
        ))
        
        # æŸ¥è¯¢è§’è‰²èµ„æ–™å·¥å…·
        self.register_tool(Tool(
            name="query_role_profile",
            description="æŸ¥è¯¢å½“å‰é…ç½®çš„è§’è‰²èµ„æ–™ä¿¡æ¯ï¼ŒåŒ…æ‹¬è§’è‰²çš„åŸºæœ¬è®¾å®šã€æ€§æ ¼ç‰¹ç‚¹ã€èƒŒæ™¯æ•…äº‹ç­‰è¯¦ç»†ä¿¡æ¯",
            inputSchema=ToolInputSchema(
                type="object",
                properties={
                    "include_metadata": {
                        "type": "boolean",
                        "description": "æ˜¯å¦åŒ…å«å…ƒæ•°æ®ä¿¡æ¯ï¼ˆåˆ›å»ºæ—¶é—´ã€æ›´æ–°æ—¶é—´ã€æ ‡ç­¾ç­‰ï¼‰",
                        "default": False
                    }
                }
            )
        ))
        
        # æœç´¢è§’è‰²çŸ¥è¯†åº“å·¥å…·
        self.register_tool(Tool(
            name="search_role_knowledge",
            description="åŸºäºå…³é”®è¯åœ¨è§’è‰²çŸ¥è¯†åº“ä¸­è¿›è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼Œè·å–ç›¸å…³çš„è§’è‰²èƒŒæ™¯çŸ¥è¯†ã€ä¸“ä¸šä¿¡æ¯ç­‰",
            inputSchema=ToolInputSchema(
                type="object",
                properties={
                    "keywords": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "æœç´¢å…³é”®è¯åˆ—è¡¨ï¼Œç”¨äºåœ¨çŸ¥è¯†åº“ä¸­æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯",
                        "minItems": 1
                    },
                    "limit": {
                        "type": "integer",
                        "description": "è¿”å›ç»“æœæ•°é‡é™åˆ¶ï¼Œé»˜è®¤ä½¿ç”¨çŸ¥è¯†åº“é…ç½®çš„é™åˆ¶",
                        "minimum": 1,
                        "maximum": 20,
                        "default": 5
                    },
                    "min_score": {
                        "type": "number",
                        "description": "æœ€å°ç›¸ä¼¼åº¦åˆ†æ•°é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼Œä½äºæ­¤åˆ†æ•°çš„ç»“æœå°†è¢«è¿‡æ»¤",
                        "minimum": 0.0,
                        "maximum": 1.0,
                        "default": 0.0
                    }
                },
                required=["keywords"]
            )
        ))
        
        # è·å–è§’è‰²æ’ä»¶çŠ¶æ€å·¥å…·
        self.register_tool(Tool(
            name="get_role_plugin_status",
            description="è·å–è§’è‰²æ’ä»¶ç³»ç»Ÿçš„çŠ¶æ€ä¿¡æ¯ï¼ŒåŒ…æ‹¬è§’è‰²èµ„æ–™å’ŒçŸ¥è¯†åº“çš„é…ç½®çŠ¶æ€",
            inputSchema=ToolInputSchema(
                type="object",
                properties={}
            )
        ))


    async def _call_tool(self, name: str, arguments: Dict[str, Any], context) -> Dict[str, Any]:
        """å¤„ç†å·¥å…·è°ƒç”¨"""
        try:
            logger.info(f"è§’è‰²æ‰®æ¼”æ•°æ®ç”Ÿæˆå·¥å…·è°ƒç”¨: {name}")
            logger.info(f"å‚æ•°: {arguments}")
            
            if name == "generate_annual_schedule":
                # ä½¿ç”¨é»˜è®¤é…ç½®ï¼Œæ— éœ€å‚æ•°
                csv_file_path = "workspace/æ–¹çŸ¥è¡¡å¹´åº¦æ—¥ç¨‹è§„åˆ’.csv"  # é»˜è®¤CSVæ–‡ä»¶è·¯å¾„
                character_description = ""  # ä»è§’è‰²æ’ä»¶ä¸­è‡ªåŠ¨è·å–
                start_from_day = 0  # ä»ç¬¬1å¤©å¼€å§‹
                max_days = 3  # æ¼”ç¤ºæ¨¡å¼ï¼šåªç”Ÿæˆå‰3å¤©
                
                logger.info(f"ğŸ“‹ å¼€å§‹ç”Ÿæˆå¹´åº¦æ—¥ç¨‹ï¼‰ï¼šCSVæ–‡ä»¶={csv_file_path}, ç”Ÿæˆå¤©æ•°={max_days}")
                
                return await self.generator.generate_annual_schedule(
                    csv_file_path, character_description, start_from_day, max_days
                )
            
            elif name == "get_generation_history":
                limit = arguments.get("limit", 20)
                if not isinstance(limit, int) or limit < 1 or limit > 50:
                    limit = 20
                
                history = self.generator.get_generation_history(limit)
                return {
                    "history": history,
                    "total_count": len(history),
                    "retrieved_at": datetime.now().isoformat()
                }
            
            elif name == "clear_generation_history":
                return self.generator.clear_generation_history()
            
            elif name == "get_time_phases":
                phases_info = []
                for phase in TimePhase:
                    phases_info.append({
                        "name": phase.phase_name,
                        "time_range": phase.time_range,
                        "start_time": phase.start_time,
                        "end_time": phase.end_time,
                        "description": f"{phase.phase_name}æ—¶é—´æ®µï¼Œé€‚åˆè¿›è¡Œç›¸åº”çš„æ´»åŠ¨å®‰æ’"
                    })
                
                return {
                    "phases": phases_info,
                    "total_phases": len(phases_info),
                    "description": "5é˜¶æ®µæ—¶é—´è§„åˆ’åŸåˆ™ï¼Œå°†ä¸€å¤©åˆ†ä¸º5ä¸ªæ—¶é—´æ®µè¿›è¡Œç²¾ç»†åŒ–ç®¡ç†"
                }
            
            elif name == "query_role_profile":
                include_metadata = arguments.get("include_metadata", False)
                return await self.generator.query_role_profile(include_metadata)
            
            elif name == "search_role_knowledge":
                keywords = arguments.get("keywords", [])
                limit = arguments.get("limit", 5)
                min_score = arguments.get("min_score", 0.0)
                return await self.generator.search_role_knowledge(keywords, limit, min_score)
            
            elif name == "get_role_plugin_status":
                return self.generator.get_role_plugin_status()
            
            else:
                return {"error": f"æœªçŸ¥å·¥å…·: {name}"}
                
        except Exception as e:
            import traceback
            error_traceback = traceback.format_exc()
            logger.error(f"è§’è‰²æ‰®æ¼”æ•°æ®ç”Ÿæˆå·¥å…·è°ƒç”¨å¤±è´¥ {name}: {e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {error_traceback}")
            return {
                "error": f"å·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}",
                "error_type": type(e).__name__,
                "tool_name": name,
                "received_arguments": arguments,
                "timestamp": datetime.now().isoformat()
            }
    
    async def cleanup(self):
        """æ¸…ç†æœåŠ¡å™¨èµ„æº"""
        if hasattr(self, 'generator') and self.generator:
            await self.generator.cleanup()


async def test_local_generation():
    """æœ¬åœ°æµ‹è¯•ç”ŸæˆåŠŸèƒ½"""
    print("ğŸš€ è§’è‰²æ‰®æ¼”æ•°æ®ç”ŸæˆæœåŠ¡ - æœ¬åœ°æµ‹è¯•æ¨¡å¼ï¼ˆ365å¤©å¹´åº¦æ—¥ç¨‹ç”Ÿæˆï¼‰")
    print("=" * 80)
    
    # åˆ›å»ºç”Ÿæˆå™¨å®ä¾‹
    generator = RolePlayDataGenerator()
    
    # ç­‰å¾…çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆ
    await asyncio.sleep(2)
    
    print("ğŸ” ç¬¬ä¸€æ­¥ï¼šæµ‹è¯•è§’è‰²æ’ä»¶çŠ¶æ€æŸ¥è¯¢...")
    plugin_status = generator.get_role_plugin_status()
    print(f"âœ… æ’ä»¶çŠ¶æ€: {plugin_status.get('description', 'N/A')}")
    print(f"ğŸ“Š è§’è‰²èµ„æ–™å¯ç”¨: {plugin_status.get('summary', {}).get('profile_available', False)}")
    print(f"ğŸ“Š çŸ¥è¯†åº“å¯ç”¨: {plugin_status.get('summary', {}).get('knowledge_base_available', False)}")
    print("-" * 80)
    
    print("ğŸ” ç¬¬äºŒæ­¥ï¼šæµ‹è¯•è§’è‰²èµ„æ–™æŸ¥è¯¢...")
    profile_result = await generator.query_role_profile(include_metadata=True)
    if profile_result["success"]:
        print("âœ… è§’è‰²èµ„æ–™æŸ¥è¯¢æˆåŠŸï¼")
        print(f"ğŸ“ å†…å®¹é•¿åº¦: {profile_result.get('content_length', 0)} å­—ç¬¦")
        if profile_result.get("content"):
            print(f"ğŸ“‹ è§’è‰²èµ„æ–™é¢„è§ˆ: {profile_result['content'][:200]}...")
        if profile_result.get("metadata"):
            print(f"ğŸ·ï¸ è§’è‰²åç§°: {profile_result.get('name', 'N/A')}")
            print(f"ğŸ·ï¸ æ ‡ç­¾: {profile_result.get('tags', [])}")
    else:
        print(f"âŒ è§’è‰²èµ„æ–™æŸ¥è¯¢å¤±è´¥: {profile_result.get('error', 'N/A')}")
    print("-" * 80)
    
    print("ğŸ” ç¬¬ä¸‰æ­¥ï¼šæµ‹è¯•è§’è‰²çŸ¥è¯†åº“æœç´¢...")
    search_keywords = ["æ—…è¡Œ", "åå¥½", "å–œå¥½", "å…´è¶£", "ä¹ æƒ¯"]
    kb_result = await generator.search_role_knowledge(
        keywords=search_keywords, 
        limit=3, 
        min_score=0.1
    )
    if kb_result["success"]:
        print("âœ… çŸ¥è¯†åº“æœç´¢æˆåŠŸï¼")
        print(f"ğŸ” æœç´¢å…³é”®è¯: {kb_result.get('keywords', [])}")
        print(f"ğŸ“Š æ€»ç»“æœæ•°: {kb_result.get('total_results', 0)}")
        print(f"ğŸ“Š è¿‡æ»¤åç»“æœæ•°: {kb_result.get('filtered_results', 0)}")
        
        results = kb_result.get("results", [])
        if results:
            print("ğŸ“‹ æœç´¢ç»“æœé¢„è§ˆ:")
            for i, result in enumerate(results[:2], 1):  # åªæ˜¾ç¤ºå‰2ä¸ªç»“æœ
                score = result.get("score", 0.0)
                content = result.get("content", "")
                print(f"  {i}. ç›¸ä¼¼åº¦: {score:.3f}")
                print(f"     å†…å®¹: {content[:150]}...")
        
        kb_info = kb_result.get("knowledge_base_info", {})
        if kb_info:
            print(f"ğŸ“š çŸ¥è¯†åº“: {kb_info.get('name', 'N/A')}")
            print(f"ğŸ“š æ•°æ®é‡: {kb_info.get('data_count', 0)} æ¡ï¼Œå‘é‡: {kb_info.get('vector_count', 0)} ä¸ª")
    else:
        print(f"âŒ çŸ¥è¯†åº“æœç´¢å¤±è´¥: {kb_result.get('error', 'N/A')}")
    print("-" * 80)
    
    # æµ‹è¯•å¹´åº¦æ—¥ç¨‹ç”Ÿæˆ
    csv_file_path = "workspace/æ–¹çŸ¥è¡¡å¹´åº¦æ—¥ç¨‹è§„åˆ’.csv"
    test_character = """
    æ–¹çŸ¥è¡¡ï¼Œ28å²ï¼Œäº‘æ¢å¤§å­¦å¤©æ–‡ç³»å®¢åº§æ•™æˆã€åšå£«
    æ€§æ ¼æ¸…å†·ä½†ä¸å‚²æ…¢ï¼Œæœ‰è´£ä»»æ„Ÿï¼Œå–„äºå†…çœ
    å¹³æ—¶å–œæ¬¢åœ¨å’–å•¡åº—å·¥ä½œï¼Œçƒ­çˆ±é˜…è¯»å’Œå¤©æ–‡ç ”ç©¶
    ä½œæ¯è§„å¾‹ï¼Œä¸å–œæ¬¢ç†¬å¤œï¼Œæœ‰æ¯æ—¥æ™¨è·‘ä¹ æƒ¯
    """
    
    print("ğŸ“ ç¬¬å››æ­¥ï¼šæµ‹è¯•365å¤©å¹´åº¦æ—¥ç¨‹ç”ŸæˆåŠŸèƒ½...")
    print(f"ğŸ“‚ CSVæ–‡ä»¶è·¯å¾„: {csv_file_path}")
    print(f"ğŸ‘¤ è§’è‰²è®¾å®š: {test_character.strip()}")
    print(f"ğŸ¯ æµ‹è¯•ç”Ÿæˆå¤©æ•°: å‰3å¤©ï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼‰")
    print("-" * 80)
    
    try:
        # æµ‹è¯•å¹´åº¦æ—¥ç¨‹ç”Ÿæˆï¼ˆåªç”Ÿæˆå‰3å¤©ä½œä¸ºæ¼”ç¤ºï¼‰
        print("ğŸ¯ å¼€å§‹ç”Ÿæˆ365å¤©å¹´åº¦è¯¦ç»†æ—¥ç¨‹ï¼ˆæ¼”ç¤ºï¼šå‰3å¤©ï¼‰...")
        annual_result = await generator.generate_annual_schedule(
            csv_file_path=csv_file_path,
            character_description=test_character,
            start_from_day=0,
            max_days=3  # æ¼”ç¤ºæ¨¡å¼ï¼Œåªç”Ÿæˆå‰3å¤©
        )
        
        if annual_result["success"]:
            print("âœ… å¹´åº¦æ—¥ç¨‹ç”ŸæˆæˆåŠŸï¼")
            print(f"ğŸ”§ æ˜¯å¦ä½¿ç”¨äº†è§’è‰²æ’ä»¶å¢å¼º: {annual_result.get('enhanced_character_used', False)}")
            print(f"ğŸ“Š ç”Ÿæˆç»Ÿè®¡: {annual_result.get('generation_stats', {})}")
            print(f"ğŸ“‹ CSVäº‹ä»¶: {annual_result.get('csv_events', {})}")
            print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {annual_result.get('output_directory', 'N/A')}")
            print(f"â±ï¸ æ€»è€—æ—¶: {annual_result.get('generation_time', 0):.2f} ç§’")
            
            # æ˜¾ç¤ºæ¯æ—¥ç”Ÿæˆç»“æœæ ·ä¾‹
            daily_samples = annual_result.get("daily_results_sample", [])
            if daily_samples:
                print("\nğŸ“… æ¯æ—¥ç”Ÿæˆç»“æœæ ·ä¾‹:")
                for sample in daily_samples:
                    if sample.get("success"):
                        day_index = sample.get("day_index", 0)
                        date = sample.get("date", "unknown")
                        weekday = sample.get("weekday", "unknown")
                        print(f"\nğŸ“… ç¬¬{day_index + 1}å¤© ({date} {weekday}):")
                        
                        daily_data = sample.get("daily_data", {})
                        if daily_data:
                            print(f"   ğŸ“ å½“æ—¥æ‘˜è¦: {daily_data.get('daily_summary', 'N/A')}")
                            print(f"   ğŸ˜Š è§’è‰²çŠ¶æ€: {daily_data.get('character_state', 'N/A')}")
                            
                            # æ˜¾ç¤º5ä¸ªæ—¶é—´æ®µçš„æ´»åŠ¨ç®€è¦
                            phases = ["morning", "noon", "afternoon", "evening", "night"]
                            phase_names = ["ä¸Šåˆ", "ä¸­åˆ", "ä¸‹åˆ", "æ™šä¸Š", "å¤œé—´"]
                            for phase, name in zip(phases, phase_names):
                                activities = daily_data.get(phase, [])
                                if activities:
                                    print(f"   ğŸ• {name}: {len(activities)} ä¸ªæ´»åŠ¨")
                                    for act in activities[:2]:  # åªæ˜¾ç¤ºå‰2ä¸ªæ´»åŠ¨
                                        print(f"      â€¢ {act.get('activity_name', 'N/A')}")
                        
                        kb_refs = sample.get("knowledge_references_used", 0)
                        print(f"   ğŸ“š ä½¿ç”¨çŸ¥è¯†åº“å‚è€ƒ: {kb_refs} æ¡")
                    else:
                        day_index = sample.get("day_index", 0)
                        error = sample.get("error", "æœªçŸ¥é”™è¯¯")
                        print(f"âŒ ç¬¬{day_index + 1}å¤©ç”Ÿæˆå¤±è´¥: {error}")
            
            # æ˜¾ç¤ºç”Ÿæˆè¿›åº¦
            progress = annual_result.get("progress", {})
            if progress:
                print(f"\nğŸ“Š ç”Ÿæˆè¿›åº¦: {progress}")
            
        else:
            print(f"âŒ å¹´åº¦æ—¥ç¨‹ç”Ÿæˆå¤±è´¥: {annual_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†èµ„æº
        await generator.cleanup()
        print("\nğŸ æµ‹è¯•å®Œæˆ")
        print(f"ğŸ“‚ å¦‚æœç”ŸæˆæˆåŠŸï¼Œè¯·æŸ¥çœ‹è¾“å‡ºç›®å½•: workspace/annual_schedule_output/")
        print(f"ğŸ’¡ æç¤ºï¼šå®Œæ•´365å¤©ç”Ÿæˆè¯·ä½¿ç”¨max_days=365å‚æ•°")


async def main():
    """ä¸»å‡½æ•°"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # æ£€æŸ¥å¯åŠ¨æ¨¡å¼
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "--test":
        # æœ¬åœ°æµ‹è¯•æ¨¡å¼
        await test_local_generation()
    else:
        # MCPæœåŠ¡å™¨æ¨¡å¼
        server = RolePlayDataServer()
        logger.info("ğŸš€ å¯åŠ¨è§’è‰²æ‰®æ¼”æ•°æ®ç”ŸæˆMCPæœåŠ¡å™¨...")
        await server.start()


if __name__ == "__main__":
    asyncio.run(main()) 