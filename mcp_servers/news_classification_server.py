#!/usr/bin/env python3
"""
æ–°é—»æ ‡é¢˜å’Œå†…å®¹åˆ†ç±»æœåŠ¡å™¨
åŸºäºMCPåè®®ï¼Œæ”¯æŒè±†åŒ…LLMè°ƒç”¨è¿›è¡Œæ–°é—»åˆ†ç±»
"""
import os
import csv
import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
import asyncio
import logging

# å¯¼å…¥è±†åŒ…LLMç›¸å…³æ¨¡å—
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.llm.doubao import DoubaoLLM
from src.core.types import Message, MessageRole, LLMConfig

logger = logging.getLogger(__name__)


class NewsClassificationServer:
    """æ–°é—»æ ‡é¢˜å’Œå†…å®¹åˆ†ç±»MCPæœåŠ¡å™¨"""
    
    # æ–°é—»åˆ†ç±»æ ‡å‡†ï¼ˆæ·»åŠ æ”¿æ²»åˆ†ç±»ï¼‰
    CLASSIFICATION_CATEGORIES = [
        "éŸ³ä¹", "å½±è§†", "æ¸¸æˆ", "è¿åŠ¨", "é˜…è¯»", "è‰ºæœ¯", "äºŒæ¬¡å…ƒ", "äºšæ–‡åŒ–", 
        "æ—…è¡Œ", "ç¾é£Ÿ", "æ ¡å›­","æ‘„å½±", "å¤§è‡ªç„¶", "ç§‘æŠ€æ•°ç ", "å®¶å±…", "ç»¿æ¤", "å® ç‰©", 
        "å¨±ä¹åœˆ", "ç©¿æ­ç¾å¦†", "ç”Ÿæ´»æ–¹å¼", "æ—¥å¸¸çäº‹", "çº¿ä¸‹æ´»åŠ¨", "ä¸¤æ€§æƒ…æ„Ÿ", 
        "èŒåœº", "å®¶åº­", "ä»·å€¼è§‚", "æ”¿æ²»"
    ]
    
    def __init__(self, data_dir: str = "./workspace/output"):
        """
        åˆå§‹åŒ–æ–°é—»åˆ†ç±»æœåŠ¡å™¨
        
        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•ï¼Œé»˜è®¤è¾“å‡ºåˆ°workspace/output
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–è±†åŒ…LLMå®¢æˆ·ç«¯
        self.doubao_client = None
        self._init_doubao_client()
    
    def _init_doubao_client(self):
        """åˆå§‹åŒ–è±†åŒ…LLMå®¢æˆ·ç«¯"""
        try:
            # åˆ›å»ºè±†åŒ…LLMé…ç½®
            config = LLMConfig(
                provider="doubao",
                model_name=os.getenv('DOUBAO_MODEL_DEEPSEEKV3', 'doubao-pro-32k'),
                api_key=os.getenv('ARK_API_KEY') or os.getenv('DOUBAO_API_KEY'),
                api_base=os.getenv('DOUBAO_BASE_URL', "https://ark.cn-beijing.volces.com/api/v3"),
                temperature=0.1,  # åˆ†ç±»ä»»åŠ¡ä½¿ç”¨è¾ƒä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
                max_tokens=1000,
                timeout=30.0
            )
            
            # åˆ›å»ºè±†åŒ…LLMå®ä¾‹
            self.doubao_client = DoubaoLLM(config)
            
            # å¼‚æ­¥åˆå§‹åŒ–ï¼ˆç¨åæ‰§è¡Œï¼‰
            logger.info("è±†åŒ…LLMå®¢æˆ·ç«¯é…ç½®æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–è±†åŒ…LLMå®¢æˆ·ç«¯å¤±è´¥: {e}")
            self.doubao_client = None

    async def classify_single_news(self, title: str, content: str = "", 
                                  model: str = "doubao-pro-32k") -> Dict[str, Any]:
        """
        å¯¹å•æ¡æ–°é—»è¿›è¡Œåˆ†ç±»
        
        Args:
            title: æ–°é—»æ ‡é¢˜
            content: æ–°é—»å†…å®¹ï¼ˆå¯é€‰ï¼‰
            model: è±†åŒ…æ¨¡å‹åç§°
            
        Returns:
            Dict: åˆ†ç±»ç»“æœ
        """
        if not self.doubao_client:
            return {
                "success": False,
                "error": "æœªè®¾ç½®è±†åŒ…APIå¯†é’¥ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ ARK_API_KEY æˆ– DOUBAO_API_KEY",
                "title": title,
                "timestamp": datetime.now().isoformat()
            }
        
        try:
            result = await self._classify_with_doubao(title, content, model)
            # æ·»åŠ æ—¶é—´æˆ³
            result["timestamp"] = datetime.now().isoformat()
            return result
            
        except Exception as e:
            logger.error(f"æ–°é—»åˆ†ç±»å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "title": title,
                "timestamp": datetime.now().isoformat()
            }

    async def _classify_with_doubao(self, title: str, content: str, model: str) -> Dict[str, Any]:
        """ä½¿ç”¨è±†åŒ…LLMè¿›è¡Œæ–°é—»åˆ†ç±»ï¼Œé‡‡ç”¨COTæ€ç»´é“¾ï¼Œæ”¯æŒå¤šæ ‡ç­¾"""
        # ç¡®ä¿è±†åŒ…å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
        if not self.doubao_client:
            raise Exception("è±†åŒ…LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
        # æ‰§è¡Œå¼‚æ­¥åˆå§‹åŒ–
        await self.doubao_client.initialize()
        
        # ç»„åˆæ–°é—»æ–‡æœ¬
        news_text = f"æ ‡é¢˜ï¼š{title}"
        if content.strip():
            news_text += f"\nå†…å®¹ï¼š{content}"  # ä¸é™åˆ¶å†…å®¹é•¿åº¦ï¼Œä½¿ç”¨å®Œæ•´å†…å®¹
        
        # ä½¿ç”¨COTæ€ç»´é“¾æç¤ºï¼Œæ”¯æŒå¤šæ ‡ç­¾ï¼Œåˆå¹¶æˆä¸€æ®µ
        prompt = f"""
æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å¯¹æ–°é—»è¿›è¡Œåˆ†ç±»ï¼š

ç¬¬ä¸€æ­¥ï¼šç†è§£åˆ†ç±»æ ‡å‡†
å¯é€‰çš„27ä¸ªåˆ†ç±»æ ‡å‡†åŠå…¶è¯´æ˜ï¼š
- éŸ³ä¹ï¼šä¸éŸ³ä¹åˆ›ä½œã€æ¼”å‡ºã€ä¹å™¨å’ŒéŸ³ä¹äº§ä¸šç›¸å…³çš„å†…å®¹
- å½±è§†ï¼šç”µå½±ã€ç”µè§†å‰§ã€çºªå½•ç‰‡ç­‰è§†å¬å¨±ä¹ä½œå“åŠå…¶åˆ¶ä½œå‘è¡Œ
- æ¸¸æˆï¼šç”µå­æ¸¸æˆã€æ‰‹æœºæ¸¸æˆã€ç”µç«æ¯”èµ›ç­‰æ¸¸æˆå¨±ä¹å†…å®¹
- è¿åŠ¨ï¼šä½“è‚²ç«æŠ€ã€å¥èº«é”»ç‚¼ã€è¿åŠ¨èµ›äº‹ç­‰èº«ä½“æ´»åŠ¨ç›¸å…³
- é˜…è¯»ï¼šä¹¦ç±å‡ºç‰ˆã€æ–‡å­¦ä½œå“ã€é˜…è¯»æ–‡åŒ–ç­‰æ–‡å­—é˜…è¯»ç›¸å…³
- è‰ºæœ¯ï¼šç»˜ç”»é›•å¡‘ã€è‰ºæœ¯å±•è§ˆã€åˆ›æ„è®¾è®¡ç­‰è‰ºæœ¯åˆ›ä½œæ´»åŠ¨
- äºŒæ¬¡å…ƒï¼šåŠ¨æ¼«ã€æ¼«ç”»ã€å£°ä¼˜ç­‰äºŒæ¬¡å…ƒæ–‡åŒ–ç›¸å…³å†…å®¹
- äºšæ–‡åŒ–ï¼šå°ä¼—å…´è¶£ã€ç‰¹æ®Šç¾¤ä½“æ–‡åŒ–ç­‰éä¸»æµæ–‡åŒ–ç°è±¡
- æ—…è¡Œï¼šæ—…æ¸¸å‡ºè¡Œã€æ™¯ç‚¹ä»‹ç»ã€æ—…è¡Œæ”»ç•¥ç­‰å‡ºæ¸¸ç›¸å…³
- ç¾é£Ÿï¼šé¤é¥®æ–‡åŒ–ã€çƒ¹é¥ªæŠ€å·§ã€é£Ÿæä»‹ç»ç­‰é¥®é£Ÿç›¸å…³
- æ ¡å›­ï¼šå­¦æ ¡æ•™è‚²ã€å­¦ç”Ÿç”Ÿæ´»ã€å­¦æœ¯ç ”ç©¶ç­‰æ•™è‚²æœºæ„ç›¸å…³
- æ‘„å½±ï¼šæ‹ç…§æŠ€æœ¯ã€ç›¸æœºè®¾å¤‡ã€æ‘„å½±è‰ºæœ¯ç­‰å½±åƒåˆ›ä½œ
- å¤§è‡ªç„¶ï¼šç¯å¢ƒä¿æŠ¤ã€é‡ç”ŸåŠ¨æ¤ç‰©ã€è‡ªç„¶ç°è±¡ç­‰è‡ªç„¶ç•Œç›¸å…³
- ç§‘æŠ€æ•°ç ï¼šç§‘æŠ€äº§å“ã€æ•°å­—æŠ€æœ¯ã€äº’è”ç½‘åº”ç”¨ç­‰æŠ€æœ¯åˆ›æ–°
- å®¶å±…ï¼šå®¶åº­è£…ä¿®ã€å®¶å…·ç”¨å“ã€å±…ä½ç¯å¢ƒç­‰å®¶åº­ç”Ÿæ´»ç©ºé—´
- ç»¿æ¤ï¼šæ¤ç‰©ç§æ¤ã€å›­è‰ºå…»æŠ¤ã€èŠ±å‰æ ½åŸ¹ç­‰æ¤ç‰©å…»æŠ¤
- å® ç‰©ï¼šåŠ¨ç‰©é¥²å…»ã€å® ç‰©æŠ¤ç†ã€å® ç‰©ç”¨å“ç­‰åŠ¨ç‰©é™ªä¼´
- å¨±ä¹åœˆï¼šæ˜æ˜ŸåŠ¨æ€ã€å¨±ä¹å…«å¦ã€æ¼”è‰ºåœˆæ–°é—»ç­‰å¨±ä¹äº§ä¸š
- ç©¿æ­ç¾å¦†ï¼šæ—¶å°šæ­é…ã€åŒ–å¦†ç¾å®¹ã€æœè£…æ½®æµç­‰å¤–åœ¨å½¢è±¡
- ç”Ÿæ´»æ–¹å¼ï¼šç”Ÿæ´»ç†å¿µã€æ—¥å¸¸ä¹ æƒ¯ã€ç”Ÿæ´»å“è´¨ååº”ç”Ÿæ´»æ€åº¦çš„å†…å®¹
- æ—¥å¸¸çäº‹ï¼šç”Ÿæ´»ä¸­çš„ç¤¾ä¼šæ–°é—»ã€çƒ­ç‚¹äº‹ä»¶å‘ç”Ÿåœ¨å¤§ä¼—èº«è¾¹çš„ç”Ÿæ´»å†…å®¹
- çº¿ä¸‹æ´»åŠ¨ï¼šèšä¼šæ´»åŠ¨ã€å±•è§ˆæ¼”å‡ºã€ç¤¾äº¤èšé›†ç­‰ç°åœºæ´»åŠ¨
- ä¸¤æ€§æƒ…æ„Ÿï¼šæ‹çˆ±å…³ç³»ã€æƒ…æ„Ÿè¯é¢˜ã€æ€§åˆ«è®®é¢˜ç­‰æƒ…æ„Ÿäº¤æµ
- èŒåœºï¼šå·¥ä½œç¯å¢ƒã€ä¼ä¸šç®¡ç†ã€èŒä¸šå‘å±•ç­‰å·¥ä½œç›¸å…³
- å®¶åº­ï¼šå®¶åº­å…³ç³»ã€å©šå§»ç”Ÿæ´»ã€äº²å­æ•™è‚²ç­‰å®¶åº­ç”Ÿæ´»
- ä»·å€¼è§‚ï¼šæ€æƒ³è§‚å¿µã€äººç”Ÿå“²ç†ã€ç¤¾ä¼šä»·å€¼ç­‰ç²¾ç¥å±‚é¢
- æ”¿æ²»ï¼šæ”¿åºœæ”¿ç­–ã€å¤–äº¤å…³ç³»ã€æ”¿æ²»äººç‰©ç­‰å›½å®¶æ²»ç†ç›¸å…³ï¼ˆæ”¿æ²»è¯é¢˜å¿…é¡»ç‹¬ç«‹å‡ºç°ï¼Œç¦æ­¢å¤šä¸ªæ ‡ç­¾åŒæ—¶å‡ºç°ï¼‰

ç¬¬äºŒæ­¥ï¼šåˆ†ææ–°é—»å†…å®¹
æ–°é—»å†…å®¹ï¼š{news_text}
è¯·åˆ†æè¿™ç¯‡æ–°é—»çš„æ ¸å¿ƒä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Œä¸»è¦è®¨è®ºçš„å†…å®¹å’Œé¢†åŸŸæ˜¯ä»€ä¹ˆï¼Œæ¶‰åŠå“ªäº›æ–¹é¢ã€‚

ç¬¬ä¸‰æ­¥ï¼šåŒ¹é…åˆ†ç±»
æ ¹æ®æ–°é—»çš„æ ¸å¿ƒä¸»é¢˜å’Œæ¶‰åŠçš„å¤šä¸ªæ–¹é¢ï¼Œä»27ä¸ªåˆ†ç±»æ ‡å‡†ä¸­é€‰æ‹©1-3ä¸ªæœ€åˆé€‚çš„åˆ†ç±»ã€‚ä¸€ç¯‡æ–°é—»å¯ä»¥æœ‰å¤šä¸ªæ ‡ç­¾ã€‚

æœ€ç»ˆè¾“å‡º
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ï¼š
{{"thinking": "ä½ çš„åˆ†ææ€è€ƒè¿‡ç¨‹", "categories": ["åˆ†ç±»1", "åˆ†ç±»2"]}}

è¦æ±‚ï¼š
1.thinkingå­—æ®µæè¿°ä½ çš„åˆ†ææ€è€ƒè¿‡ç¨‹ 
2.categorieså­—æ®µæ˜¯æ•°ç»„ï¼ŒåŒ…å«1-3ä¸ªåˆ†ç±»ï¼Œæ¯ä¸ªåˆ†ç±»å¿…é¡»æ˜¯ä¸Šè¿°27ä¸ªåˆ†ç±»ä¹‹ä¸€ 
3.å¦‚æœæ–°é—»æ¶‰åŠå¤šä¸ªé¢†åŸŸï¼Œå¯ä»¥é€‰æ‹©å¤šä¸ªåˆ†ç±» 4.è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ 
5.åˆ†ç±»ä¸èƒ½é‡å¤ 6.æ”¿æ²»è¯é¢˜å¿…é¡»ç‹¬ç«‹å‡ºç°ï¼Œç¦æ­¢å¤šä¸ªæ ‡ç­¾åŒæ—¶å‡ºç° 7
.æ ¹æ®åˆ†ç±»è¯´æ˜ä»”ç»†åŒºåˆ†ç›¸ä¼¼ç±»åˆ«"""
        
        # è°ƒç”¨è±†åŒ…LLM
        success, response = await self.doubao_client.call_llm(
            prompt=prompt,
            max_tokens=800,
            temperature=0.1
        )
        
        if not success:
            raise Exception(f"è±†åŒ…LLMè°ƒç”¨å¤±è´¥: {response}")
        
        try:
            # æ¸…ç†å“åº”ï¼ˆç§»é™¤å¯èƒ½çš„markdownä»£ç å—ï¼‰
            clean_response = response.strip()
            if clean_response.startswith("```json"):
                clean_response = clean_response[7:]
            if clean_response.endswith("```"):
                clean_response = clean_response[:-3]
            clean_response = clean_response.strip()
            
            # è§£æJSONå“åº”
            result_data = json.loads(clean_response)
            
            # éªŒè¯åˆ†ç±»ç»“æœ
            categories = result_data.get("categories", [])
            thinking = result_data.get("thinking", "")
            
            # ç¡®ä¿categoriesæ˜¯åˆ—è¡¨
            if not isinstance(categories, list):
                categories = [str(categories)]
            
            # éªŒè¯æ¯ä¸ªåˆ†ç±»éƒ½åœ¨é¢„å®šä¹‰èŒƒå›´å†…
            valid_categories = []
            for category in categories:
                if category in self.CLASSIFICATION_CATEGORIES:
                    valid_categories.append(category)
                else:
                    # å°è¯•æ‰¾åˆ°æœ€æ¥è¿‘çš„åˆ†ç±»
                    category_lower = category.lower()
                    for valid_category in self.CLASSIFICATION_CATEGORIES:
                        if valid_category in category_lower or category_lower in valid_category:
                            if valid_category not in valid_categories:
                                valid_categories.append(valid_category)
                            break
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆåˆ†ç±»ï¼Œé»˜è®¤ä¸º"æ—¥å¸¸çäº‹"
            if not valid_categories:
                valid_categories = ["æ—¥å¸¸çäº‹"]
                thinking += f" (æ³¨ï¼šåŸå§‹åˆ†ç±»æ— æ•ˆï¼Œå·²ä¿®æ­£ä¸º'æ—¥å¸¸çäº‹')"
            
            return {
                "success": True,
                "method": "doubao_llm_cot_multi",
                "categories": valid_categories,
                "thinking": thinking,
                "title": title,
                "model": model
            }
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}, å“åº”å†…å®¹: {response}")
            raise Exception(f"LLMå“åº”æ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æJSON: {response[:200]}...")

    async def classify_batch_news(self, input_file: str, output_file: Optional[str] = None,
                                 title_column: str = "title", content_column: str = "content",
                                 model: str = "doubao-pro-32k",
                                 batch_size: int = 10) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†æ–°é—»åˆ†ç±»
        
        Args:
            input_file: è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ŒNoneæ—¶è‡ªåŠ¨ç”Ÿæˆ
            title_column: æ ‡é¢˜åˆ—å
            content_column: å†…å®¹åˆ—å
            model: è±†åŒ…æ¨¡å‹åç§°
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            Dict: å¤„ç†ç»“æœç»Ÿè®¡
        """
        try:
            # è¯»å–è¾“å…¥æ–‡ä»¶
            input_path = Path(input_file)
            if not input_path.exists():
                raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            
            df = pd.read_csv(input_path)
            total_count = len(df)
            
            # éªŒè¯å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
            if title_column not in df.columns:
                raise ValueError(f"æ ‡é¢˜åˆ— '{title_column}' ä¸å­˜åœ¨")
            
            # å¦‚æœå†…å®¹åˆ—ä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºåˆ—
            if content_column not in df.columns:
                df[content_column] = ""
            
            logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {total_count} æ¡æ–°é—»")
            
            # å‡†å¤‡ç»“æœåˆ—è¡¨
            results = []
            processed_count = 0
            success_count = 0
            
            # æ‰¹é‡å¤„ç†
            for i in range(0, total_count, batch_size):
                batch_df = df.iloc[i:i + batch_size]
                
                # å¤„ç†å½“å‰æ‰¹æ¬¡
                batch_tasks = []
                for _, row in batch_df.iterrows():
                    title = str(row[title_column]) if pd.notna(row[title_column]) else ""
                    content = str(row[content_column]) if pd.notna(row[content_column]) else ""
                    
                    task = self.classify_single_news(title, content, model)
                    batch_tasks.append(task)
                
                # å¹¶å‘æ‰§è¡Œå½“å‰æ‰¹æ¬¡
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                
                # å¤„ç†æ‰¹æ¬¡ç»“æœ
                for j, result in enumerate(batch_results):
                    row_idx = i + j
                    original_row = df.iloc[row_idx].to_dict()
                    
                    if isinstance(result, Exception):
                        # å¤„ç†å¼‚å¸¸
                        result_dict = {
                            "success": False,
                            "error": str(result),
                            "title": original_row.get(title_column, ""),
                            "timestamp": datetime.now().isoformat()
                        }
                        title_display = original_row.get(title_column, "")
                        print(f"\nâŒ ç¬¬{row_idx+1}æ¡åˆ†ç±»å¤±è´¥:")
                        print(f"   æ ‡é¢˜: {title_display}")
                        print(f"   é”™è¯¯: {str(result)}")
                    else:
                        result_dict = result
                        title_display = result_dict.get('title', '')
                        content_display = original_row.get(content_column, "")
                        
                        if result_dict.get("success", False):
                            success_count += 1
                            # æ‰“å°æ¯æ¡æˆåŠŸçš„åˆ†ç±»ç»“æœï¼ŒåŒ…å«å®Œæ•´å†…å®¹å’Œæ€è€ƒè¿‡ç¨‹
                            categories_str = "ã€".join(result_dict.get("categories", []))
                            thinking = result_dict.get("thinking", "")
                            
                            print(f"\nâœ… ç¬¬{row_idx+1}æ¡åˆ†ç±»æˆåŠŸ:")
                            print(f"   æ ‡é¢˜: {title_display}")
                            if content_display.strip():
                                print(f"   å†…å®¹: {content_display}")
                            print(f"   åˆ†ç±»: {categories_str}")
                            print(f"   æ€è€ƒè¿‡ç¨‹: {thinking}")
                        else:
                            error_msg = result_dict.get('error', 'æœªçŸ¥é”™è¯¯')
                            print(f"\nâŒ ç¬¬{row_idx+1}æ¡åˆ†ç±»å¤±è´¥:")
                            print(f"   æ ‡é¢˜: {title_display}")
                            if content_display.strip():
                                print(f"   å†…å®¹: {content_display}")
                            print(f"   é”™è¯¯: {error_msg}")
                    
                    # åˆå¹¶åŸå§‹æ•°æ®å’Œåˆ†ç±»ç»“æœ
                    combined_result = {**original_row, **result_dict}
                    results.append(combined_result)
                    processed_count += 1
                
                # æ˜¾ç¤ºè¿›åº¦
                progress = (processed_count / total_count) * 100
                logger.info(f"æ‰¹é‡åˆ†ç±»è¿›åº¦: {processed_count}/{total_count} ({progress:.1f}%)")
                
                # APIé™åˆ¶ï¼šå»¶è¿Ÿä¸€ä¸‹é¿å…é¢‘ç‡é™åˆ¶
                if i + batch_size < total_count:
                    await asyncio.sleep(1.5)  # 1.5ç§’å»¶è¿Ÿ
            
            # ä¿å­˜ç»“æœ
            if output_file is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = self.data_dir / f"news_classification_result_{timestamp}.csv"
            else:
                output_file = Path(output_file)
            
            # å°†ç»“æœä¿å­˜ä¸ºDataFrame
            result_df = pd.DataFrame(results)
            result_df.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            # ç»Ÿè®¡ç»“æœ
            stats = {
                "total_processed": processed_count,
                "success_count": success_count,
                "error_count": processed_count - success_count,
                "success_rate": success_count / processed_count if processed_count > 0 else 0,
                "output_file": str(output_file),
                "model": model
            }
            
            logger.info(f"æ‰¹é‡åˆ†ç±»å®Œæˆ: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"æ‰¹é‡åˆ†ç±»å¤±è´¥: {e}")
            raise

    async def get_classification_stats(self, result_file: str) -> Dict[str, Any]:
        """
        è·å–åˆ†ç±»ç»“æœç»Ÿè®¡ä¿¡æ¯
        
        Args:
            result_file: ç»“æœæ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            result_path = Path(result_file)
            if not result_path.exists():
                raise FileNotFoundError(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {result_file}")
            
            df = pd.read_csv(result_path)
            
            # åŸºç¡€ç»Ÿè®¡
            total_count = len(df)
            success_count = df['success'].sum() if 'success' in df.columns else 0
            
            # åˆ†ç±»åˆ†å¸ƒç»Ÿè®¡ï¼ˆå¤„ç†å¤šæ ‡ç­¾ï¼‰
            category_distribution = {}
            if 'categories' in df.columns:
                all_categories = []
                for categories_str in df['categories'].dropna():
                    try:
                        # å°è¯•è§£æJSONæ ¼å¼çš„categories
                        if categories_str.startswith('['):
                            categories = json.loads(categories_str)
                        else:
                            # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼ŒæŒ‰é€—å·åˆ†å‰²
                            categories = [cat.strip() for cat in str(categories_str).split(',')]
                        all_categories.extend(categories)
                    except:
                        # å¦‚æœè§£æå¤±è´¥ï¼Œå½“ä½œå•ä¸ªåˆ†ç±»å¤„ç†
                        all_categories.append(str(categories_str))
                
                # ç»Ÿè®¡æ¯ä¸ªåˆ†ç±»çš„å‡ºç°æ¬¡æ•°
                from collections import Counter
                category_counts = Counter(all_categories)
                category_distribution = dict(category_counts)
            
            # æ–¹æ³•åˆ†å¸ƒç»Ÿè®¡
            method_distribution = {}
            if 'method' in df.columns:
                method_counts = df['method'].value_counts()
                method_distribution = method_counts.to_dict()
            
            return {
                "file_path": result_file,
                "total_count": total_count,
                "success_count": int(success_count),
                "success_rate": success_count / total_count if total_count > 0 else 0,
                "category_distribution": category_distribution,
                "method_distribution": method_distribution,
                "analysis_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ç»Ÿè®¡åˆ†æå¤±è´¥: {e}")
            raise

    def get_classification_categories(self) -> List[str]:
        """è·å–æ‰€æœ‰åˆ†ç±»æ ‡å‡†"""
        return self.CLASSIFICATION_CATEGORIES.copy()


# MCPå·¥å…·å‡½æ•°
async def classify_single_news_tool(title: str, content: str = "", model: str = "doubao-pro-32k") -> Dict[str, Any]:
    """MCPå·¥å…·ï¼šå•æ¡æ–°é—»åˆ†ç±»"""
    server = NewsClassificationServer()
    return await server.classify_single_news(title, content, model)


async def classify_batch_news_tool(input_file: str, output_file: Optional[str] = None,
                                  title_column: str = "title", content_column: str = "content",
                                  model: str = "doubao-pro-32k",
                                  batch_size: int = 10) -> Dict[str, Any]:
    """MCPå·¥å…·ï¼šæ‰¹é‡æ–°é—»åˆ†ç±»"""
    server = NewsClassificationServer()
    return await server.classify_batch_news(input_file, output_file, title_column, content_column, model, batch_size)


async def get_classification_stats_tool(result_file: str) -> Dict[str, Any]:
    """MCPå·¥å…·ï¼šè·å–åˆ†ç±»ç»Ÿè®¡"""
    server = NewsClassificationServer()
    return await server.get_classification_stats(result_file)


def get_classification_categories_tool() -> List[str]:
    """MCPå·¥å…·ï¼šè·å–åˆ†ç±»æ ‡å‡†"""
    server = NewsClassificationServer()
    return server.get_classification_categories()


# ç®€åŒ–çš„æœ¬åœ°è¿è¡Œå‡½æ•°
async def main():
    """æœ¬åœ°è¿è¡Œå…¥å£å‡½æ•°"""
    print("æ–°é—»åˆ†ç±»æœåŠ¡å™¨å¯åŠ¨ä¸­...")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    api_key = os.getenv('ARK_API_KEY') or os.getenv('DOUBAO_API_KEY')
    if not api_key:
        print("âŒ é”™è¯¯: æœªè®¾ç½®è±†åŒ…APIå¯†é’¥")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ ARK_API_KEY æˆ– DOUBAO_API_KEY")
        return
    else:
        print(f"âœ… è±†åŒ…APIå¯†é’¥å·²è®¾ç½®: {api_key[:10]}...")
    
    # åˆ›å»ºæœåŠ¡å™¨å®ä¾‹
    server = NewsClassificationServer()
    
    # æµ‹è¯•å•æ¡æ–°é—»åˆ†ç±»
    print("\n=== æµ‹è¯•å•æ¡æ–°é—»åˆ†ç±» ===")
    test_cases = [
        ("å°ç±³14 Ultraæ­£å¼å‘å¸ƒï¼Œæ­è½½å¾•å¡ä¸“ä¸šæ‘„å½±ç³»ç»Ÿ", "å°ç±³å…¬å¸ä»Šæ—¥æ­£å¼å‘å¸ƒäº†å¹´åº¦æ——èˆ°æ‰‹æœºå°ç±³14 Ultra"),
        ("ç¾å›½æ€»ç»Ÿæ‹œç™»ä¼šè§ä¸­å›½å¤–äº¤éƒ¨é•¿", "åŒæ–¹å°±ä¸­ç¾å…³ç³»å’Œåœ°åŒºå®‰å…¨é—®é¢˜è¿›è¡Œæ·±å…¥äº¤æµ"),
        ("ã€Šæµæµªåœ°çƒ3ã€‹å®šæ¡£æ˜¥èŠ‚ï¼Œåˆ˜æ…ˆæ¬£ç§‘å¹»ä½œå“å†ä¸Šé“¶å¹•", ""),
        ("NBAæ€»å†³èµ›ä»Šæ™šå¼€æˆ˜ï¼Œæ¹–äººvså‡¯å°”ç‰¹äºº", ""),
        ("æ˜¥å¤©èµèŠ±æ”»ç•¥ï¼šåŒ—äº¬æœ€ç¾çš„10ä¸ªå…¬å›­", "è¸é’èµèŠ±çš„æœ€ä½³æ—¶èŠ‚å·²ç»åˆ°æ¥")
    ]
    
    for i, (title, content) in enumerate(test_cases, 1):
        print(f"\nğŸ“° æµ‹è¯•æ¡ˆä¾‹ {i}:")
        print(f"æ ‡é¢˜: {title}")
        
        result = await server.classify_single_news(title, content)
        
        if result.get("success", False):
            categories_str = "ã€".join(result['categories'])
            print(f"âœ… åˆ†ç±»: {categories_str}")
            print(f"ğŸ§  æ€è€ƒè¿‡ç¨‹: {result['thinking']}")
        else:
            print(f"âŒ åˆ†ç±»å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    # æµ‹è¯•æ‰¹é‡åˆ†ç±»ï¼ˆå¦‚æœæœ‰æµ‹è¯•æ–‡ä»¶ï¼‰
    test_file = "workspace/input/news_examples_content_hot.csv"
    if os.path.exists(test_file):
        print(f"\n=== æµ‹è¯•æ‰¹é‡åˆ†ç±» ===")
        print(f"ğŸ“ å¤„ç†æ–‡ä»¶: {test_file}")
        
        batch_result = await server.classify_batch_news(
            input_file=test_file,
            batch_size=3  # å°æ‰¹æ¬¡æµ‹è¯•
        )
        
        print(f"\nğŸ“Š æ‰¹é‡å¤„ç†ç»“æœ:")
        print(f"   æ€»æ•°: {batch_result['total_processed']}")
        print(f"   æˆåŠŸ: {batch_result['success_count']}")
        print(f"   æˆåŠŸç‡: {batch_result['success_rate']:.2%}")
        print(f"   è¾“å‡ºæ–‡ä»¶: {batch_result['output_file']}")
    else:
        print(f"\nâš ï¸  æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
    
    # æ˜¾ç¤ºæ”¯æŒçš„åˆ†ç±»
    print(f"\n=== æ”¯æŒçš„åˆ†ç±»æ ‡å‡† ({len(server.get_classification_categories())}ä¸ª) ===")
    categories = server.get_classification_categories()
    for i in range(0, len(categories), 5):
        print("  " + "ã€".join(categories[i:i+5]))
    
    print("\nğŸ‰ æ–°é—»åˆ†ç±»æœåŠ¡å™¨æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    # æœ¬åœ°ç›´æ¥è¿è¡Œ
    asyncio.run(main())