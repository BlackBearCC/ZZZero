#!/usr/bin/env python3
"""
æ–°é—»æ ‡é¢˜å’Œå†…å®¹åˆ†ç±»æœåŠ¡å™¨
åŸºäºMCPåè®®ï¼Œæ”¯æŒè±†åŒ…LLMè°ƒç”¨è¿›è¡Œæ–°é—»åˆ†ç±»
"""
import os
import csv
import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from datetime import datetime
import asyncio
import logging

# å¯¼å…¥è±†åŒ…LLMç›¸å…³æ¨¡å—
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.llm.doubao import DoubaoLLM
from src.core.types import Message, MessageRole, LLMConfig

logger = logging.getLogger(__name__)


class NewsClassificationServer:
    """æ–°é—»æ ‡é¢˜å’Œå†…å®¹åˆ†ç±»MCPæœåŠ¡å™¨"""
    
    # æ–°é—»åˆ†ç±»æ ‡å‡†ï¼ˆæ·»åŠ æ”¿æ²»åˆ†ç±»ï¼‰
    CLASSIFICATION_CATEGORIES = [
        "éŸ³ä¹", "å½±è§†", "æ¸¸æˆ", "è¿åŠ¨", "é˜…è¯»", "è‰ºæœ¯", "äºŒæ¬¡å…ƒ", "äºšæ–‡åŒ–", 
        "æ—…è¡Œ", "ç¾é£Ÿ", "æ ¡å›­","æ‘„å½±", "å¤§è‡ªç„¶", "ç§‘æŠ€æ•°ç ", "å®¶å±…", "ç»¿æ¤", "å® ç‰©", 
        "å¨±ä¹åœˆ", "ç©¿æ­ç¾å¦†", "ç”Ÿæ´»æ–¹å¼", "æ—¥å¸¸çäº‹", "çº¿ä¸‹æ´»åŠ¨", "ä¸¤æ€§æƒ…æ„Ÿ", 
        "èŒåœº", "å®¶åº­", "ä»·å€¼è§‚", "æ”¿æ²»"
    ]
    
    def __init__(self, data_dir: str = "./workspace/output"):
        """
        åˆå§‹åŒ–æ–°é—»åˆ†ç±»æœåŠ¡å™¨
        
        Args:
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•ï¼Œé»˜è®¤è¾“å‡ºåˆ°workspace/output
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–è±†åŒ…LLMå®¢æˆ·ç«¯
        self.doubao_client = None
        self._init_doubao_client()
    
    def _init_doubao_client(self):
        """åˆå§‹åŒ–è±†åŒ…LLMå®¢æˆ·ç«¯"""
        try:
            # åˆ›å»ºè±†åŒ…LLMé…ç½®
            config = LLMConfig(
                provider="doubao",
                model_name=os.getenv('DOUBAO_MODEL_DEEPSEEKV3', 'doubao-pro-32k'),
                api_key=os.getenv('ARK_API_KEY') or os.getenv('DOUBAO_API_KEY'),
                api_base=os.getenv('DOUBAO_BASE_URL', "https://ark.cn-beijing.volces.com/api/v3"),
                temperature=0.1,  # åˆ†ç±»ä»»åŠ¡ä½¿ç”¨è¾ƒä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§
                max_tokens=1000,
                timeout=30.0
            )
            
            # åˆ›å»ºè±†åŒ…LLMå®ä¾‹
            self.doubao_client = DoubaoLLM(config)
            
            # å¼‚æ­¥åˆå§‹åŒ–ï¼ˆç¨åæ‰§è¡Œï¼‰
            logger.info("è±†åŒ…LLMå®¢æˆ·ç«¯é…ç½®æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–è±†åŒ…LLMå®¢æˆ·ç«¯å¤±è´¥: {e}")
            self.doubao_client = None

    async def classify_single_news(self, title: str, content: str = "", 
                                  model: str = "doubao-pro-32k") -> Dict[str, Any]:
        """
        å¯¹å•æ¡æ–°é—»è¿›è¡Œåˆ†ç±»
        
        Args:
            title: æ–°é—»æ ‡é¢˜
            content: æ–°é—»å†…å®¹ï¼ˆå¯é€‰ï¼‰
            model: è±†åŒ…æ¨¡å‹åç§°
            
        Returns:
            Dict: åˆ†ç±»ç»“æœ
        """
        if not self.doubao_client:
            return {
                "success": False,
                "error": "æœªè®¾ç½®è±†åŒ…APIå¯†é’¥ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ ARK_API_KEY æˆ– DOUBAO_API_KEY",
                "title": title,
                "timestamp": datetime.now().isoformat()
            }
        
        try:
            result = await self._classify_with_doubao(title, content, model)
            # æ·»åŠ æ—¶é—´æˆ³
            result["timestamp"] = datetime.now().isoformat()
            return result
            
        except Exception as e:
            logger.error(f"æ–°é—»åˆ†ç±»å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "title": title,
                "timestamp": datetime.now().isoformat()
            }

    async def _classify_with_doubao(self, title: str, content: str, model: str) -> Dict[str, Any]:
        """ä½¿ç”¨è±†åŒ…LLMè¿›è¡Œæ–°é—»åˆ†ç±»ï¼Œé‡‡ç”¨COTæ€ç»´é“¾ï¼Œæ”¯æŒå¤šæ ‡ç­¾"""
        # ç¡®ä¿è±†åŒ…å®¢æˆ·ç«¯å·²åˆå§‹åŒ–
        if not self.doubao_client:
            raise Exception("è±†åŒ…LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        
        # æ‰§è¡Œå¼‚æ­¥åˆå§‹åŒ–
        await self.doubao_client.initialize()
        
        # æ„å»ºåˆ†ç±»æç¤ºè¯
        categories_str = "ã€".join(self.CLASSIFICATION_CATEGORIES)
        
        # ç»„åˆæ–°é—»æ–‡æœ¬
        news_text = f"æ ‡é¢˜ï¼š{title}"
        if content.strip():
            news_text += f"\nå†…å®¹ï¼š{content[:500]}"  # é™åˆ¶å†…å®¹é•¿åº¦
        
        # ä½¿ç”¨COTæ€ç»´é“¾æç¤ºï¼Œæ”¯æŒå¤šæ ‡ç­¾
        prompt = f"""è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å¯¹æ–°é—»è¿›è¡Œåˆ†ç±»ï¼š

**ç¬¬ä¸€æ­¥ï¼šç†è§£åˆ†ç±»æ ‡å‡†**
å¯é€‰çš„26ä¸ªåˆ†ç±»æ ‡å‡†ï¼š{categories_str}

**ç¬¬äºŒæ­¥ï¼šåˆ†ææ–°é—»å†…å®¹**
æ–°é—»å†…å®¹ï¼š
{news_text}

è¯·åˆ†æè¿™ç¯‡æ–°é—»çš„æ ¸å¿ƒä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Œä¸»è¦è®¨è®ºçš„å†…å®¹å’Œé¢†åŸŸæ˜¯ä»€ä¹ˆï¼Œæ¶‰åŠå“ªäº›æ–¹é¢ã€‚

**ç¬¬ä¸‰æ­¥ï¼šåŒ¹é…åˆ†ç±»**
æ ¹æ®æ–°é—»çš„æ ¸å¿ƒä¸»é¢˜å’Œæ¶‰åŠçš„å¤šä¸ªæ–¹é¢ï¼Œä»26ä¸ªåˆ†ç±»æ ‡å‡†ä¸­é€‰æ‹©1-3ä¸ªæœ€åˆé€‚çš„åˆ†ç±»ã€‚ä¸€ç¯‡æ–°é—»å¯ä»¥æœ‰å¤šä¸ªæ ‡ç­¾ã€‚

**æœ€ç»ˆè¾“å‡º**
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ï¼š
{{
    "thinking": "ä½ çš„åˆ†ææ€è€ƒè¿‡ç¨‹",
    "categories": ["åˆ†ç±»1", "åˆ†ç±»2"]
}}

è¦æ±‚ï¼š
1. thinkingå­—æ®µæè¿°ä½ çš„åˆ†ææ€è€ƒè¿‡ç¨‹
2. categorieså­—æ®µæ˜¯æ•°ç»„ï¼ŒåŒ…å«1-3ä¸ªåˆ†ç±»ï¼Œæ¯ä¸ªåˆ†ç±»å¿…é¡»æ˜¯ä¸Šè¿°26ä¸ªåˆ†ç±»ä¹‹ä¸€
3. å¦‚æœæ–°é—»æ¶‰åŠå¤šä¸ªé¢†åŸŸï¼Œå¯ä»¥é€‰æ‹©å¤šä¸ªåˆ†ç±»
4. è¾“å‡ºå¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼
5. åˆ†ç±»ä¸èƒ½é‡å¤
6. æ”¿æ²»è¯é¢˜ç¦æ­¢å½’ä¸ºèŒåœºï¼Œå¿…é¡»ç‹¬ç«‹åˆ†ç±»"""
        
        # è°ƒç”¨è±†åŒ…LLM
        success, response = await self.doubao_client.call_llm(
            prompt=prompt,
            max_tokens=800,
            temperature=0.1
        )
        
        if not success:
            raise Exception(f"è±†åŒ…LLMè°ƒç”¨å¤±è´¥: {response}")
        
        try:
            # æ¸…ç†å“åº”ï¼ˆç§»é™¤å¯èƒ½çš„markdownä»£ç å—ï¼‰
            clean_response = response.strip()
            if clean_response.startswith("```json"):
                clean_response = clean_response[7:]
            if clean_response.endswith("```"):
                clean_response = clean_response[:-3]
            clean_response = clean_response.strip()
            
            # è§£æJSONå“åº”
            result_data = json.loads(clean_response)
            
            # éªŒè¯åˆ†ç±»ç»“æœ
            categories = result_data.get("categories", [])
            thinking = result_data.get("thinking", "")
            
            # ç¡®ä¿categoriesæ˜¯åˆ—è¡¨
            if not isinstance(categories, list):
                categories = [str(categories)]
            
            # éªŒè¯æ¯ä¸ªåˆ†ç±»éƒ½åœ¨é¢„å®šä¹‰èŒƒå›´å†…
            valid_categories = []
            for category in categories:
                if category in self.CLASSIFICATION_CATEGORIES:
                    valid_categories.append(category)
                else:
                    # å°è¯•æ‰¾åˆ°æœ€æ¥è¿‘çš„åˆ†ç±»
                    category_lower = category.lower()
                    for valid_category in self.CLASSIFICATION_CATEGORIES:
                        if valid_category in category_lower or category_lower in valid_category:
                            if valid_category not in valid_categories:
                                valid_categories.append(valid_category)
                            break
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆåˆ†ç±»ï¼Œé»˜è®¤ä¸º"æ—¥å¸¸çäº‹"
            if not valid_categories:
                valid_categories = ["æ—¥å¸¸çäº‹"]
                thinking += f" (æ³¨ï¼šåŸå§‹åˆ†ç±»æ— æ•ˆï¼Œå·²ä¿®æ­£ä¸º'æ—¥å¸¸çäº‹')"
            
            return {
                "success": True,
                "method": "doubao_llm_cot_multi",
                "categories": valid_categories,
                "thinking": thinking,
                "title": title,
                "model": model
            }
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}, å“åº”å†…å®¹: {response}")
            raise Exception(f"LLMå“åº”æ ¼å¼é”™è¯¯ï¼Œæ— æ³•è§£æJSON: {response[:200]}...")

    async def classify_batch_news(self, input_file: str, output_file: Optional[str] = None,
                                 title_column: str = "title", content_column: str = "content",
                                 model: str = "doubao-pro-32k",
                                 batch_size: int = 10) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†æ–°é—»åˆ†ç±»
        
        Args:
            input_file: è¾“å…¥CSVæ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ŒNoneæ—¶è‡ªåŠ¨ç”Ÿæˆ
            title_column: æ ‡é¢˜åˆ—å
            content_column: å†…å®¹åˆ—å
            model: è±†åŒ…æ¨¡å‹åç§°
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            Dict: å¤„ç†ç»“æœç»Ÿè®¡
        """
        try:
            # è¯»å–è¾“å…¥æ–‡ä»¶
            input_path = Path(input_file)
            if not input_path.exists():
                raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
            
            df = pd.read_csv(input_path)
            total_count = len(df)
            
            # éªŒè¯å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
            if title_column not in df.columns:
                raise ValueError(f"æ ‡é¢˜åˆ— '{title_column}' ä¸å­˜åœ¨")
            
            # å¦‚æœå†…å®¹åˆ—ä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºåˆ—
            if content_column not in df.columns:
                df[content_column] = ""
            
            logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {total_count} æ¡æ–°é—»")
            
            # å‡†å¤‡ç»“æœåˆ—è¡¨
            results = []
            processed_count = 0
            success_count = 0
            
            # æ‰¹é‡å¤„ç†
            for i in range(0, total_count, batch_size):
                batch_df = df.iloc[i:i + batch_size]
                
                # å¤„ç†å½“å‰æ‰¹æ¬¡
                batch_tasks = []
                for _, row in batch_df.iterrows():
                    title = str(row[title_column]) if pd.notna(row[title_column]) else ""
                    content = str(row[content_column]) if pd.notna(row[content_column]) else ""
                    
                    task = self.classify_single_news(title, content, model)
                    batch_tasks.append(task)
                
                # å¹¶å‘æ‰§è¡Œå½“å‰æ‰¹æ¬¡
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                
                # å¤„ç†æ‰¹æ¬¡ç»“æœ
                for j, result in enumerate(batch_results):
                    row_idx = i + j
                    original_row = df.iloc[row_idx].to_dict()
                    
                    if isinstance(result, Exception):
                        # å¤„ç†å¼‚å¸¸
                        result_dict = {
                            "success": False,
                            "error": str(result),
                            "title": original_row.get(title_column, ""),
                            "timestamp": datetime.now().isoformat()
                        }
                        print(f"âŒ ç¬¬{row_idx+1}æ¡åˆ†ç±»å¤±è´¥: {original_row.get(title_column, '')[:50]}... - {str(result)}")
                    else:
                        result_dict = result
                        if result_dict.get("success", False):
                            success_count += 1
                            # æ‰“å°æ¯æ¡æˆåŠŸçš„åˆ†ç±»ç»“æœ
                            categories_str = "ã€".join(result_dict.get("categories", []))
                            print(f"âœ… ç¬¬{row_idx+1}æ¡: {result_dict.get('title', '')[:50]}... â†’ {categories_str}")
                        else:
                            print(f"âŒ ç¬¬{row_idx+1}æ¡åˆ†ç±»å¤±è´¥: {result_dict.get('title', '')[:50]}... - {result_dict.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    
                    # åˆå¹¶åŸå§‹æ•°æ®å’Œåˆ†ç±»ç»“æœ
                    combined_result = {**original_row, **result_dict}
                    results.append(combined_result)
                    processed_count += 1
                
                # æ˜¾ç¤ºè¿›åº¦
                progress = (processed_count / total_count) * 100
                logger.info(f"æ‰¹é‡åˆ†ç±»è¿›åº¦: {processed_count}/{total_count} ({progress:.1f}%)")
                
                # APIé™åˆ¶ï¼šå»¶è¿Ÿä¸€ä¸‹é¿å…é¢‘ç‡é™åˆ¶
                if i + batch_size < total_count:
                    await asyncio.sleep(1.5)  # 1.5ç§’å»¶è¿Ÿ
            
            # ä¿å­˜ç»“æœ
            if output_file is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = self.data_dir / f"news_classification_result_{timestamp}.csv"
            else:
                output_file = Path(output_file)
            
            # å°†ç»“æœä¿å­˜ä¸ºDataFrame
            result_df = pd.DataFrame(results)
            result_df.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            # ç»Ÿè®¡ç»“æœ
            stats = {
                "total_processed": processed_count,
                "success_count": success_count,
                "error_count": processed_count - success_count,
                "success_rate": success_count / processed_count if processed_count > 0 else 0,
                "output_file": str(output_file),
                "model": model
            }
            
            logger.info(f"æ‰¹é‡åˆ†ç±»å®Œæˆ: {stats}")
            return stats
            
        except Exception as e:
            logger.error(f"æ‰¹é‡åˆ†ç±»å¤±è´¥: {e}")
            raise

    async def get_classification_stats(self, result_file: str) -> Dict[str, Any]:
        """
        è·å–åˆ†ç±»ç»“æœç»Ÿè®¡ä¿¡æ¯
        
        Args:
            result_file: ç»“æœæ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            result_path = Path(result_file)
            if not result_path.exists():
                raise FileNotFoundError(f"ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {result_file}")
            
            df = pd.read_csv(result_path)
            
            # åŸºç¡€ç»Ÿè®¡
            total_count = len(df)
            success_count = df['success'].sum() if 'success' in df.columns else 0
            
            # åˆ†ç±»åˆ†å¸ƒç»Ÿè®¡ï¼ˆå¤„ç†å¤šæ ‡ç­¾ï¼‰
            category_distribution = {}
            if 'categories' in df.columns:
                all_categories = []
                for categories_str in df['categories'].dropna():
                    try:
                        # å°è¯•è§£æJSONæ ¼å¼çš„categories
                        if categories_str.startswith('['):
                            categories = json.loads(categories_str)
                        else:
                            # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼ŒæŒ‰é€—å·åˆ†å‰²
                            categories = [cat.strip() for cat in str(categories_str).split(',')]
                        all_categories.extend(categories)
                    except:
                        # å¦‚æœè§£æå¤±è´¥ï¼Œå½“ä½œå•ä¸ªåˆ†ç±»å¤„ç†
                        all_categories.append(str(categories_str))
                
                # ç»Ÿè®¡æ¯ä¸ªåˆ†ç±»çš„å‡ºç°æ¬¡æ•°
                from collections import Counter
                category_counts = Counter(all_categories)
                category_distribution = dict(category_counts)
            
            # æ–¹æ³•åˆ†å¸ƒç»Ÿè®¡
            method_distribution = {}
            if 'method' in df.columns:
                method_counts = df['method'].value_counts()
                method_distribution = method_counts.to_dict()
            
            return {
                "file_path": result_file,
                "total_count": total_count,
                "success_count": int(success_count),
                "success_rate": success_count / total_count if total_count > 0 else 0,
                "category_distribution": category_distribution,
                "method_distribution": method_distribution,
                "analysis_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ç»Ÿè®¡åˆ†æå¤±è´¥: {e}")
            raise

    def get_classification_categories(self) -> List[str]:
        """è·å–æ‰€æœ‰åˆ†ç±»æ ‡å‡†"""
        return self.CLASSIFICATION_CATEGORIES.copy()


# MCPå·¥å…·å‡½æ•°
async def classify_single_news_tool(title: str, content: str = "", model: str = "doubao-pro-32k") -> Dict[str, Any]:
    """MCPå·¥å…·ï¼šå•æ¡æ–°é—»åˆ†ç±»"""
    server = NewsClassificationServer()
    return await server.classify_single_news(title, content, model)


async def classify_batch_news_tool(input_file: str, output_file: Optional[str] = None,
                                  title_column: str = "title", content_column: str = "content",
                                  model: str = "doubao-pro-32k",
                                  batch_size: int = 10) -> Dict[str, Any]:
    """MCPå·¥å…·ï¼šæ‰¹é‡æ–°é—»åˆ†ç±»"""
    server = NewsClassificationServer()
    return await server.classify_batch_news(input_file, output_file, title_column, content_column, model, batch_size)


async def get_classification_stats_tool(result_file: str) -> Dict[str, Any]:
    """MCPå·¥å…·ï¼šè·å–åˆ†ç±»ç»Ÿè®¡"""
    server = NewsClassificationServer()
    return await server.get_classification_stats(result_file)


def get_classification_categories_tool() -> List[str]:
    """MCPå·¥å…·ï¼šè·å–åˆ†ç±»æ ‡å‡†"""
    server = NewsClassificationServer()
    return server.get_classification_categories()


# ç®€åŒ–çš„æœ¬åœ°è¿è¡Œå‡½æ•°
async def main():
    """æœ¬åœ°è¿è¡Œå…¥å£å‡½æ•°"""
    print("æ–°é—»åˆ†ç±»æœåŠ¡å™¨å¯åŠ¨ä¸­...")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    api_key = os.getenv('ARK_API_KEY') or os.getenv('DOUBAO_API_KEY')
    if not api_key:
        print("âŒ é”™è¯¯: æœªè®¾ç½®è±†åŒ…APIå¯†é’¥")
        print("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ ARK_API_KEY æˆ– DOUBAO_API_KEY")
        return
    else:
        print(f"âœ… è±†åŒ…APIå¯†é’¥å·²è®¾ç½®: {api_key[:10]}...")
    
    # åˆ›å»ºæœåŠ¡å™¨å®ä¾‹
    server = NewsClassificationServer()
    
    # æµ‹è¯•å•æ¡æ–°é—»åˆ†ç±»
    print("\n=== æµ‹è¯•å•æ¡æ–°é—»åˆ†ç±» ===")
    test_cases = [
        ("å°ç±³14 Ultraæ­£å¼å‘å¸ƒï¼Œæ­è½½å¾•å¡ä¸“ä¸šæ‘„å½±ç³»ç»Ÿ", "å°ç±³å…¬å¸ä»Šæ—¥æ­£å¼å‘å¸ƒäº†å¹´åº¦æ——èˆ°æ‰‹æœºå°ç±³14 Ultra"),
        ("ç¾å›½æ€»ç»Ÿæ‹œç™»ä¼šè§ä¸­å›½å¤–äº¤éƒ¨é•¿", "åŒæ–¹å°±ä¸­ç¾å…³ç³»å’Œåœ°åŒºå®‰å…¨é—®é¢˜è¿›è¡Œæ·±å…¥äº¤æµ"),
        ("ã€Šæµæµªåœ°çƒ3ã€‹å®šæ¡£æ˜¥èŠ‚ï¼Œåˆ˜æ…ˆæ¬£ç§‘å¹»ä½œå“å†ä¸Šé“¶å¹•", ""),
        ("NBAæ€»å†³èµ›ä»Šæ™šå¼€æˆ˜ï¼Œæ¹–äººvså‡¯å°”ç‰¹äºº", ""),
        ("æ˜¥å¤©èµèŠ±æ”»ç•¥ï¼šåŒ—äº¬æœ€ç¾çš„10ä¸ªå…¬å›­", "è¸é’èµèŠ±çš„æœ€ä½³æ—¶èŠ‚å·²ç»åˆ°æ¥")
    ]
    
    for i, (title, content) in enumerate(test_cases, 1):
        print(f"\nğŸ“° æµ‹è¯•æ¡ˆä¾‹ {i}:")
        print(f"æ ‡é¢˜: {title}")
        
        result = await server.classify_single_news(title, content)
        
        if result.get("success", False):
            categories_str = "ã€".join(result['categories'])
            print(f"âœ… åˆ†ç±»: {categories_str}")
            print(f"ğŸ§  æ€è€ƒè¿‡ç¨‹: {result['thinking']}")
        else:
            print(f"âŒ åˆ†ç±»å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    
    # æµ‹è¯•æ‰¹é‡åˆ†ç±»ï¼ˆå¦‚æœæœ‰æµ‹è¯•æ–‡ä»¶ï¼‰
    test_file = "workspace/input/news_examples_content_hot.csv"
    if os.path.exists(test_file):
        print(f"\n=== æµ‹è¯•æ‰¹é‡åˆ†ç±» ===")
        print(f"ğŸ“ å¤„ç†æ–‡ä»¶: {test_file}")
        
        batch_result = await server.classify_batch_news(
            input_file=test_file,
            batch_size=3  # å°æ‰¹æ¬¡æµ‹è¯•
        )
        
        print(f"\nğŸ“Š æ‰¹é‡å¤„ç†ç»“æœ:")
        print(f"   æ€»æ•°: {batch_result['total_processed']}")
        print(f"   æˆåŠŸ: {batch_result['success_count']}")
        print(f"   æˆåŠŸç‡: {batch_result['success_rate']:.2%}")
        print(f"   è¾“å‡ºæ–‡ä»¶: {batch_result['output_file']}")
    else:
        print(f"\nâš ï¸  æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
    
    # æ˜¾ç¤ºæ”¯æŒçš„åˆ†ç±»
    print(f"\n=== æ”¯æŒçš„åˆ†ç±»æ ‡å‡† ({len(server.get_classification_categories())}ä¸ª) ===")
    categories = server.get_classification_categories()
    for i in range(0, len(categories), 5):
        print("  " + "ã€".join(categories[i:i+5]))
    
    print("\nğŸ‰ æ–°é—»åˆ†ç±»æœåŠ¡å™¨æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    # æœ¬åœ°ç›´æ¥è¿è¡Œ
    asyncio.run(main())